{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg-python\n",
      "  Using cached ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting future\n",
      "  Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Installing collected packages: future, ffmpeg-python\n",
      "Successfully installed ffmpeg-python-0.2.0 future-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (4.10.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: moviepy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (4.10.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.66.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (69.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install SpeechRecognition moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: moviepy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (4.10.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.66.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (69.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in An Important Advice-AI .wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#install libs\n",
    "!pip install SpeechRecognition moviepy\n",
    "import moviepy.editor as mpe\n",
    "#convert to audio\n",
    "video = mpe.VideoFileClip(\"videos\\An Important Advice-AI Is Advancing Just Start Learning AI Before Its Late.mp4\")\n",
    "video.audio.write_audiofile(r\"An Important Advice-AI .wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RequestError",
     "evalue": "recognition request failed: Bad Request",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\speech_recognition\\recognizers\\google.py:205\u001b[0m, in \u001b[0;36mobtain_transcription\u001b[1;34m(request, timeout)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    560\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    493\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m             af\u001b[38;5;241m.\u001b[39mwrite(text)\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mspeech\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m, in \u001b[0;36mspeech\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mAudioFile(filename) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m speech_engine\u001b[38;5;241m.\u001b[39mrecord(f)\n\u001b[1;32m----> 8\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mspeech_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize_google\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mde-DE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     af\u001b[38;5;241m.\u001b[39mwrite(text)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(text)\n",
      "File \u001b[1;32mc:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\speech_recognition\\recognizers\\google.py:244\u001b[0m, in \u001b[0;36mrecognize_legacy\u001b[1;34m(recognizer, audio_data, key, language, pfilter, show_all, with_confidence)\u001b[0m\n\u001b[0;32m    239\u001b[0m request_builder \u001b[38;5;241m=\u001b[39m create_request_builder(\n\u001b[0;32m    240\u001b[0m     key\u001b[38;5;241m=\u001b[39mkey, language\u001b[38;5;241m=\u001b[39mlanguage, filter_level\u001b[38;5;241m=\u001b[39mpfilter\n\u001b[0;32m    241\u001b[0m )\n\u001b[0;32m    242\u001b[0m request \u001b[38;5;241m=\u001b[39m request_builder\u001b[38;5;241m.\u001b[39mbuild(audio_data)\n\u001b[1;32m--> 244\u001b[0m response_text \u001b[38;5;241m=\u001b[39m \u001b[43mobtain_transcription\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperation_timeout\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m output_parser \u001b[38;5;241m=\u001b[39m OutputParser(\n\u001b[0;32m    249\u001b[0m     show_all\u001b[38;5;241m=\u001b[39mshow_all, with_confidence\u001b[38;5;241m=\u001b[39mwith_confidence\n\u001b[0;32m    250\u001b[0m )\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_parser\u001b[38;5;241m.\u001b[39mparse(response_text)\n",
      "File \u001b[1;32mc:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\speech_recognition\\recognizers\\google.py:207\u001b[0m, in \u001b[0;36mobtain_transcription\u001b[1;34m(request, timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m     response \u001b[38;5;241m=\u001b[39m urlopen(request, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecognition request failed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e\u001b[38;5;241m.\u001b[39mreason))\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecognition connection failed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e\u001b[38;5;241m.\u001b[39mreason)\n\u001b[0;32m    211\u001b[0m     )\n",
      "\u001b[1;31mRequestError\u001b[0m: recognition request failed: Bad Request"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "filename = \"An Important Advice-AI .wav\"\n",
    "def speech(filename):\n",
    "    speech_engine = sr.Recognizer()\n",
    "    with open(\"An Important Advice-AI.txt\", \"w+\") as af:\n",
    "        with sr.AudioFile(filename) as f:\n",
    "            data = speech_engine.record(f)\n",
    "            text = speech_engine.recognize_google(data, language=\"de-DE\")\n",
    "            af.write(text)\n",
    "            print(text)\n",
    "speech(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in DATASCIENCE.mp4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "import wave, math, contextlib\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import AudioFileClip\n",
    "transcribed_audio_file_name = \"DATASCIENCE.mp4.wav\"\n",
    "zoom_video_file_name = \"An Important Advice-AI .wav\"\n",
    "audioclip = AudioFileClip(zoom_video_file_name)\n",
    "audioclip.write_audiofile(transcribed_audio_file_name)\n",
    "with contextlib.closing(wave.open(transcribed_audio_file_name,'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)\n",
    "total_duration = math.ceil(duration / 60)\n",
    "r = sr.Recognizer()\n",
    "for i in range(0, total_duration):\n",
    "    with sr.AudioFile(transcribed_audio_file_name) as source:\n",
    "        audio = r.record(source, offset=i*60, duration=60)\n",
    "    f = open(\"DATASCIENCE.txt\", \"a\")\n",
    "    f.write(r.recognize_google(audio))\n",
    "    f.write(\" \")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: Error: recognition request failed: Bad Request\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    # Load the audio file\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio = recognizer.record(source, duration=300)  # Transcribe the first 300 seconds (5 minutes) of audio\n",
    "    \n",
    "    # Transcribe the audio\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"An Important Advice-AI .wav\"  # Path to your input audio file\n",
    "    transcription = transcribe_audio(audio_file)\n",
    "    print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def split_audio(input_audio, output_folder, segment_duration=60000):\n",
    "    audio = AudioSegment.from_file(input_audio)\n",
    "    segment_index = 0\n",
    "\n",
    "    while len(audio) > segment_duration:\n",
    "        segment = audio[:segment_duration]\n",
    "        segment.export(os.path.join(output_folder, f\"segment_{segment_index}.wav\"), format=\"wav\")\n",
    "        audio = audio[segment_duration:]\n",
    "        segment_index += 1\n",
    "\n",
    "    if len(audio) > 0:\n",
    "        audio.export(os.path.join(output_folder, f\"segment_{segment_index}.wav\"), format=\"wav\")\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio_data)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def main(input_audio, output_text):\n",
    "    output_folder = \"audio_segments\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Split audio into segments\n",
    "    split_audio(input_audio, output_folder)\n",
    "\n",
    "    # Transcribe each segment\n",
    "    transcriptions = []\n",
    "    for segment_file in os.listdir(output_folder):\n",
    "        segment_path = os.path.join(output_folder, segment_file)\n",
    "        transcription = transcribe_audio(segment_path)\n",
    "        transcriptions.append(transcription)\n",
    "\n",
    "    # Combine transcriptions\n",
    "    with open(output_text, \"w\") as f:\n",
    "        for transcription in transcriptions:\n",
    "            f.write(transcription + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_audio = \"An Important Advice-AI .wav\"  # Path to your input audio file\n",
    "    output_text = \"output_transcription.txt\"  # Path to save the transcription\n",
    "    main(input_audio, output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#its video to audio \n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def video_to_audio(video_file, audio_file):\n",
    "    # Load the video file\n",
    "    video_clip = VideoFileClip(video_file)\n",
    "\n",
    "    # Extract the audio from the video\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    # Save the extracted audio to a file\n",
    "    audio_clip.write_audiofile(audio_file)\n",
    "\n",
    "    # Close the video and audio clips\n",
    "    video_clip.close()\n",
    "    audio_clip.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"videos\\Devin AI Capabilities,What Can First AI Software Engineer Do_ Future Of Software Engineering.mp4\"\n",
    "    audio_file = \"davin.wav\"  # You can specify the desired audio format here\n",
    "\n",
    "    # Convert video to audio\n",
    "    video_to_audio(video_file, audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#its working good \n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio_data)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def split_audio(input_audio):\n",
    "    audio = AudioSegment.from_file(input_audio)\n",
    "    segment_duration_ms = 60 * 1000  # 1 minute in milliseconds\n",
    "    segments = []\n",
    "\n",
    "    for start_time in range(0, len(audio), segment_duration_ms):\n",
    "        segment = audio[start_time:start_time + segment_duration_ms]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def main(input_audio):\n",
    "    segments = split_audio(input_audio)\n",
    "    transcriptions = []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment.export(f\"segment_{i}.wav\", format=\"wav\")\n",
    "        transcription = transcribe_audio(f\"segment_{i}.wav\")\n",
    "        transcriptions.append(transcription)\n",
    "        os.remove(f\"segment_{i}.wav\")\n",
    "\n",
    "    combined_transcription = \" \".join(transcriptions)\n",
    "    return combined_transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_audio = \"DATASCIENCE.mp4.wav\"  # Path to your input audio file\n",
    "    transcription = main(input_audio)\n",
    "    print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: hello all my name is krish Naik and welcome to my YouTube channel so guys yes this particular video is again related to some advice and guidance that I really want to provide you all and yes in this video I am going to talk about AI I want to go to provide you some kind of advice at least to start learning AI and there is a reason why I am specifically saying you this I am not saying that ok your end goal should be probably making a transition working in some\n",
      "Chunk 2: companies analytics industry not as such but start incorporating AI in your day to day lives ai's evolving a lot lot of new things are probably coming up initially we had machine learning deep learning now generally models and problem in the upcoming two years definitely lot of companies are coming to come up with different different Startup ideas just using this kind of LLM models where they are specifically solving some kind of problem ok\n",
      "Chunk 3: but my advice will be that you know start learning AI to incorporate the capabilities that you can actually put in your day to day activities you know as you know data data Speaks a lot you know and if you probably know AI machine learning deep learning trust me you will be able to explore many more information from that specific data and not saying that ok learn just to get a job but instead learn to make\n",
      "Chunk 4: more productive and you can definitely do that I know many people are working in different domains different Technologies and different programming language they having a different work but at least have an idea about AI start incorporating that in your life you may be thinking Crush you are a youtuber you are doing it for your purpose you know you may gain subscribers you may probably bring up your telling people to buy courses I am not saying nothing as such I am saying that wherever you get some sources\n",
      "Chunk 5: learning at not my channel at least from somewhere else is so many open source documentation that are available you know once you start incorporating it trust me opportunities are them anymore in the world not only see if you don't want to work anyway at least use it in your personal life right you have your financial data right you have your let's say I'll give you one example one of my friend is call me this morning right he was saying the crush I am running a business you know I have this specific use cases and this is\n",
      "Chunk 6: possible because of machine learning can you help me out in this you know and he is saying his purposely saying that I also want to learn the snap you know let it be a business create you are a person who is running a business you are a person who who is working on some of the other thing and there if you get a bit of chances of automatic things trust me I will come into picture ok and this is super important this is the advice that I really want to give to everyone ok again\n",
      "Chunk 7: my main aim is to democracy this AI education to everyone that is the reason why I have come up with this YouTube channel when I specifically upload videos related to everything that is in Ai tomorrow anything that probably comes I will be explaining you I will be teaching you I will be showing you multiple examples you know yeah it is up to you whether you want to make it as a full-time opportunity whether you want to learn it in such a way that you may probably get a job but start incorporating it is up to you guys I am not forcing you but start\n",
      "Chunk 8: setting it try to use it in your day to day practices you will be able to see the changes I have seen some of my friends getting productive you know I have my cousin brother who is working in US and from past two years you know his working in his architect working in something else some other Technology some other domain but still he has started using AI in his day to day activities and this is the most important advice that I really want to give it to you right again my\n",
      "Chunk 9: to democracy education to everyone that is the reason I am uploading this many number of videos with respect to learning anything it is up to you find out any sources but start learning this is the advice that I really want to give it to you because you will be seeing how much changes it is going to come up in the upcoming 2 years right now lines in is going on you can actually create your own LLM models you know I was just solving a use case right now what ever documents I have let's say\n",
      "Chunk 10: I want to I want to probably create an LLM model with respect to my data and I was just seeing that I had a 2GB of files that is present PDF files which are a lot of content I was able to train my own chat box so this kind of examples will definitely come up I will show you how you can probably train at night but I want to do it for my day to day purpose see let's say that I have my Excel sheets of all the expenditures all the expenses that I am doing right in some format right I can also train that specific thing\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"DATASCIENCE.mp4.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: hello all my name is Krishna and welcome to my YouTube channel so guys I hope from past couple of days you have seen a lot of videos related to Deven the first AI software engineer probably every youtuber has uploaded the specific video and obviously they have brought a lot of points with respect comparing to a software engineer jobs but let's understand in this video what all things doesn't can actually do and as you all know many people have applied for the beta version of 7 and they have got the access and with respect to that they have created\n",
      "Chunk 2: so we'll just try to get a clear idea like what all efficient task it can actually do and after understanding all these things we will be discussing about the pros and cons and as a software engineer what you really need to focus to be in the industry with a good demand what all things you can actually do will also be discussing about that so please make sure that he was this video tilly and because there will be a lot of things to discuss in this ok so here was the video of the devil\n",
      "Chunk 3: AI software we just saw the demo but just to understand you can see what is exactly doesn't it is the world's first autonomous AI software engineer devil is tireless skilled him at equally ready to build alongside you are independently complete task for you to review so in short since it is an eye so it really need to work tirelessly right with human beings that is not possible right now a very important point is over here with devil engineers can focus more on the\n",
      "Chunk 4: problem statement and engineer teams can Strike for more ambitious goals because all the previous problem statement are already solved by the engineers and with respect to that they already have the data code whatever things are specifically required so obviously you really need to have a new problem statement to think upon which Devin cannot solve whatever things are actually sold demon will be able to solve it because they already trained on that right so this is the most major point right still you know any new thing any new\n",
      "Chunk 5: statement that may come devil will not be able to solve it because it needs to get train right it's just like any high if it if there is a solution then it will be trained then it will be able to do it ok unless and until Agi does not come I don't think so it will be able to do it now let's understand about Demons capability ok here you will be able to see we have given them in the ability to actively collaborate with the user demon reports on its progress in real time except feedback and work together with you through\n",
      "Chunk 6: choices are needed so obviously As a Software Engineer whatever task is required for a software engineer it is able to do they are working in a collaborative way they have to make sure that they have all the documentation spend stories needs to be assigned very much in an equal way to all the developers itself now with respect to the feedback there really need to consider it and really need to solve it now let's understand from all the people who have actually got the excess water can actually do with respect to the implementation that they have done so here you can see devil can learn how to use\n",
      "Chunk 7: similar technologies also after reading a block post doesn't runs control not on model to produce images with conceals messages for Sara so one of the users in they were able to do from this particular blog then can build and and deploy apps end to end so this was actually possible by devil they were able to build and deploy it because already you know how to build it and how to deploy it that is the time of task and obviously from the demo that we have seen a couple days back it has those kind of functionalities along\n",
      "Chunk 8: this whenever we see the next task over here you can see that devil can also autonomously find and fix bugs in the code basis that basically is demon helps angry maintain and debug is open source competitive programming book so obviously when you have this entire codebase you know and it is being able to solve it ok and will also be seeing there is something called as SW benchmark OK will discuss about it like in the GitHub open issues like how much it was able to solve so here the next task here you\n",
      "Chunk 9: devil country and find units on AI models David can address bugs and feature request in open source repositories just give the GitHub issues and do the setup and context gathering that is only needed and Devil will be able to do it doesn't can contribute to mature production repositories which is also very really good we even tried giving driven real jobs on upwork and it could do those two so if you don't know about upwork it is all about freelancing and just\n",
      "Chunk 10: you don't even have to stay there to do the any freelancing work it can probably considerate it can probably solve it and with respect to all this work that you can actually see all the demo videos actually given and how it was able to do it now just by seeing this obviously we can actually see that doesn't can really do a lot many things right and if I proud consider a normal software engineer right now what is actually expected in interviews in the future in upcoming 12 years there will be a lot of expectation you really need to\n",
      "Chunk 11: jack of all trades you know you should have knowledge with respect to everything but yes if there is a problem statement that requires actual research out of box thinking out of box thinking to solve it the human beings are the best people to solve it later on whatever task are there whatever repetitive task are there whatever things that I already implemented them and can actually do it for you this was one of the thing which I was talking about SW bench so in the GitHub repository whatever the open issues right\n",
      "Chunk 12: Falcon privacy world get a wishes how much it was able to solve it so here you can see we evaluated driven on SW bench a challenging benchmark that ask agent to resolve Real world data wishes found in open source projects and Django and sky David correctly results 13.86 % all the issues and to end you know which is quite good and the previous state of art was 1.96 now with respect to all the other tools right like cloudy to SW Lama 13 bsw Lama 7 BJP 4G PT 3.5\n",
      "Chunk 13: you can actually see you know the difference is quite huge and Devin is really really having a good number here that is 13.86 now this really looks yes then can do a lot many task but as I said when it comes to problem statement that comes out of the box to solve it definitely them cannot do because they are not trained in those kind of data but as a software engineer one thing that you really need to do is that companies will also look at you know\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"davin.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: whisper in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.1.10)\n",
      "Requirement already satisfied: six in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from whisper) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\shamshad ahmed\\appdata\\local\\temp\\pip-req-build-3a0ve_gs\n",
      "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (4.66.2)\n",
      "Requirement already satisfied: torch in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (2.2.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (0.6.0)\n",
      "Requirement already satisfied: numba in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (0.59.1)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (10.2.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from numba->openai-whisper==20231117) (0.42.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (2024.2.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (1.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (4.10.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm->openai-whisper==20231117) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/openai/whisper.git 'C:\\Users\\Shamshad ahmed\\AppData\\Local\\Temp\\pip-req-build-3a0ve_gs'\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.39.1-py3-none-any.whl (8.8 MB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.39.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg-python in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"choco\" - maybe you meant \"check\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip choco install ffmpeg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall ffmpeg\n",
    "# pip3 uninstall ffmpeg\n",
    "# pip3 uninstall ffmpeg-python\n",
    "# pip uninstall ffmpeg\n",
    "# pip uninstall ffmpeg-python\n",
    "# brew uninstall ffmpeg\n",
    "# Install ffmpeg binary using this 405 guide.\n",
    "\n",
    "# Install ffmpeg for python\n",
    "\n",
    "# pip3 install ffmpeg\n",
    "# pip3 install ffmpeg-python\n",
    "# pip install ffmpeg\n",
    "# pip install ffmpeg-python\n",
    "# # That’s all it should work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pip3 (from versions: none)\n",
      "ERROR: No matching distribution found for pip3\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pip3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg-python in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg-python --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch device is set to cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello all, my name is Krashanayak and welcome to my YouTube channel. So guys, yes, this particular video is again related to some advice, some guidance that I really want to provide you all. And yes, in this video, I'm going to talk about AI. I want to go to provide you some kind of advice at least to start learning AI. And there is a reason why I am specifically saying you this. I'm not saying that, okay that your end goal should be probably making a transition, working in some AI companies, analytics industry, not as such, but start incorporating AI in your day to day lives. AI is evolving a lot, a lot of new things are probably coming up. Initially, we had machine learning, deep learning, now, generally AI, L LLM models and probably in the upcoming two years definitely a lot of companies are going to come up with different different startup ideas just using this kind of LLM models where they are specifically solving some kind of problem. Okay, but my advice will be that you know start learning AI to incorporate the capabilities that you can actually put in your day to day activities, you know, as you know, data, data speaks a lot, you know, and if you probably know AI, machine learning, deep learning, trust me you will be able to explore many more information from that specific data. I'm not saying that okay learn just to get a job but instead learn to make yourself more productive and you can definitely do that. I know many people are working in different domains, different technologies and different programming languages, they are having a different work but at least have an idea about AI, start incorporating that in your life. You may be thinking, Chris, you are a YouTuber, you're doing it for your purpose. You know, you may gain subscribers, you may probably bring up, you're telling people to buy courses. I'm not saying nothing as such. I'm saying that wherever you get some sources, start learning it, not my channel, at least from somewhere else. There's so many open source documentation that are available. You know, once you start incorporating it, trust me, opportunities are there many more in the world, not only see if you don't want to work anywhere, at least use it in your personal life. Right. You have your financial data, right? You have your, let's say, I'll give you one example. One of my friends just called me this morning, right? He was saying that I'm running a business, you know, I have this specific use cases and this is only possible because of machine learning. Can you help me out in this? You know, and he's saying, he's purposely saying that I also want to learn this now. Let it be a business creator, you are a person who is running a business, you are a person who is working on some or the other thing and there if you get a bit of chances of automating things, trust me AI will come into picture. Okay, and this is super important. This is the advice that I really want to give to everyone. Okay, again, my main aim is to democratize this AI education to everyone. That is the reason why I have come up with this YouTube channel where I specifically upload videos related to everything that is in AI. Tomorrow anything that probably comes, I will be explaining you, I will be teaching you, I will be showing you multiple examples. It is up to you whether you want to make it as a full time opportunity, whether you want to learn it in such a way that you may probably get a job but start incorporating it. It is up to you guys. I'm not forcing you but start incorporating it. Try to use it in your day to day practices. You will be able to see the changes. I've seen some of my friends getting productive. You know, I have my cousin brother who is working in US and from past two years, you know, he's working in some, he's a architect working in something else, some other technology, some other domain, but still he has started using AI in his day to day activities and he can see that productivity. And this is the most important advice that I really want to give it to you. Again, my main aim is to democratize AI education to everyone. That is the reason why I'm uploading this many number of videos with respect to learning anything. It is up to you. Find out any sources but start learning. This is the advice I really want to give to you because you will be seeing how much changes it is going to come up in the upcoming two years. Right now, Lanchin is going on. You can actually create your own LLM models. I was just solving a use case right now. Whatever documents I have, let's say I want to probably create an LLM model with respect to my data. I was just seeing that I had a 2GB of files that is present, PDF files which had a lot of content and I was able to train my own chatbot. So this kind of examples will definitely come up. I'll show you how you can probably train it. Right. But I want to do it for my day to day purpose. and my LLM models can give advice where I can probably save some money. This is just one of the examples that I really want to come up with in front of you. A lot of applications will be there and definitely do make sure that you take up this advice, take it seriously, not for making, not for working full time at least, but at least try to incorporate this knowledge in your life. So yes, I hope you like this particular video. If you like this, I'll see you all in the next video. Have a great day. Thank you, Wanda. Bye-bye, take care.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the device\n",
    "pytorch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current PyTorch device is set to\", pytorch_device)\n",
    "\n",
    "# Define the path to the audio file\n",
    "audio_path = \"DATASCIENCE.mp4.wav\"\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = pipeline(model=\"openai/whisper-small\", device=pytorch_device)\n",
    "\n",
    "# Process the audio file\n",
    "predictions = pipe(audio_path, chunk_length_s=20, stride_length_s=5)\n",
    "\n",
    "# Extract the recognized text from the predictions\n",
    "recognized_text = predictions[\"text\"]\n",
    "print(recognized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch device is set to cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello all my name is Chris Snaik and welcome to my youtube channel. So guys first of all a very happy Dashira to all of you I will hardly take around two to three minutes because I really want to convey a very important message as this is your festival I definitely want you all to enjoy with your family. If I talk about Dashira guys, I just want to say this saying, Burai ko aag lagao, your life, do good things in your life, always be truthful to yourself, truthful with others. I know in this world of negativity, competition, jealousy, many more things are there in our life, you know, in your office place, in your workplace, in your day-to-day activities with your friends, with your enemies, you know. What Desira specifies to you is that at the end of the day, the truth wins. And I feel many people have forgotten this. Again, I just want to stress on one point, please bring positivity in your life. Please bring positivity in others life also if you can. You know, always try to do the right thing. I know many people do not do the right thing. There may be multiple things, multiple factors that may be affecting them to do the right things. It is always good and better because at the end of the day, the good deed that you specifically do wins everybody's heart in this world. I know right now if I'm speaking about this word many people may not agree with me. I've spent more than 13 plus years in my IT company. I have seen each and everything. Basically in IT company I have worked in multiple places with respect to work, with respect to multiple things within our friends group. I've seen jealousy, I've seen hate, I've seen good deeds, I've seen everything but at the end of the day from all this experience what I feel is that when you do good, when you're truthful to yourself and you're truthful to others at the end of the day, from all this experience, what I feel is that when you do good, when you're truthful to yourself and you're truthful to others, at the end of the day, it is always beneficial for you. So this is what the She-Ra all signifies in our life itself. Again guys I really want to stress on this specific point, burai ko hatao, remove all the negativity in your life, try to be truthful, try to bring that positiveness. By that, everybody around you will always be something. at the end of the day. They are human being, you know, the thought process that goes in their mind because of some something, you know, it may be something right at the end of the day. We are all brothers out there living in this beautiful earth and we should definitely help out each other. So considering this I'm just starting this positivity message to everyone. All the free content with respect to data science will be available in the description along with the written materials along with the videos everything as such. Again guys don't study today at least enjoy this day with your family and friends. This is the festival this month then Diwali and many more things. So yes this was it for my side I hope you like this particular video I'll see you all in the next video have a great day thank you and take care bye bye\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the device\n",
    "pytorch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current PyTorch device is set to\", pytorch_device)\n",
    "\n",
    "# Define the path to the audio file\n",
    "audio_path = \"videos\\Many People Have Forgetten This!.mp4\"\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = pipeline(model=\"openai/whisper-small\", device=pytorch_device)\n",
    "\n",
    "# Process the audio file\n",
    "predictions = pipe(audio_path, chunk_length_s=20, stride_length_s=5)\n",
    "\n",
    "# Extract the recognized text from the predictions\n",
    "recognized_text = predictions[\"text\"]\n",
    "print(recognized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
