{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: SpeechRecognition in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (4.10.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (69.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install moviepy SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Wave in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (0.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install Wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (4.10.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (57.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install moviepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: brew in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (0.1.3)\n",
      "Requirement already satisfied: ffmpeg in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install brew ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in chunk_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 1: tell me about yourself and I completed my B tech CSE and in VIT Chennai from Hyderabad and my skills and all language python\n",
      "MoviePy - Writing audio in chunk_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 2: what is the formula which is used for data manipulation and creation and also for data cleaning and it is a part of python what are the main methods available in Pandas to generate the data there are two main method they are series and data frame what is the series method is the\n",
      "MoviePy - Writing audio in chunk_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 3: Single Single column but with indexes and what is the data frame data frame we can create multiple columns and multiple rows at a time ok so what is the describe method in one class method is used for knowing the standard deviation and mean and the minimum value and maximum value of the data so what is the information\n",
      "MoviePy - Writing audio in chunk_3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 4: columns data type and columns how many numbers of columns present in the data frame so how to remove the double duplicates in the data set using underscore duplicate we can remove the and also we can use by Group by method to check the duplicates and remove so how to rename the column\n",
      "MoviePy - Writing audio in chunk_4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 5: the linear method we can rename the column mentioning the data frame so how to identify the null values inside the data frame null values there are three methods to identify to manipulate first first is fill in the and replace method and also I am asking\n",
      "MoviePy - Writing audio in chunk_5.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 6: regional we can use the regional and also and also is any so how to remove the duplicates sorry delete the elements do you know there are three method so how to calculate\n",
      "MoviePy - Writing audio in chunk_6.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 7: calculate the mean we can use the mean method so what is the difference between all and anything sorry how to how to read the data set in Windows using read csv we can read the csv file\n",
      "MoviePy - Writing audio in chunk_7.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 8: how to convert csv to accept yes we can convert using to using 2 to excel and giving the Excel so can I possible to connect with Pandas through database as we can connect any SQL database using Pandas and we can also manipulate the data using once you read the data set OK so the data set it is\n",
      "MoviePy - Writing audio in chunk_8.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 9: healthy or not how to check the healthy data centre using describe we can check all the so I have a one character problem so that can I have a 34 values are repeated so how to identify using value counts you can see the number of number of times the values repeated we can say\n",
      "MoviePy - Writing audio in chunk_9.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 10: what about the characteristics of the most repeated value and will replace that into the character column so in Pandas how to generate the customer\n",
      "MoviePy - Writing audio in chunk_10.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 11: using the column name and equals to the values with first the data frame then the column name then equal to the values then the column will be inserted So In basic Python you know that list what is the list list it is used with close bracket which consists of value second what is the set method using set or are also\n",
      "MoviePy - Writing audio in chunk_11.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 12: child brackets suppose you have a list of some duplicates how to delete the duplicates what is the dictionary dictionary it consist of key and values in which we can reduce the values using the keys\n",
      "MoviePy - Writing audio in chunk_12.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 13: what is I need something can you explain ok so so how to show you are missing partner what is the how to register so I have some values how to delete\n",
      "MoviePy - Writing audio in chunk_13.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 14: so same way in Pandas also ok some questions are missing so how to drop the elements how to replace how to have this practice is\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load the video\n",
    "video_path = 'videos\\DATASCIENCE  MOCK INTERVIEW _ Technical Round _ DATASCIENCE Interview _  @magneqsoftware6896 _.mp4'\n",
    "video_clip = mp.VideoFileClip(video_path)\n",
    "\n",
    "# Extract audio from the video\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Split audio into chunks if necessary\n",
    "# You may want to adjust chunk_duration as needed\n",
    "chunk_duration = 30  # seconds\n",
    "chunks = [audio_clip.subclip(t_start, min(t_start + chunk_duration, audio_clip.duration))\n",
    "          for t_start in range(0, int(audio_clip.duration), chunk_duration)]\n",
    "\n",
    "# Perform speech recognition on each chunk\n",
    "recognizer = sr.Recognizer()\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_path = f'chunk_{i}.wav'\n",
    "    chunk.write_audiofile(chunk_path)\n",
    "\n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(f\"Chunk {i+1}:\", text)\n",
    "\n",
    "# Cleanup\n",
    "audio_clip.close()\n",
    "video_clip.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in chunk_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 1: tell me about yourself and I completed my B tech CSE and in VIT Chennai from Hyderabad and my skills and all language which is used for data manipulation and creation and also for data cleaning and it is a part of what are the main method they are series and date of\n",
      "MoviePy - Writing audio in chunk_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 2: Single Single column but with indexes and what is the data frame data frame we can create multiple columns and multiple rows at a time ok so what is the described method in one class method is used for knowing the standard deviation and mean and the minimum value and maximum value of the data so what is the information is used to know the columns data type and columns how many numbers of columns present in the data so how to remove the duplicates in the dataset using underscore duplicate we can remove the and also we can use by Group by method to check the duplicates and how to\n",
      "MoviePy - Writing audio in chunk_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 3: the linear method we can rename the column mentioning the data frame so how to identify the null values inside the data frame null values there are three methods to identify to manipulate first first fill in the and replace method and also we can use the legal and also how to remove the duplicates delete and replace method so how to calculate\n",
      "MoviePy - Writing audio in chunk_3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 4: calculate the mean we can use the mean method so what is the difference between all and anything sorry how to how to read the data set in Windows using read csv we can read the csv file to convert we can convert using to using to excel and giving the excellent possible to connect to database as we can connect SQL database using Pandas and we can also manipulate the data using once you read the data set OK so the data set it is\n",
      "MoviePy - Writing audio in chunk_4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 5: healthy or not how to check the healthy data centre using describe we can check all the so I have a one character problem so that can I have a 34 values are repeated so how to identify using value counts you can see the number of number of times the values repeated we can change the most repeated value and will replace that into the character called generator\n",
      "MoviePy - Writing audio in chunk_5.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 6: using the column name and equals to the values with first the data frame then the column name then equal to the values then the column will be inserted So In basic Python you know that list what is the list list it is used with close bracket which consists of value second what is the second method using set or are also brackets suppose you have a list of some duplicates how to delete the duplicates what is the dictionary it consists of key and values in which we can reduce the values using the keys\n",
      "MoviePy - Writing audio in chunk_6.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 7: what is can you explain ok so so how to show you are missing partner is the how to register so I have some values how to delete so how to drop the elements how to request\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load the video\n",
    "video_path = 'videos\\DATASCIENCE  MOCK INTERVIEW _ Technical Round _ DATASCIENCE Interview _  @magneqsoftware6896 _.mp4'\n",
    "video_clip = mp.VideoFileClip(video_path)\n",
    "\n",
    "# Extract audio from the video\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Split audio into chunks if necessary\n",
    "# You may want to adjust chunk_duration as needed\n",
    "chunk_duration =60 # seconds\n",
    "chunks = [audio_clip.subclip(t_start, min(t_start + chunk_duration, audio_clip.duration))\n",
    "          for t_start in range(0, int(audio_clip.duration), chunk_duration)]\n",
    "\n",
    "# Perform speech recognition on each chunk\n",
    "recognizer = sr.Recognizer()\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_path = f'chunk_{i}.wav'\n",
    "    chunk.write_audiofile(chunk_path)\n",
    "\n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(f\"Chunk {i+1}:\", text)\n",
    "\n",
    "# Cleanup\n",
    "audio_clip.close()\n",
    "video_clip.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in chunk_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 1: Mary hi hello handsome and I am applying for one of your kitchen jobs here is the copy of my recipe thank you but I want to learn I work hard at home I love to learn new things very organised and I found the direction of the company actually give me a special certificate for coming to work on time every day for you and\n",
      "MoviePy - Writing audio in chunk_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 2: improve my writing skills that's great why did you leave your laptop it was graveyard and I need to work days I said I am from 8:00 a.m. and 5 p.m. ok would you have any questions for yes what kind of training is needed not a lot more questions\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load the video\n",
    "video_path = 'videos\\Job Interview_ I Want to Learn (ESL).mp4'\n",
    "video_clip = mp.VideoFileClip(video_path)\n",
    "\n",
    "# Extract aud\n",
    "# io from the video\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Split audio into chunks if necessary\n",
    "# You may want to adjust chunk_duration as needed\n",
    "chunk_duration =60 # seconds\n",
    "chunks = [audio_clip.subclip(t_start, min(t_start + chunk_duration, audio_clip.duration))\n",
    "          for t_start in range(0, int(audio_clip.duration), chunk_duration)]\n",
    "\n",
    "# Perform speech recognition on each chunk\n",
    "recognizer = sr.Recognizer()\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_path = f'chunk_{i}.wav'\n",
    "    chunk.write_audiofile(chunk_path)\n",
    "\n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(f\"Chunk {i+1}:\", text)\n",
    "\n",
    "# Cleanup\n",
    "audio_clip.close()\n",
    "video_clip.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in chunk_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 1: given an array of integers with n + 1 numbers and its ranging from 1 to N there is only one repeated number in this entire array how can you find it without modifying the array and only using constant extra space hi everyone and welcome to another mark interview with exponent I am here today with Ahmed would you like to introduce yourself one numbers in IT ranging from 1 to 10 number in this entire how can you find it without modifying the array and only using constant extra space\n",
      "MoviePy - Writing audio in chunk_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 2: OK so the Android for this question is an array of integers from 1 to N and I am required to return this element that repeated in this array right yes exactly ok so what I'm thinking about is the brute force solution which is operating through all the array two times the first one is activated through every element and the second time is comparing this element with all the other elements in the ring and if we find the duplicate we can return the message and we will be\n",
      "MoviePy - Writing audio in chunk_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 3: the first one is Looking through all the elements in Delhi and inside this folder we will have another for loop by creating through all the elements again in the rain what's starting from this end because we don't need to be repeating comparisons more than one time starting\n",
      "MoviePy - Writing audio in chunk_3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 4: and if we finish the photo without finding any element I believe you should return some invalid value like negative one example but this shouldn't be happening according to our places you always have it for now and then yeah that's so this is the first session and the believe the time complexity here is in square because we have two for loops and space complexity because we want to change the time complexity here and improve it first of all what is the time complexity right now and how would you improve\n",
      "MoviePy - Writing audio in chunk_4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 5: what's the temperature of the array and it's not the best I think the second solution to improve the time complexity is sorting the array and comparing every element with the element that after it but this will only increase the time will improve the time complexity to end login I believe that the easiest solution is used then it means that we already found another one that's the same problem\n",
      "MoviePy - Writing audio in chunk_5.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 6: we know that you mentioned in the in the problem that we can use extra space here is the tricky part we already know that the elements starting from 1 to N so we can use the array itself as a set of OK so how we will be doing this is a great things to all the elements and whenever we find the element we go to its index or the index that is equal to its value and we want this element is visited by negative\n",
      "MoviePy - Writing audio in chunk_6.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 7: is the index needs to be the element -1 but the element might be visited before so it might have negative values that we need to get the absolute value for this image and since the elements from 1 to end but the array is zero based index so we need to subtract one to be from 0 to and - 1 4 1 5 9 1\n",
      "MoviePy - Writing audio in chunk_7.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 8: other than that we need to work this element as visited so we say reflex ok every element we calculate the corresponding index which is like using Hash Function and for this and this will check if we visited this element before or not if we visited so duplicate if we didn't know how we can use that ok\n",
      "MoviePy - Writing audio in chunk_8.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 9: start with the simple one to use this function inside the main since the meaning of statistics through it had to be also sorry that has values from 12 and is 4 cm and\n",
      "MoviePy - Writing audio in chunk_9.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 10: show the applications for example if you have this duplicate but for multiple times exam believe it's like this so you only have one duplicate values duplicated for more than two times you have any suggestions for you know I think the thing that people mostly get stuck on is starting with the optimal solution and try to improve it not starting\n",
      "MoviePy - Writing audio in chunk_10.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 11: yeah exactly thank you very much really appreciate having you here today thanks so much for watching don't forget to hit the like and subscribe videos\n"
     ]
    }
   ],
   "source": [
    "#its 10.40 minutes video take time to convert into text 8.6 mintues \n",
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load the video\n",
    "video_path = 'videos\\Software Engineering Mock Interview_ Find the Duplicate Number (with Google SWE).mp4'\n",
    "video_clip = mp.VideoFileClip(video_path)\n",
    "\n",
    "# Extract audio from the video\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Split audio into chunks if necessary\n",
    "# You may want to adjust chunk_duration as needed\n",
    "chunk_duration =60 # seconds\n",
    "chunks = [audio_clip.subclip(t_start, min(t_start + chunk_duration, audio_clip.duration))\n",
    "          for t_start in range(0, int(audio_clip.duration), chunk_duration)]\n",
    "\n",
    "# Perform speech recognition on each chunk\n",
    "recognizer = sr.Recognizer()\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_path = f'chunk_{i}.wav'\n",
    "    chunk.write_audiofile(chunk_path)\n",
    "\n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(f\"Chunk {i+1}:\", text)\n",
    "\n",
    "# Cleanup\n",
    "audio_clip.close()\n",
    "video_clip.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in chunk_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in chunk_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcribed text: Mary hi hello handsome and I am applying for one of your kitchen jobs here is the copy of my recipe thank you but I want to learn I work hard at home I love to learn new things very organised and I found the direction of the company actually give me a special certificate for coming to work on time every day for you and\n",
      "improve my writing skills that's great why did you leave your laptop it was graveyard and I need to work days I said I am from 8:00 a.m. and 5 p.m. ok would you have any questions for yes what kind of training is needed not a lot more questions\n",
      "Word count: 116\n"
     ]
    }
   ],
   "source": [
    "#its 1.45 mintues video take time convert into text 2.79 minutes\n",
    "import os\n",
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "def transcribe_video(video_path, chunk_duration=60):\n",
    "    # Load the video\n",
    "    video_clip = mp.VideoFileClip(video_path)\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    # Split audio into chunks\n",
    "    chunks = [audio_clip.subclip(t_start, min(t_start + chunk_duration, audio_clip.duration))\n",
    "              for t_start in range(0, int(audio_clip.duration), chunk_duration)]\n",
    "\n",
    "    # Perform speech recognition on each chunk\n",
    "    recognizer = sr.Recognizer()\n",
    "    transcribed_texts = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_path = f'chunk_{i}.wav'\n",
    "        chunk.write_audiofile(chunk_path)\n",
    "\n",
    "        with sr.AudioFile(chunk_path) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            transcribed_texts.append(text)\n",
    "\n",
    "        # Remove the chunk file\n",
    "        os.remove(chunk_path)\n",
    "\n",
    "    # Combine text from different chunks\n",
    "    combined_text = '\\n'.join(transcribed_texts)\n",
    "    word_count = len(combined_text.split())\n",
    "\n",
    "    # Cleanup\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n",
    "\n",
    "    return combined_text, word_count\n",
    "\n",
    "# Example usage\n",
    "video_path = 'videos\\Job Interview_ I Want to Learn (ESL).mp4'\n",
    "transcribed_text, word_count = transcribe_video(video_path)\n",
    "print(\"Transcribed text:\", transcribed_text)\n",
    "print(\"Word count:\", word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepspeech\n",
      "  Downloading deepspeech-0.9.3-cp39-cp39-win_amd64.whl (8.0 MB)\n",
      "Requirement already satisfied: numpy>=1.19.4 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from deepspeech) (1.26.4)\n",
      "Installing collected packages: deepspeech\n",
      "Successfully installed deepspeech-0.9.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install deepspeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in newdata.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "An error occurred: name 'deepspeech' is not defined\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import moviepy.editor as mp\n",
    "#import deepspeech \n",
    "\n",
    "import deepspeech as ds\n",
    "\n",
    "# Step 1: Load the video and extract audio\n",
    "video_path = 'videos\\Job Interview_ I Want to Learn (ESL).mp4'\n",
    "video_clip = mp.VideoFileClip(video_path)\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Step 2: Save audio to a temporary file\n",
    "temp_audio_path = 'newdata.wav'\n",
    "audio_clip.write_audiofile(temp_audio_path)\n",
    "\n",
    "# Step 3: Initialize DeepSpeech model\n",
    "#model_path=ds.Model(r\"..\\deepspeech\\deepspeech-0.7.3-models.pbmm\")\n",
    "model_path ='path/to/deepspeech-0.9.3-models.pbmm'\n",
    "scorer_path = 'path/to/deepspeech-0.9.3-models.scorer'\n",
    "\n",
    "try:\n",
    "    model = deepspeech.Model(model_path)\n",
    "    model.enableExternalScorer(scorer_path)\n",
    "\n",
    "    # Step 4: Transcribe audio to text using DeepSpeech\n",
    "    with open(temp_audio_path, 'rb') as audio_file:\n",
    "        audio_data = audio_file.read()\n",
    "        text = model.stt(audio_data)\n",
    "\n",
    "    print(\"Transcribed text:\", text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "finally:\n",
    "    # Step 5: Cleanup\n",
    "    os.remove(temp_audio_path)\n",
    "\n",
    "    # Close video and audio clips\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in newspeech.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "#this method take less time convert into text . its fast \n",
    "import wave, math, contextlib\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import AudioFileClip\n",
    "transcribed_audio_file_name = \"newspeech.wav\"\n",
    "zoom_video_file_name = \"videos\\Job Interview_ I Want to Learn (ESL).mp4\"\n",
    "audioclip = AudioFileClip(zoom_video_file_name)\n",
    "audioclip.write_audiofile(transcribed_audio_file_name)\n",
    "with contextlib.closing(wave.open(transcribed_audio_file_name,'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)\n",
    "total_duration = math.ceil(duration / 60)\n",
    "r = sr.Recognizer()\n",
    "for i in range(0, total_duration):\n",
    "    with sr.AudioFile(transcribed_audio_file_name) as source:\n",
    "        audio = r.record(source, offset=i*60, duration=60)\n",
    "    f = open(\"transcription.txt\", \"a\")\n",
    "    f.write(r.recognize_google(audio))\n",
    "    f.write(\" \")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in Software_Engineering.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "import wave, math, contextlib\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import AudioFileClip\n",
    "transcribed_audio_file_name = \"Software_Engineering.wav\"\n",
    "zoom_video_file_name = \"videos\\Softwar.mp4\"\n",
    "audioclip = AudioFileClip(zoom_video_file_name)\n",
    "audioclip.write_audiofile(transcribed_audio_file_name)\n",
    "with contextlib.closing(wave.open(transcribed_audio_file_name,'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)\n",
    "total_duration = math.ceil(duration / 60)\n",
    "r = sr.Recognizer()\n",
    "for i in range(0, total_duration):\n",
    "    with sr.AudioFile(transcribed_audio_file_name) as source:\n",
    "        audio = r.record(source, offset=i*60, duration=60)\n",
    "    f = open(\"Software_Engineering.txt\", \"a\")\n",
    "    f.write(r.recognize_google(audio))\n",
    "    f.write(\" \")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in DATASCIENCE.mp4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "import wave, math, contextlib\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import AudioFileClip\n",
    "transcribed_audio_file_name = \"DATASCIENCE.mp4.wav\"\n",
    "zoom_video_file_name = \"videos\\DATASCIENCE.mp4\"\n",
    "audioclip = AudioFileClip(zoom_video_file_name)\n",
    "audioclip.write_audiofile(transcribed_audio_file_name)\n",
    "with contextlib.closing(wave.open(transcribed_audio_file_name,'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)\n",
    "total_duration = math.ceil(duration / 60)\n",
    "r = sr.Recognizer()\n",
    "for i in range(0, total_duration):\n",
    "    with sr.AudioFile(transcribed_audio_file_name) as source:\n",
    "        audio = r.record(source, offset=i*60, duration=60)\n",
    "    f = open(\"DATASCIENCE.txt\", \"a\")\n",
    "    f.write(r.recognize_google(audio))\n",
    "    f.write(\" \")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcript:\n",
      " Mary Hi hello and I am applying for one of your kitchen jobs here is the copy of my recipe thank you but I want to learn I work hard at home I love to learn new things very organised and I found the company actually give me a special certificate for coming to work on time every time you so much for your time thank you\n",
      "Word Count: 67\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "def extract_audio(video_file, audio_file):\n",
    "    video_clip = mp.VideoFileClip(video_file)\n",
    "    audio_clip = video_clip.audio\n",
    "    audio_clip.write_audiofile(audio_file)\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand the audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"videos\\Job Interview_ I Want to Learn (ESL).mp4\"  # Provide the path to your video file\n",
    "    audio_file = \"temp_audio.wav\"  # Temporary audio file path\n",
    "\n",
    "    extract_audio(video_file, audio_file)\n",
    "    transcript = transcribe_audio(audio_file)\n",
    "    \n",
    "    if transcript:\n",
    "        word_count = count_words(transcript)\n",
    "        print(\"Transcript:\\n\", transcript)\n",
    "        print(\"Word Count:\", word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in newlearn.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Total Word Count: 128\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import math\n",
    "import contextlib\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import AudioFileClip\n",
    "\n",
    "transcribed_audio_file_name = \"newlearn.wav\"\n",
    "zoom_video_file_name = r\"videos\\Job Interview_ I Want to Learn (ESL).mp4\"\n",
    "\n",
    "# Extract audio from video\n",
    "audioclip = AudioFileClip(zoom_video_file_name)\n",
    "audioclip.write_audiofile(transcribed_audio_file_name)\n",
    "\n",
    "# Calculate duration\n",
    "with contextlib.closing(wave.open(transcribed_audio_file_name, 'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)\n",
    "\n",
    "total_duration = math.ceil(duration / 60)\n",
    "\n",
    "# Transcribe audio in chunks of 60 seconds and count words\n",
    "word_count = 0\n",
    "r = sr.Recognizer()\n",
    "\n",
    "for i in range(0, total_duration):\n",
    "    with sr.AudioFile(transcribed_audio_file_name) as source:\n",
    "        audio = r.record(source, offset=i*60, duration=60)\n",
    "    \n",
    "    # Transcribe audio\n",
    "    transcribed_text = r.recognize_google(audio)\n",
    "\n",
    "    # Count words\n",
    "    words = transcribed_text.split()\n",
    "    word_count += len(words)\n",
    "\n",
    "    # Write transcribed text to file\n",
    "    with open(\"newDATASCIENCE.txt\", \"a\") as f:\n",
    "        f.write(transcribed_text)\n",
    "        f.write(\" \")\n",
    "\n",
    "# Print word count\n",
    "print(\"Total Word Count:\", word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in teio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcript:\n",
      " Mary hi hello handsome and I am applying for one of your kitchen jobs here is the copy of my recipe thank you but I want to learn I work hard at home I love to learn new things very organised and I follow directions\n",
      "Reference Text:\n",
      " Your reference text goes here\n",
      "Accuracy: 0.2\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "import Levenshtein\n",
    "\n",
    "def extract_audio(video_file, audio_file):\n",
    "    video_clip = mp.VideoFileClip(video_file)\n",
    "    audio_clip = video_clip.audio\n",
    "    audio_clip.write_audiofile(audio_file)\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand the audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "\n",
    "# def calculate_accuracy(reference_text, transcribed_text):\n",
    "#     distance = Levenshtein.distance(reference_text, transcribed_text)\n",
    "#     accuracy = 1 - (distance / len(reference_text))\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "def calculate_accuracy(reference_text, transcribed_text):\n",
    "    reference_words = reference_text.split()\n",
    "    transcribed_words = transcribed_text.split()\n",
    "\n",
    "    common_words = set(reference_words) & set(transcribed_words)\n",
    "    accuracy = len(common_words) / len(reference_words)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"videos\\Job Interview_ I Want to Learn (ESL).mp4\"  # Provide the path to your video file\n",
    "    audio_file = \"teio.wav\"  # Temporary audio file path\n",
    "    reference_text = \"Your reference text goes here\"  # Actual text of what is being said in the video\n",
    "\n",
    "    extract_audio(video_file, audio_file)\n",
    "    transcript = transcribe_audio(audio_file)\n",
    "    \n",
    "    if transcript:\n",
    "        print(\"Transcript:\\n\", transcript)\n",
    "        print(\"Reference Text:\\n\", reference_text)\n",
    "        accuracy = calculate_accuracy(reference_text, transcript)\n",
    "        print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting whisper\n",
      "  Using cached whisper-1.1.10-py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from whisper) (1.16.0)\n",
      "Installing collected packages: whisper\n",
      "Successfully installed whisper-1.1.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyDub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: PyDub\n",
      "Successfully installed PyDub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install PyDub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyDub in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (0.25.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pip install PyDub --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\shamshad ahmed\\appdata\\local\\temp\\pip-req-build-fel8wdtl\n",
      "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.1-cp39-cp39-win_amd64.whl (198.5 MB)\n",
      "Collecting numba\n",
      "  Downloading numba-0.59.1-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (1.26.4)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.6.0-cp39-cp39-win_amd64.whl (798 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (4.66.2)\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0\n",
      "  Downloading llvmlite-0.42.0-cp39-cp39-win_amd64.whl (28.1 MB)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2023.12.25-cp39-cp39-win_amd64.whl (269 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (4.10.0)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm->openai-whisper==20231117) (0.4.6)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (PEP 517): started\n",
      "  Building wheel for openai-whisper (PEP 517): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=812161 sha256=784bf3fc97a644aca66185ddbf2618faf82e6c96272e5d0716719c241a628ff8\n",
      "  Stored in directory: C:\\Users\\Shamshad ahmed\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-5ma7k1fb\\wheels\\fe\\03\\29\\e7919208d11b4ab32972cb448bb84a9a675d92cd52c9a48341\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: mpmath, sympy, regex, networkx, llvmlite, fsspec, filelock, torch, tiktoken, numba, more-itertools, openai-whisper\n",
      "Successfully installed filelock-3.13.1 fsspec-2024.3.1 llvmlite-0.42.0 more-itertools-10.2.0 mpmath-1.3.0 networkx-3.2.1 numba-0.59.1 openai-whisper-20231117 regex-2023.12.25 sympy-1.12 tiktoken-0.6.0 torch-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/openai/whisper.git 'C:\\Users\\Shamshad ahmed\\AppData\\Local\\Temp\\pip-req-build-fel8wdtl'\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/openai/whisper.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"choco\" - maybe you meant \"check\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip choco install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"scoop\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip scoop install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools-rust\n",
      "  Downloading setuptools_rust-1.9.0-py3-none-any.whl (26 kB)\n",
      "Collecting tomli>=1.2.1\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting semantic-version<3,>=2.8.2\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting setuptools>=62.4\n",
      "  Using cached setuptools-69.2.0-py3-none-any.whl (821 kB)\n",
      "Installing collected packages: tomli, setuptools, semantic-version, setuptools-rust\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 57.4.0\n",
      "    Uninstalling setuptools-57.4.0:\n",
      "      Successfully uninstalled setuptools-57.4.0\n",
      "Successfully installed semantic-version-2.10.0 setuptools-69.2.0 setuptools-rust-1.9.0 tomli-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install setuptools-rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     31\u001b[0m     audio_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mJob Interview_ I Want to Learn (ESL).mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your audio file path\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[43msplit_audio_into_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[0;32m     35\u001b[0m         chunk\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 20\u001b[0m, in \u001b[0;36msplit_audio_into_chunks\u001b[1;34m(audio_file_path, chunk_duration)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_audio_into_chunks\u001b[39m(audio_file_path, chunk_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m---> 20\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mAudioSegment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Split the audio into chunks of specified duration\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m split_on_silence(audio, silence_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m40\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\pydub\\audio_segment.py:728\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[1;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 728\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[43mmediainfo_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_ahead_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_ahead_limit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[0;32m    730\u001b[0m     audio_streams \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    731\u001b[0m                      \u001b[38;5;28;01mif\u001b[39;00m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodec_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\pydub\\utils.py:274\u001b[0m, in \u001b[0;36mmediainfo_json\u001b[1;34m(filepath, read_ahead_limit)\u001b[0m\n\u001b[0;32m    271\u001b[0m         file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    273\u001b[0m command \u001b[38;5;241m=\u001b[39m [prober, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-of\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m command_args\n\u001b[1;32m--> 274\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstdin_parameter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m output, stderr \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mstdin_data)\n\u001b[0;32m    276\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py:1420\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1422\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1433\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1436\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1437\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "def transcribe_audio_chunk(audio_chunk):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_chunk) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Web Speech API could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Web Speech API; {e}\")\n",
    "\n",
    "def split_audio_into_chunks(audio_file_path, chunk_duration=30):\n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "\n",
    "    # Split the audio into chunks of specified duration\n",
    "    chunks = split_on_silence(audio, silence_thresh=-40)\n",
    "\n",
    "    # Ensure each chunk is at least the specified duration\n",
    "    chunks = [chunk if len(chunk) >= chunk_duration * 1000 else chunk + AudioSegment.silent(duration=chunk_duration * 1000 - len(chunk)) for chunk in chunks]\n",
    "\n",
    "    return chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file_path = \"videos\\Job Interview_ I Want to Learn (ESL).mp4\"  # Replace with your audio file path\n",
    "    chunks = split_audio_into_chunks(audio_file_path)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.export(f\"chunk_{i + 1}.wav\", format=\"wav\")\n",
    "        text = transcribe_audio_chunk(f\"chunk_{i + 1}.wav\")\n",
    "        print(f\"Chunk {i + 1} Text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.2.1 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# load audio and pad/trim it to fit 30 seconds\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mShamshad ahmed\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mobject detection\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m audio \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mpad_or_trim(audio)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# make log-Mel spectrogram and move to the same device as the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\whisper\\audio.py:58\u001b[0m, in \u001b[0;36mload_audio\u001b[1;34m(file, sr)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# fmt: on\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m    503\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m--> 505\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    507\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py:1420\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1422\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1433\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1436\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1437\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os \n",
    "import pandas as pd\n",
    "import whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"C:\\\\Users\\\\Shamshad ahmed\\\\object detection\\\\temp_audio.wav\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions(fp16 = False)\n",
    "#options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscribed_speech.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#print(results[0]['text'])\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#print(results[1]['text'])\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[1;32mc:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\whisper\\transcribe.py:133\u001b[0m, in \u001b[0;36mtranscribe\u001b[1;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[0;32m    130\u001b[0m     decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Pad 30-seconds of silence to the input audio, for slicing\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[43mlog_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_SAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m content_frames \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m N_FRAMES\n\u001b[0;32m    135\u001b[0m content_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(content_frames \u001b[38;5;241m*\u001b[39m HOP_LENGTH \u001b[38;5;241m/\u001b[39m SAMPLE_RATE)\n",
      "File \u001b[1;32mc:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\whisper\\audio.py:141\u001b[0m, in \u001b[0;36mlog_mel_spectrogram\u001b[1;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(audio, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    140\u001b[0m         audio \u001b[38;5;241m=\u001b[39m load_audio(audio)\n\u001b[1;32m--> 141\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     audio \u001b[38;5;241m=\u001b[39m audio\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected np.ndarray (got list)"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import numpy as np\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "results = model.transcribe([\"transcribed_speech.wav\"])\n",
    "#print(results[0]['text'])\n",
    "#print(results[1]['text'])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyannote.core\n",
      "  Downloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
      "Collecting sortedcontainers>=2.0.4\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.core) (1.26.4)\n",
      "Collecting scipy>=1.1\n",
      "  Downloading scipy-1.12.0-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.core) (4.10.0)\n",
      "Installing collected packages: sortedcontainers, scipy, pyannote.core\n",
      "Successfully installed pyannote.core-5.0.0 scipy-1.12.0 sortedcontainers-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pyannote.core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyannote.audioNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading pyannote.audio-3.1.1-py2.py3-none-any.whl (208 kB)\n",
      "Collecting huggingface-hub>=0.13.0\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Collecting lightning>=2.0.1\n",
      "  Downloading lightning-2.2.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting einops>=0.6.0\n",
      "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "Collecting torch-audiomentations>=0.11.0\n",
      "  Downloading torch_audiomentations-0.11.1-py3-none-any.whl (50 kB)\n",
      "Collecting pyannote.pipeline>=3.0.1\n",
      "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
      "Collecting rich>=12.0.0\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Collecting omegaconf<3.0,>=2.1\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Collecting pyannote.metrics>=3.2\n",
      "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
      "Collecting tensorboardX>=2.6\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: pyannote.core>=5.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.audio) (5.0.0)\n",
      "Collecting torchaudio>=2.0.0\n",
      "  Downloading torchaudio-2.2.1-cp39-cp39-win_amd64.whl (2.4 MB)\n",
      "Collecting soundfile>=0.12.1\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Collecting asteroid-filterbanks>=0.4\n",
      "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
      "Collecting pytorch-metric-learning>=2.1.0\n",
      "  Downloading pytorch_metric_learning-2.4.1-py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.audio) (2.2.1)\n",
      "Collecting pyannote.database>=5.0.1\n",
      "  Downloading pyannote.database-5.0.1-py3-none-any.whl (48 kB)\n",
      "Collecting speechbrain>=0.5.14\n",
      "  Downloading speechbrain-1.0.0-py3-none-any.whl (760 kB)\n",
      "Collecting semver>=3.0.0\n",
      "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Collecting torchmetrics>=0.11.0\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (4.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.31.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (3.13.1)\n",
      "Collecting lightning-utilities<2.0,>=0.8.0\n",
      "  Downloading lightning_utilities-0.11.0-py3-none-any.whl (25 kB)\n",
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Using cached aiohttp-3.9.3-cp39-cp39-win_amd64.whl (366 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.5-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (23.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.4-cp39-cp39-win_amd64.whl (76 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.1-cp39-cp39-win_amd64.whl (50 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from lightning-utilities<2.0,>=0.8.0->lightning>=2.0.1->pyannote.audio) (69.2.0)\n",
      "Collecting antlr4-python3-runtime==4.9.*\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=1.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.12.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.4 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
      "Requirement already satisfied: pandas>=0.19 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.1)\n",
      "Collecting typer[all]>=0.2.1\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2024.1)\n",
      "Requirement already satisfied: sympy>=1.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.12)\n",
      "Collecting matplotlib>=2.0.0\n",
      "  Downloading matplotlib-3.8.3-cp39-cp39-win_amd64.whl (7.6 MB)\n",
      "Collecting scikit-learn>=0.17.1\n",
      "  Downloading scikit_learn-1.4.1.post1-cp39-cp39-win_amd64.whl (10.6 MB)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting docopt>=0.6.2\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (10.2.0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.5-cp39-cp39-win_amd64.whl (56 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.2.0-cp39-cp39-win_amd64.whl (181 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.50.0-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.18.1)\n",
      "Collecting optuna>=3.1\n",
      "  Downloading optuna-3.6.0-py3-none-any.whl (379 kB)\n",
      "Collecting sqlalchemy>=1.3.0\n",
      "  Downloading SQLAlchemy-2.0.28-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "Collecting Mako\n",
      "  Using cached Mako-1.3.2-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from rich>=12.0.0->pyannote.audio) (2.17.2)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Collecting cffi>=1.0\n",
      "  Downloading cffi-1.16.0-cp39-cp39-win_amd64.whl (181 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting hyperpyyaml\n",
      "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.0.3-cp39-cp39-win_amd64.whl (290 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio) (1.3.0)\n",
      "Collecting protobuf>=3.20\n",
      "  Downloading protobuf-5.26.0-cp39-cp39-win_amd64.whl (420 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch>=2.0.0->pyannote.audio) (3.1.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch>=2.0.0->pyannote.audio) (3.2.1)\n",
      "Collecting torch-pitch-shift>=1.2.2\n",
      "  Downloading torch_pitch_shift-1.2.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting julius<0.3,>=0.2.3\n",
      "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
      "Collecting librosa>=0.6.0\n",
      "  Downloading librosa-0.10.1-py3-none-any.whl (253 kB)\n",
      "Collecting pooch>=1.0\n",
      "  Downloading pooch-1.8.1-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.59.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (4.4.2)\n",
      "Collecting msgpack>=1.0\n",
      "  Downloading msgpack-1.0.8-cp39-cp39-win_amd64.whl (75 kB)\n",
      "Collecting lazy-loader>=0.1\n",
      "  Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Collecting audioread>=2.1.9\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Collecting soxr>=0.3.2\n",
      "  Downloading soxr-0.3.7-cp39-cp39-win_amd64.whl (184 kB)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from numba>=0.51.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pooch>=1.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2024.2.2)\n",
      "Collecting primePy>=1.3\n",
      "  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.13.0->pyannote.audio) (0.4.6)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting shellingham<2.0.0,>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting ruamel.yaml>=0.17.28\n",
      "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
      "Collecting ruamel.yaml.clib>=0.2.7\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp39-cp39-win_amd64.whl (118 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jinja2->torch>=2.0.0->pyannote.audio) (2.1.5)\n",
      "Using legacy 'setup.py install' for julius, since package 'wheel' is not installed.\n",
      "Installing collected packages: multidict, mdurl, frozenlist, yarl, pycparser, markdown-it-py, greenlet, click, async-timeout, aiosignal, typer, threadpoolctl, sqlalchemy, shellingham, ruamel.yaml.clib, rich, Mako, lightning-utilities, joblib, cffi, aiohttp, torchmetrics, torchaudio, soxr, soundfile, scikit-learn, ruamel.yaml, pyyaml, pyparsing, primePy, pooch, msgpack, lazy-loader, kiwisolver, importlib-resources, fonttools, cycler, contourpy, colorlog, audioread, alembic, torch-pitch-shift, tabulate, sentencepiece, pytorch-lightning, pyannote.database, protobuf, optuna, matplotlib, librosa, julius, hyperpyyaml, huggingface-hub, docopt, antlr4-python3-runtime, torch-audiomentations, tensorboardX, speechbrain, semver, pytorch-metric-learning, pyannote.pipeline, pyannote.metrics, omegaconf, lightning, einops, asteroid-filterbanks, pyannote.audio\n",
      "    Running setup.py install for julius: started\n",
      "    Running setup.py install for julius: finished with status 'done'\n",
      "Successfully installed Mako-1.3.2 aiohttp-3.9.3 aiosignal-1.3.1 alembic-1.13.1 antlr4-python3-runtime-4.9.3 asteroid-filterbanks-0.4.0 async-timeout-4.0.3 audioread-3.0.1 cffi-1.16.0 click-8.1.7 colorlog-6.8.2 contourpy-1.2.0 cycler-0.12.1 docopt-0.6.2 einops-0.7.0 fonttools-4.50.0 frozenlist-1.4.1 greenlet-3.0.3 huggingface-hub-0.21.4 hyperpyyaml-1.2.2 importlib-resources-6.4.0 joblib-1.3.2 julius-0.2.7 kiwisolver-1.4.5 lazy-loader-0.3 librosa-0.10.1 lightning-2.2.1 lightning-utilities-0.11.0 markdown-it-py-3.0.0 matplotlib-3.8.3 mdurl-0.1.2 msgpack-1.0.8 multidict-6.0.5 omegaconf-2.3.0 optuna-3.6.0 pooch-1.8.1 primePy-1.3 protobuf-5.26.0 pyannote.audio-3.1.1 pyannote.database-5.0.1 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pycparser-2.21 pyparsing-3.1.2 pytorch-lightning-2.2.1 pytorch-metric-learning-2.4.1 pyyaml-6.0.1 rich-13.7.1 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 scikit-learn-1.4.1.post1 semver-3.0.2 sentencepiece-0.2.0 shellingham-1.5.4 soundfile-0.12.1 soxr-0.3.7 speechbrain-1.0.0 sqlalchemy-2.0.28 tabulate-0.9.0 tensorboardX-2.6.2.2 threadpoolctl-3.4.0 torch-audiomentations-0.11.1 torch-pitch-shift-1.2.4 torchaudio-2.2.1 torchmetrics-1.3.2 typer-0.9.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "pip install pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (20231117)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper) (0.6.0)\n",
      "Requirement already satisfied: numba in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper) (0.59.1)\n",
      "Requirement already satisfied: torch in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper) (2.2.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper) (4.66.2)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper) (10.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper) (1.26.4)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from numba->openai-whisper) (0.42.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper) (3.2.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper) (4.10.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper) (3.1.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper) (3.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not download 'pyannote/speaker-diarization-3.1' pipeline.\n",
      "It might be because the pipeline is private or gated so make\n",
      "sure to authenticate. Visit https://hf.co/settings/tokens to\n",
      "create your access token and retry with:\n",
      "\n",
      "   >>> Pipeline.from_pretrained('pyannote/speaker-diarization-3.1',\n",
      "   ...                          use_auth_token=YOUR_AUTH_TOKEN)\n",
      "\n",
      "If this still does not work, it might be because the pipeline is gated:\n",
      "visit https://hf.co/pyannote/speaker-diarization-3.1 to accept the user conditions.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyannote/speaker-diarization-3.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# inference on the whole file\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnewspeech.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# inference on an excerpt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyannote\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Segment\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "  \n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\")\n",
    "\n",
    "# inference on the whole file\n",
    "pipeline(\"newspeech.wav\")\n",
    "\n",
    "# inference on an excerpt\n",
    "from pyannote.core import Segment\n",
    "excerpt = Segment(start=2.0, end=5.0)\n",
    "\n",
    "from pyannote.audio import Audio\n",
    "waveform, sample_rate = Audio().crop(\"file.wav\", excerpt)\n",
    "pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingsoundNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Shamshad ahmed\\\\object detection\\\\myenv\\\\Lib\\\\site-packages\\\\~apidfuzz\\\\distance\\\\metrics_cpp_avx2.cp39-win_amd64.pyd'\n",
      "Check the permissions.\n",
      "\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading huggingsound-0.1.6-py3-none-any.whl (28 kB)\n",
      "Collecting datasets<3.0.0,>=2.6.1\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Collecting jiwer<3.0.0,>=2.5.1\n",
      "  Downloading jiwer-2.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting torch!=1.12.0,<1.13.0,>=1.7\n",
      "  Downloading torch-1.12.1-cp39-cp39-win_amd64.whl (161.8 MB)\n",
      "Collecting librosa<0.10.0,>=0.9.2\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "Collecting transformers<5.0.0,>=4.23.1\n",
      "  Downloading transformers-4.39.0-py3-none-any.whl (8.8 MB)\n",
      "Collecting fsspec[http]<=2024.2.0,>=2023.1.0\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (0.21.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (4.66.2)\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (2.31.0)\n",
      "Collecting pyarrow>=12.0.0\n",
      "  Downloading pyarrow-15.0.2-cp39-cp39-win_amd64.whl (24.9 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (3.13.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (2.2.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (3.9.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (1.26.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.6.1->huggingsound) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.6.1->huggingsound) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.6.1->huggingsound) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.6.1->huggingsound) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.6.1->huggingsound) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.6.1->huggingsound) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets<3.0.0,>=2.6.1->huggingsound) (4.10.0)\n",
      "Collecting rapidfuzz==2.13.7\n",
      "  Downloading rapidfuzz-2.13.7-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jiwer<3.0.0,>=2.5.1->huggingsound) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from click<9.0.0,>=8.1.3->jiwer<3.0.0,>=2.5.1->huggingsound) (0.4.6)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (0.12.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (1.4.1.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (3.0.1)\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (4.4.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (1.12.0)\n",
      "Requirement already satisfied: numba>=0.45.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (0.59.1)\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (1.8.1)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (1.3.2)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from numba>=0.45.1->librosa<0.10.0,>=0.9.2->huggingsound) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pooch>=1.0->librosa<0.10.0,>=0.9.2->huggingsound) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.6.1->huggingsound) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.6.1->huggingsound) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.6.1->huggingsound) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.6.1->huggingsound) (2.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from scikit-learn>=0.19.1->librosa<0.10.0,>=0.9.2->huggingsound) (3.4.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from soundfile>=0.10.2->librosa<0.10.0,>=0.9.2->huggingsound) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.10.0,>=0.9.2->huggingsound) (2.21)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.2-cp39-none-win_amd64.whl (2.2 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.23.1->huggingsound) (2023.12.25)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.2-cp39-none-win_amd64.whl (269 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.6.1->huggingsound) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.6.1->huggingsound) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.6.1->huggingsound) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets<3.0.0,>=2.6.1->huggingsound) (1.16.0)\n",
      "Installing collected packages: fsspec, dill, xxhash, tokenizers, safetensors, resampy, rapidfuzz, pyarrow-hotfix, pyarrow, multiprocess, transformers, torch, librosa, jiwer, datasets, huggingsound\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.1\n",
      "    Uninstalling fsspec-2024.3.1:\n",
      "      Successfully uninstalled fsspec-2024.3.1\n",
      "  Attempting uninstall: rapidfuzz\n",
      "    Found existing installation: rapidfuzz 3.6.2\n",
      "    Uninstalling rapidfuzz-3.6.2:\n",
      "      Successfully uninstalled rapidfuzz-3.6.2\n"
     ]
    }
   ],
   "source": [
    "pip install huggingsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'huggingsound'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingsound\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpeechRecognitionModel\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m SpeechRecognitionModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjonatasgrosman/wav2vec2-large-xlsr-53-english\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m audio_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftware_Engineering.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'huggingsound'"
     ]
    }
   ],
   "source": [
    "from huggingsound import SpeechRecognitionModel\n",
    "\n",
    "model = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\n",
    "audio_paths = [\"Software_Engineering.wav\"]\n",
    "\n",
    "transcriptions = model.transcribe(audio_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "LANG_ID = \"en\"\n",
    "MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "SAMPLES = 10\n",
    "\n",
    "test_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to read the audio files as arrays\n",
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n",
    "    batch[\"speech\"] = speech_array\n",
    "    batch[\"sentence\"] = batch[\"sentence\"].upper()\n",
    "    return batch\n",
    "\n",
    "test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
    "inputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "predicted_sentences = processor.batch_decode(predicted_ids)\n",
    "\n",
    "for i, predicted_sentence in enumerate(predicted_sentences):\n",
    "    print(\"-\" * 100)\n",
    "    print(\"Reference:\", test_dataset[i][\"sentence\"])\n",
    "    print(\"Prediction:\", predicted_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: hello all my name is krish Naik and welcome to my YouTube channel so guys yes this particular video is again related to some advice and guidance that I really want to provide you all and yes in this video I am going to talk about AI I want to go to provide you some kind of advice at least to start learning AI and there is a reason why I am specifically saying you this I am not saying that ok and goal should be probably making a transition working in some AI companies analytics industry not as such but start incorporating AI in your day to day life evolving a lot of new things are probably coming up initially we had machine learning deep learning not generally models and probably in the upcoming two years definitely a lot of companies are come going to come up with different different Startup ideas just using this kind of LLB models where they are specifically solving some kind of problem ok but my advice will be that you know start learning AI to incorporate the capabilities that you can actually put in your day to day activities you know as you know data data Speaks a lot you know and if you probably know AI machine learning deep learning trust me you will be able to explore many more information from that specific data and not saying that ok learn just to get a job but instead learn to make yourself more productive and you can definitely do that I know many people are working in different domains different technologies in different programming language they have been a different work but at least have an idea about AI start incorporating that in your life you may be thinking Crush you are a youtuber you are doing it for your purpose you know you may gain subscribers you may probably bring up your telling people to buy courses I am not saying nothing as such I am saying that wherever you get some sources learning at not my channel at least from somewhere else is so many open source documentation that are available you know once you start incorporating it trust me opportunities are them anymore in the world not only see if you don't want to work anyway at least use it in your personal life right you have your financial data right you have your let's say I'll give you one example one of my friend is call me this morning right he was saying the crush I am running a business you know I have this specific use cases and this is only possible because of machine learning can you help me out in this you know that I also want to learn this now let it be a business create you are a person who is running a business you are a person who is working on some of the other thing and there if you get a bit of chances of automatic things trust me I will come into picture ok and this is super important this is the advice that I really want to give to everyone ok my main aim is to democracy this AI education to everyone that is the reason why I have come up with this YouTube channel when I specifically upload videos related to everything that is in Ai tomorrow anything that probably comes I will be explaining you I will be teaching you I will be showing you multiple examples you know yeah it is up to you whether you want to make it as a full-time opportunity whether you want to learn it in such a way that you may probably get a job but start incorporating it is up to you guys I am not forcing you but starting it try to use it in your day to day practices you will be able to see the changes have seen some of my friends getting productive you know I have my cousin brother who is working in US and from past two years you know he is working his architect working with something else some other Technology some other domain but still he has started using AI in his day to day activities and this is the most important advice that I really want to give it to you right again my to democracy education to everyone that is the reason I am uploading this many number of videos with respect to learning anything it is up to you find out any sources but start learning this is the advice that I really want to give it to you because you will be seeing how much changes it is going to come up in the upcoming 2 years right now lines in is going on you can actually create your own LLM models you know I was just solving a use case right now what are documents I have let's say I want to I want to probably create an LLM model with respect to my data and I was seeing that I had a 2GB of files that is present PDF files which are a lot of content I was able to train my own chat box so this kind of examples will definitely come up I'll show you how you can probably train it right but I want to do it for my day today let's say that I have my Excel sheet of all the expenditures all the expenses that I am doing right in some format right I can also train that specific thing Malayalam models can I give an advice where I can probably save some money right this is just one some of the examples that I really want to come up with in front of you right lot of applications will be there and definitely do make sure that you take up this advice take it seriously not for making not for working full time at least but at least try to incorporate this knowledge in your life so yes I hope you like this particular video if you like this I'll see all in the next video I have a great day thank you wonder take care\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio_data)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def split_audio(input_audio):\n",
    "    audio = AudioSegment.from_file(input_audio)\n",
    "    segment_duration_ms = 60 * 1000  # 1 minute in milliseconds\n",
    "    segments = []\n",
    "\n",
    "    for start_time in range(0, len(audio), segment_duration_ms):\n",
    "        segment = audio[start_time:start_time + segment_duration_ms]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def main(input_audio):\n",
    "    segments = split_audio(input_audio)\n",
    "    transcriptions = []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment.export(f\"segment_{i}.wav\", format=\"wav\")\n",
    "        transcription = transcribe_audio(f\"segment_{i}.wav\")\n",
    "        transcriptions.append(transcription)\n",
    "        os.remove(f\"segment_{i}.wav\")\n",
    "\n",
    "    combined_transcription = \" \".join(transcriptions)\n",
    "    return combined_transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_audio = \"DATASCIENCE.mp4.wav\"  # Path to your input audio file\n",
    "    transcription = main(input_audio)\n",
    "    print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: moviepy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (4.10.0)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.66.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (69.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "MoviePy couldn't find the codec associated with the filename. Provide the 'codec' parameter in write_audiofile.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\moviepy\\audio\\AudioClip.py:200\u001b[0m, in \u001b[0;36mAudioClip.write_audiofile\u001b[1;34m(self, filename, fps, nbytes, buffersize, codec, bitrate, ffmpeg_params, write_logfile, verbose, logger)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     codec \u001b[38;5;241m=\u001b[39m \u001b[43mextensions_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodec\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: ''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#convert to audio\u001b[39;00m\n\u001b[0;32m      5\u001b[0m video \u001b[38;5;241m=\u001b[39m mpe\u001b[38;5;241m.\u001b[39mVideoFileClip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDevin AI Capabilities,What Can First AI Software Engineer Do_ Future Of Software Engineering.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mvideo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_audiofile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdavin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<decorator-gen-63>:2\u001b[0m, in \u001b[0;36mwrite_audiofile\u001b[1;34m(self, filename, fps, nbytes, buffersize, codec, bitrate, ffmpeg_params, write_logfile, verbose, logger)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\moviepy\\decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(clip, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n",
      "File \u001b[1;32mc:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\moviepy\\audio\\AudioClip.py:202\u001b[0m, in \u001b[0;36mAudioClip.write_audiofile\u001b[1;34m(self, filename, fps, nbytes, buffersize, codec, bitrate, ffmpeg_params, write_logfile, verbose, logger)\u001b[0m\n\u001b[0;32m    200\u001b[0m         codec \u001b[38;5;241m=\u001b[39m extensions_dict[ext[\u001b[38;5;241m1\u001b[39m:]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodec\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoviePy couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find the codec associated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith the filename. Provide the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodec\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter in write_audiofile.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ffmpeg_audiowrite(\u001b[38;5;28mself\u001b[39m, filename, fps, nbytes, buffersize,\n\u001b[0;32m    207\u001b[0m                          codec\u001b[38;5;241m=\u001b[39mcodec, bitrate\u001b[38;5;241m=\u001b[39mbitrate,\n\u001b[0;32m    208\u001b[0m                          write_logfile\u001b[38;5;241m=\u001b[39mwrite_logfile, verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    209\u001b[0m                          ffmpeg_params\u001b[38;5;241m=\u001b[39mffmpeg_params,\n\u001b[0;32m    210\u001b[0m                          logger\u001b[38;5;241m=\u001b[39mlogger)\n",
      "\u001b[1;31mValueError\u001b[0m: MoviePy couldn't find the codec associated with the filename. Provide the 'codec' parameter in write_audiofile."
     ]
    }
   ],
   "source": [
    "#install libs\n",
    "!pip install SpeechRecognition moviepy\n",
    "import moviepy.editor as mpe\n",
    "#convert to audio\n",
    "video = mpe.VideoFileClip(\"videos\\Devin AI Capabilities,What Can First AI Software Engineer Do_ Future Of Software Engineering.mp4\")\n",
    "video.audio.write_audiofile(r\"davin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in Many-pepoles.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#video to audio \n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def video_to_audio(video_file, audio_file):\n",
    "    # Load the video file\n",
    "    video_clip = VideoFileClip(video_file)\n",
    "\n",
    "    # Extract the audio from the video\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    # Save the extracted audio to a file\n",
    "    audio_clip.write_audiofile(audio_file)\n",
    "\n",
    "    # Close the video and audio clips\n",
    "    video_clip.close()\n",
    "    audio_clip.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"videos\\Many People Have Forgetten This!.mp4\"\n",
    "    audio_file = \"Many-pepoles.wav\"  # You can specify the desired audio format here\n",
    "\n",
    "    # Convert video to audio\n",
    "    video_to_audio(video_file, audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: hello Google my name is krish Naik and welcome to my YouTube channel so guys first of all a very happy Dussehra to all of you I will hardly take around 2 to 3 minutes because I really want to convey a very important message as this is your festival I definitely you want you all to enjoy with your family if I talk about Dussehra guys I just want to say this saying Burai Ko Aag lagao achhai ko Apna aur Khushiyan Bano that basically means remove all negativity from your life do good things in your life always be truthful to yourself truthful with others I know in this world of negativity competition jealousy many more things are there in our life you know in your office place in your workplace in your day to day activities with your friends with your enemies you know what does specifies to you is that at the end of the day the truth wins and I feel people have forgotten this again I just want to stress on one point please bring positivity in your life please bring positivity in others life also if you can you know always try to do the right thing I know many people do not do the right thing they may be multiple things multiple factors that may be affected them to do the right things it is always good and better because at the end of the day the good deed that you specifically do wins everybody's heart in this world I know right now if I speaking about this word many people may not agree with me have spent more than 13 plus years in IIT company right and everything basically it company have worked in multiple places with respect to work with respect to multiple things within of friends group everything but at the end of the day from all this experience what I feel is that when you do good when you are truthful to yourself in your truthful to others at the end of the day it is always beneficial for you so this is what Dussehra all signifies in our life itself again guys I really want to stressed on this specific point Burai ko hatao remove all the negativity in our life try to be truthful try to be try to bring that positive means by that everybody around you will always be happy so yes I did not want to take much time but the a lot of free things that I am actually giving in my YouTube channel everything with respect to start this positivity you can also share this message with everyone go ahead and tell your friend something good about them I know you may be you may be having some Enemies it's ok I love your enemies at the end of the day the human being you know the thought process that goes in their mind because of some something you know it may be something right of the day we are all brothers out their living in this beautiful Earth and we should definitely help out each other so considering this I'm just starting this positivity message to everyone all the free content with respect to data science will be available in the description along with the return material along with the videos everything as such again guys don't study today at least enjoy this day with your family and friends this is the festival this month then Diwali and many more things I hope you like this particular take care bye-bye\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio_data)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def split_audio(input_audio):\n",
    "    audio = AudioSegment.from_file(input_audio)\n",
    "    segment_duration_ms = 60 * 1000  # 1 minute in milliseconds\n",
    "    segments = []\n",
    "\n",
    "    for start_time in range(0, len(audio), segment_duration_ms):\n",
    "        segment = audio[start_time:start_time + segment_duration_ms]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def main(input_audio):\n",
    "    segments = split_audio(input_audio)\n",
    "    transcriptions = []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment.export(f\"segment_{i}.wav\", format=\"wav\")\n",
    "        transcription = transcribe_audio(f\"segment_{i}.wav\")\n",
    "        transcriptions.append(transcription)\n",
    "        os.remove(f\"segment_{i}.wav\")\n",
    "\n",
    "    combined_transcription = \" \".join(transcriptions)\n",
    "    return combined_transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_audio = \"Many-pepoles.wav\"  # Path to your input audio file\n",
    "    transcription = main(input_audio)\n",
    "    print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: hello Google my name is krish Naik and welcome to my YouTube channel so guys first of all a very happy Dussehra to all of you I will hardly take around 2 to 3 minutes because I really want to convey a very important message as this is your festival I definitely you want you all to enjoy with your family if I talk about Dussehra guys I just want to say this saying Burai Ko Aag lagao achhai ko Apna aur Khushiyan Bano that basically means\n",
      "Chunk 2: remove all negativity from your life do good things in your life always be truthful to yourself truthful with others I know in this world of negativity competition jealousy many more things are there in our life you know in your office place in your workplace in your day to day activities with your friends with your enemies you know what Dussehra specifies to you is that at the end of the day the truth wins and I feel\n",
      "Chunk 3: people have forgotten this again I just want to stress on one point please bring positivity in your life please bring positivity in others life also if you can you know always try to do the right thing I know many people do not do the right thing they may be multiple things multiple factors that may be affected them to do the right things it is always good and better because at the end of the day\n",
      "Chunk 4: the good deed that you specifically do wins everybody's heart in this world I know right now if I speaking about this word many people may not agree with me have spent more than 13 plus years in IIT company right I have seen each and everything basically in IT company I have worked in multiple places with respect to work with respect to multiple things within of friends group have seen good deeds have\n",
      "Chunk 5: everything but at the end of the day from all this experience what I feel is that when you do good when you are truthful to yourself in your truthful to others at the end of the day it is always beneficial for you so this is what Dussehra all signifies in our life itself again guys I really want to stressed on this specific point Burai ko hatao remove all the negativity in our life try to be truthful try to be try to bring that positive means by that everybody around\n",
      "Chunk 6: you will always be happy so yes I do not want to take much time but they are a lot of free things that I am actually giving in my YouTube channel everything with respect to this just to start this positivity you can also share this message with everyone go ahead and tell your friend something good about them I know you may be you may be having someone name is it's ok I love your enemies at the end of the day the human being you know the thought process that goes in their mind because of some something you know it may be something right\n",
      "Chunk 7: of the day we are all brothers out their living in this beautiful Earth and we should definitely help out each other so considering this I'm just starting this positivity message to everyone all the free content with respect to data science will be available in the description along with the return material along with the videos everything as such again guys don't study today at least enjoy this day with your family and friends this is the festival this month then Diwali and many more things I hope you like this particular\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"Many-pepoles.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
