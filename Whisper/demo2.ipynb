{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg-python in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (4.10.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: moviepy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (4.10.0)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.66.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (69.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install SpeechRecognition moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: moviepy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (4.10.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.66.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (69.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in An Important Advice-AI .wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#install libs\n",
    "!pip install SpeechRecognition moviepy\n",
    "import moviepy.editor as mpe\n",
    "#convert to audio\n",
    "video = mpe.VideoFileClip(\"videos\\An Important Advice-AI Is Advancing Just Start Learning AI Before Its Late.mp4\")\n",
    "video.audio.write_audiofile(r\"An Important Advice-AI .wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in DATASCIENCE.mp4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "import wave, math, contextlib\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import AudioFileClip\n",
    "transcribed_audio_file_name = \"DATASCIENCE.mp4.wav\"\n",
    "zoom_video_file_name = \"An Important Advice-AI .wav\"\n",
    "audioclip = AudioFileClip(zoom_video_file_name)\n",
    "audioclip.write_audiofile(transcribed_audio_file_name)\n",
    "with contextlib.closing(wave.open(transcribed_audio_file_name,'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)\n",
    "total_duration = math.ceil(duration / 60)\n",
    "r = sr.Recognizer()\n",
    "for i in range(0, total_duration):\n",
    "    with sr.AudioFile(transcribed_audio_file_name) as source:\n",
    "        audio = r.record(source, offset=i*60, duration=60)\n",
    "    f = open(\"DATASCIENCE.txt\", \"a\")\n",
    "    f.write(r.recognize_google(audio))\n",
    "    f.write(\" \")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: Error: recognition request failed: Bad Request\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    # Load the audio file\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio = recognizer.record(source, duration=300)  # Transcribe the first 300 seconds (5 minutes) of audio\n",
    "    \n",
    "    # Transcribe the audio\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"An Important Advice-AI .wav\"  # Path to your input audio file\n",
    "    transcription = transcribe_audio(audio_file)\n",
    "    print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def split_audio(input_audio, output_folder, segment_duration=60000):\n",
    "    audio = AudioSegment.from_file(input_audio)\n",
    "    segment_index = 0\n",
    "\n",
    "    while len(audio) > segment_duration:\n",
    "        segment = audio[:segment_duration]\n",
    "        segment.export(os.path.join(output_folder, f\"segment_{segment_index}.wav\"), format=\"wav\")\n",
    "        audio = audio[segment_duration:]\n",
    "        segment_index += 1\n",
    "\n",
    "    if len(audio) > 0:\n",
    "        audio.export(os.path.join(output_folder, f\"segment_{segment_index}.wav\"), format=\"wav\")\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio_data)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def main(input_audio, output_text):\n",
    "    output_folder = \"audio_segments\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Split audio into segments\n",
    "    split_audio(input_audio, output_folder)\n",
    "\n",
    "    # Transcribe each segment\n",
    "    transcriptions = []\n",
    "    for segment_file in os.listdir(output_folder):\n",
    "        segment_path = os.path.join(output_folder, segment_file)\n",
    "        transcription = transcribe_audio(segment_path)\n",
    "        transcriptions.append(transcription)\n",
    "\n",
    "    # Combine transcriptions\n",
    "    with open(output_text, \"w\") as f:\n",
    "        for transcription in transcriptions:\n",
    "            f.write(transcription + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_audio = \"An Important Advice-AI .wav\"  # Path to your input audio file\n",
    "    output_text = \"output_transcription.txt\"  # Path to save the transcription\n",
    "    main(input_audio, output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#its video to audio \n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def video_to_audio(video_file, audio_file):\n",
    "    # Load the video file\n",
    "    video_clip = VideoFileClip(video_file)\n",
    "\n",
    "    # Extract the audio from the video\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    # Save the extracted audio to a file\n",
    "    audio_clip.write_audiofile(audio_file)\n",
    "\n",
    "    # Close the video and audio clips\n",
    "    video_clip.close()\n",
    "    audio_clip.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"videos\\Devin AI Capabilities,What Can First AI Software Engineer Do_ Future Of Software Engineering.mp4\"\n",
    "    audio_file = \"davin.wav\"  # You can specify the desired audio format here\n",
    "\n",
    "    # Convert video to audio\n",
    "    video_to_audio(video_file, audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#its working good \n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio_data)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def split_audio(input_audio):\n",
    "    audio = AudioSegment.from_file(input_audio)\n",
    "    segment_duration_ms = 60 * 1000  # 1 minute in milliseconds\n",
    "    segments = []\n",
    "\n",
    "    for start_time in range(0, len(audio), segment_duration_ms):\n",
    "        segment = audio[start_time:start_time + segment_duration_ms]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def main(input_audio):\n",
    "    segments = split_audio(input_audio)\n",
    "    transcriptions = []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment.export(f\"segment_{i}.wav\", format=\"wav\")\n",
    "        transcription = transcribe_audio(f\"segment_{i}.wav\")\n",
    "        transcriptions.append(transcription)\n",
    "        os.remove(f\"segment_{i}.wav\")\n",
    "\n",
    "    combined_transcription = \" \".join(transcriptions)\n",
    "    return combined_transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_audio = \"DATASCIENCE.mp4.wav\"  # Path to your input audio file\n",
    "    transcription = main(input_audio)\n",
    "    print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: hello all my name is krish Naik and welcome to my YouTube channel so guys yes this particular video is again related to some advice and guidance that I really want to provide you all and yes in this video I am going to talk about AI I want to go to provide you some kind of advice at least to start learning AI and there is a reason why I am specifically saying you this I am not saying that ok your end goal should be probably making a transition working in some\n",
      "Chunk 2: companies analytics industry not as such but start incorporating AI in your day to day lives ai's evolving a lot lot of new things are probably coming up initially we had machine learning deep learning now generally models and problem in the upcoming two years definitely lot of companies are coming to come up with different different Startup ideas just using this kind of LLM models where they are specifically solving some kind of problem ok\n",
      "Chunk 3: but my advice will be that you know start learning AI to incorporate the capabilities that you can actually put in your day to day activities you know as you know data data Speaks a lot you know and if you probably know AI machine learning deep learning trust me you will be able to explore many more information from that specific data and not saying that ok learn just to get a job but instead learn to make\n",
      "Chunk 4: more productive and you can definitely do that I know many people are working in different domains different Technologies and different programming language they having a different work but at least have an idea about AI start incorporating that in your life you may be thinking Crush you are a youtuber you are doing it for your purpose you know you may gain subscribers you may probably bring up your telling people to buy courses I am not saying nothing as such I am saying that wherever you get some sources\n",
      "Chunk 5: learning at not my channel at least from somewhere else is so many open source documentation that are available you know once you start incorporating it trust me opportunities are them anymore in the world not only see if you don't want to work anyway at least use it in your personal life right you have your financial data right you have your let's say I'll give you one example one of my friend is call me this morning right he was saying the crush I am running a business you know I have this specific use cases and this is\n",
      "Chunk 6: possible because of machine learning can you help me out in this you know and he is saying his purposely saying that I also want to learn the snap you know let it be a business create you are a person who is running a business you are a person who who is working on some of the other thing and there if you get a bit of chances of automatic things trust me I will come into picture ok and this is super important this is the advice that I really want to give to everyone ok again\n",
      "Chunk 7: my main aim is to democracy this AI education to everyone that is the reason why I have come up with this YouTube channel when I specifically upload videos related to everything that is in Ai tomorrow anything that probably comes I will be explaining you I will be teaching you I will be showing you multiple examples you know yeah it is up to you whether you want to make it as a full-time opportunity whether you want to learn it in such a way that you may probably get a job but start incorporating it is up to you guys I am not forcing you but start\n",
      "Chunk 8: setting it try to use it in your day to day practices you will be able to see the changes I have seen some of my friends getting productive you know I have my cousin brother who is working in US and from past two years you know his working in his architect working in something else some other Technology some other domain but still he has started using AI in his day to day activities and this is the most important advice that I really want to give it to you right again my\n",
      "Chunk 9: to democracy education to everyone that is the reason I am uploading this many number of videos with respect to learning anything it is up to you find out any sources but start learning this is the advice that I really want to give it to you because you will be seeing how much changes it is going to come up in the upcoming 2 years right now lines in is going on you can actually create your own LLM models you know I was just solving a use case right now what ever documents I have let's say\n",
      "Chunk 10: I want to I want to probably create an LLM model with respect to my data and I was just seeing that I had a 2GB of files that is present PDF files which are a lot of content I was able to train my own chat box so this kind of examples will definitely come up I will show you how you can probably train at night but I want to do it for my day to day purpose see let's say that I have my Excel sheets of all the expenditures all the expenses that I am doing right in some format right I can also train that specific thing\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"DATASCIENCE.mp4.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: hello all my name is Krishna and welcome to my YouTube channel so guys I hope from past couple of days you have seen a lot of videos related to Deven the first AI software engineer probably every youtuber has uploaded the specific video and obviously they have brought a lot of points with respect comparing to a software engineer jobs but let's understand in this video what all things doesn't can actually do and as you all know many people have applied for the beta version of 7 and they have got the access and with respect to that they have created\n",
      "Chunk 2: so we'll just try to get a clear idea like what all efficient task it can actually do and after understanding all these things we will be discussing about the pros and cons and as a software engineer what you really need to focus to be in the industry with a good demand what all things you can actually do will also be discussing about that so please make sure that he was this video tilly and because there will be a lot of things to discuss in this ok so here was the video of the devil\n",
      "Chunk 3: AI software we just saw the demo but just to understand you can see what is exactly doesn't it is the world's first autonomous AI software engineer devil is tireless skilled him at equally ready to build alongside you are independently complete task for you to review so in short since it is an eye so it really need to work tirelessly right with human beings that is not possible right now a very important point is over here with devil engineers can focus more on the\n",
      "Chunk 4: problem statement and engineer teams can Strike for more ambitious goals because all the previous problem statement are already solved by the engineers and with respect to that they already have the data code whatever things are specifically required so obviously you really need to have a new problem statement to think upon which Devin cannot solve whatever things are actually sold demon will be able to solve it because they already trained on that right so this is the most major point right still you know any new thing any new\n",
      "Chunk 5: statement that may come devil will not be able to solve it because it needs to get train right it's just like any high if it if there is a solution then it will be trained then it will be able to do it ok unless and until Agi does not come I don't think so it will be able to do it now let's understand about Demons capability ok here you will be able to see we have given them in the ability to actively collaborate with the user demon reports on its progress in real time except feedback and work together with you through\n",
      "Chunk 6: choices are needed so obviously As a Software Engineer whatever task is required for a software engineer it is able to do they are working in a collaborative way they have to make sure that they have all the documentation spend stories needs to be assigned very much in an equal way to all the developers itself now with respect to the feedback there really need to consider it and really need to solve it now let's understand from all the people who have actually got the excess water can actually do with respect to the implementation that they have done so here you can see devil can learn how to use\n",
      "Chunk 7: similar technologies also after reading a block post doesn't runs control not on model to produce images with conceals messages for Sara so one of the users in they were able to do from this particular blog then can build and and deploy apps end to end so this was actually possible by devil they were able to build and deploy it because already you know how to build it and how to deploy it that is the time of task and obviously from the demo that we have seen a couple days back it has those kind of functionalities along\n",
      "Chunk 8: this whenever we see the next task over here you can see that devil can also autonomously find and fix bugs in the code basis that basically is demon helps angry maintain and debug is open source competitive programming book so obviously when you have this entire codebase you know and it is being able to solve it ok and will also be seeing there is something called as SW benchmark OK will discuss about it like in the GitHub open issues like how much it was able to solve so here the next task here you\n",
      "Chunk 9: devil country and find units on AI models David can address bugs and feature request in open source repositories just give the GitHub issues and do the setup and context gathering that is only needed and Devil will be able to do it doesn't can contribute to mature production repositories which is also very really good we even tried giving driven real jobs on upwork and it could do those two so if you don't know about upwork it is all about freelancing and just\n",
      "Chunk 10: you don't even have to stay there to do the any freelancing work it can probably considerate it can probably solve it and with respect to all this work that you can actually see all the demo videos actually given and how it was able to do it now just by seeing this obviously we can actually see that doesn't can really do a lot many things right and if I proud consider a normal software engineer right now what is actually expected in interviews in the future in upcoming 12 years there will be a lot of expectation you really need to\n",
      "Chunk 11: jack of all trades you know you should have knowledge with respect to everything but yes if there is a problem statement that requires actual research out of box thinking out of box thinking to solve it the human beings are the best people to solve it later on whatever task are there whatever repetitive task are there whatever things that I already implemented them and can actually do it for you this was one of the thing which I was talking about SW bench so in the GitHub repository whatever the open issues right\n",
      "Chunk 12: Falcon privacy world get a wishes how much it was able to solve it so here you can see we evaluated driven on SW bench a challenging benchmark that ask agent to resolve Real world data wishes found in open source projects and Django and sky David correctly results 13.86 % all the issues and to end you know which is quite good and the previous state of art was 1.96 now with respect to all the other tools right like cloudy to SW Lama 13 bsw Lama 7 BJP 4G PT 3.5\n",
      "Chunk 13: you can actually see you know the difference is quite huge and Devin is really really having a good number here that is 13.86 now this really looks yes then can do a lot many task but as I said when it comes to problem statement that comes out of the box to solve it definitely them cannot do because they are not trained in those kind of data but as a software engineer one thing that you really need to do is that companies will also look at you know\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"davin.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: whisper in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.1.10)\n",
      "Requirement already satisfied: six in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from whisper) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\shamshad ahmed\\appdata\\local\\temp\\pip-req-build-3a0ve_gs\n",
      "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (4.66.2)\n",
      "Requirement already satisfied: torch in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (2.2.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (0.6.0)\n",
      "Requirement already satisfied: numba in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (0.59.1)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (10.2.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from numba->openai-whisper==20231117) (0.42.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (2024.2.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (1.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (4.10.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm->openai-whisper==20231117) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/openai/whisper.git 'C:\\Users\\Shamshad ahmed\\AppData\\Local\\Temp\\pip-req-build-3a0ve_gs'\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.39.1-py3-none-any.whl (8.8 MB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.39.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg-python in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"choco\" - maybe you meant \"check\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip choco install ffmpeg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall ffmpeg\n",
    "# pip3 uninstall ffmpeg\n",
    "# pip3 uninstall ffmpeg-python\n",
    "# pip uninstall ffmpeg\n",
    "# pip uninstall ffmpeg-python\n",
    "# brew uninstall ffmpeg\n",
    "# Install ffmpeg binary using this 405 guide.\n",
    "\n",
    "# Install ffmpeg for python\n",
    "\n",
    "# pip3 install ffmpeg\n",
    "# pip3 install ffmpeg-python\n",
    "# pip install ffmpeg\n",
    "# pip install ffmpeg-python\n",
    "# # Thatâ€™s all it should work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pip3 (from versions: none)\n",
      "ERROR: No matching distribution found for pip3\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pip3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg-python in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg-python --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch device is set to cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello all, my name is Krashanayak and welcome to my YouTube channel. So guys, yes, this particular video is again related to some advice, some guidance that I really want to provide you all. And yes, in this video, I'm going to talk about AI. I want to go to provide you some kind of advice at least to start learning AI. And there is a reason why I am specifically saying you this. I'm not saying that, okay that your end goal should be probably making a transition, working in some AI companies, analytics industry, not as such, but start incorporating AI in your day to day lives. AI is evolving a lot, a lot of new things are probably coming up. Initially, we had machine learning, deep learning, now, generally AI, L LLM models and probably in the upcoming two years definitely a lot of companies are going to come up with different different startup ideas just using this kind of LLM models where they are specifically solving some kind of problem. Okay, but my advice will be that you know start learning AI to incorporate the capabilities that you can actually put in your day to day activities, you know, as you know, data, data speaks a lot, you know, and if you probably know AI, machine learning, deep learning, trust me you will be able to explore many more information from that specific data. I'm not saying that okay learn just to get a job but instead learn to make yourself more productive and you can definitely do that. I know many people are working in different domains, different technologies and different programming languages, they are having a different work but at least have an idea about AI, start incorporating that in your life. You may be thinking, Chris, you are a YouTuber, you're doing it for your purpose. You know, you may gain subscribers, you may probably bring up, you're telling people to buy courses. I'm not saying nothing as such. I'm saying that wherever you get some sources, start learning it, not my channel, at least from somewhere else. There's so many open source documentation that are available. You know, once you start incorporating it, trust me, opportunities are there many more in the world, not only see if you don't want to work anywhere, at least use it in your personal life. Right. You have your financial data, right? You have your, let's say, I'll give you one example. One of my friends just called me this morning, right? He was saying that I'm running a business, you know, I have this specific use cases and this is only possible because of machine learning. Can you help me out in this? You know, and he's saying, he's purposely saying that I also want to learn this now. Let it be a business creator, you are a person who is running a business, you are a person who is working on some or the other thing and there if you get a bit of chances of automating things, trust me AI will come into picture. Okay, and this is super important. This is the advice that I really want to give to everyone. Okay, again, my main aim is to democratize this AI education to everyone. That is the reason why I have come up with this YouTube channel where I specifically upload videos related to everything that is in AI. Tomorrow anything that probably comes, I will be explaining you, I will be teaching you, I will be showing you multiple examples. It is up to you whether you want to make it as a full time opportunity, whether you want to learn it in such a way that you may probably get a job but start incorporating it. It is up to you guys. I'm not forcing you but start incorporating it. Try to use it in your day to day practices. You will be able to see the changes. I've seen some of my friends getting productive. You know, I have my cousin brother who is working in US and from past two years, you know, he's working in some, he's a architect working in something else, some other technology, some other domain, but still he has started using AI in his day to day activities and he can see that productivity. And this is the most important advice that I really want to give it to you. Again, my main aim is to democratize AI education to everyone. That is the reason why I'm uploading this many number of videos with respect to learning anything. It is up to you. Find out any sources but start learning. This is the advice I really want to give to you because you will be seeing how much changes it is going to come up in the upcoming two years. Right now, Lanchin is going on. You can actually create your own LLM models. I was just solving a use case right now. Whatever documents I have, let's say I want to probably create an LLM model with respect to my data. I was just seeing that I had a 2GB of files that is present, PDF files which had a lot of content and I was able to train my own chatbot. So this kind of examples will definitely come up. I'll show you how you can probably train it. Right. But I want to do it for my day to day purpose. and my LLM models can give advice where I can probably save some money. This is just one of the examples that I really want to come up with in front of you. A lot of applications will be there and definitely do make sure that you take up this advice, take it seriously, not for making, not for working full time at least, but at least try to incorporate this knowledge in your life. So yes, I hope you like this particular video. If you like this, I'll see you all in the next video. Have a great day. Thank you, Wanda. Bye-bye, take care.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the device\n",
    "pytorch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current PyTorch device is set to\", pytorch_device)\n",
    "\n",
    "# Define the path to the audio file\n",
    "audio_path = \"DATASCIENCE.mp4.wav\"\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = pipeline(model=\"openai/whisper-small\", device=pytorch_device)\n",
    "\n",
    "# Process the audio file\n",
    "predictions = pipe(audio_path, chunk_length_s=20, stride_length_s=5)\n",
    "\n",
    "# Extract the recognized text from the predictions\n",
    "recognized_text = predictions[\"text\"]\n",
    "print(recognized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch device is set to cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello all my name is Chris Snaik and welcome to my youtube channel. So guys first of all a very happy Dashira to all of you I will hardly take around two to three minutes because I really want to convey a very important message as this is your festival I definitely want you all to enjoy with your family. If I talk about Dashira guys, I just want to say this saying, Burai ko aag lagao, your life, do good things in your life, always be truthful to yourself, truthful with others. I know in this world of negativity, competition, jealousy, many more things are there in our life, you know, in your office place, in your workplace, in your day-to-day activities with your friends, with your enemies, you know. What Desira specifies to you is that at the end of the day, the truth wins. And I feel many people have forgotten this. Again, I just want to stress on one point, please bring positivity in your life. Please bring positivity in others life also if you can. You know, always try to do the right thing. I know many people do not do the right thing. There may be multiple things, multiple factors that may be affecting them to do the right things. It is always good and better because at the end of the day, the good deed that you specifically do wins everybody's heart in this world. I know right now if I'm speaking about this word many people may not agree with me. I've spent more than 13 plus years in my IT company. I have seen each and everything. Basically in IT company I have worked in multiple places with respect to work, with respect to multiple things within our friends group. I've seen jealousy, I've seen hate, I've seen good deeds, I've seen everything but at the end of the day from all this experience what I feel is that when you do good, when you're truthful to yourself and you're truthful to others at the end of the day, from all this experience, what I feel is that when you do good, when you're truthful to yourself and you're truthful to others, at the end of the day, it is always beneficial for you. So this is what the She-Ra all signifies in our life itself. Again guys I really want to stress on this specific point, burai ko hatao, remove all the negativity in your life, try to be truthful, try to bring that positiveness. By that, everybody around you will always be something. at the end of the day. They are human being, you know, the thought process that goes in their mind because of some something, you know, it may be something right at the end of the day. We are all brothers out there living in this beautiful earth and we should definitely help out each other. So considering this I'm just starting this positivity message to everyone. All the free content with respect to data science will be available in the description along with the written materials along with the videos everything as such. Again guys don't study today at least enjoy this day with your family and friends. This is the festival this month then Diwali and many more things. So yes this was it for my side I hope you like this particular video I'll see you all in the next video have a great day thank you and take care bye bye\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the device\n",
    "pytorch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current PyTorch device is set to\", pytorch_device)\n",
    "\n",
    "# Define the path to the audio file\n",
    "audio_path = \"videos\\Many People Have Forgetten This!.mp4\"\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = pipeline(model=\"openai/whisper-small\", device=pytorch_device)\n",
    "\n",
    "# Process the audio file\n",
    "predictions = pipe(audio_path, chunk_length_s=20, stride_length_s=5)\n",
    "\n",
    "# Extract the recognized text from the predictions\n",
    "recognized_text = predictions[\"text\"]\n",
    "print(recognized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\shamshad ahmed\\appdata\\local\\temp\\pip-req-build-wauoflmr\n",
      "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: tqdm in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (4.66.2)\n",
      "Requirement already satisfied: torch in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (2.2.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (10.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (1.26.4)\n",
      "Requirement already satisfied: numba in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (0.59.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from numba->openai-whisper==20231117) (0.42.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (4.10.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (1.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm->openai-whisper==20231117) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/openai/whisper.git 'C:\\Users\\Shamshad ahmed\\AppData\\Local\\Temp\\pip-req-build-wauoflmr'\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: hi\n",
      "Hello world my name is Krishnaak and welcome to my youtube channel So guys first of all a very happy Dashira to all of you I will hardly take around 2-3 minutes because I really want to convey a very important message As this is your first ever I definitely want you all to enjoy with your family If I talk about Dashira guys I just want to say this say Burai ko ag lagau a chai ko apnao or khushiya bato\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"Many-pepoles.wav\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions(fp16 = False)\n",
    "#options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: hello Google my name is krish Naik and welcome to my YouTube channel so guys first of all a very happy Dussehra to all of you I will hardly take around 2 to 3 minutes because I really want to convey a very important message as this is your festival I definitely you want you all to enjoy with your family if I talk about Dussehra guys I just want to say this saying Burai Ko Aag lagao achhai ko Apna aur Khushiyan Bano that basically means\n",
      "Chunk 2: remove all negativity from your life do good things in your life always be truthful to yourself truthful with others I know in this world of negativity competition jealousy many more things are there in our life you know in your office place in your workplace in your day to day activities with your friends with your enemies you know what Dussehra specifies to you is that at the end of the day the truth wins and I feel\n",
      "Chunk 3: people have forgotten this again I just want to stress on one point please bring positivity in your life please bring positivity in others life also if you can you know always try to do the right thing I know many people do not do the right thing they may be multiple things multiple factors that may be affected them to do the right things it is always good and better because at the end of the day\n",
      "Chunk 4: the good deed that you specifically do wins everybody's heart in this world I know right now if I speaking about this word many people may not agree with me have spent more than 13 plus years in IIT company right I have seen each and everything basically in IT company I have worked in multiple places with respect to work with respect to multiple things within of friends group have seen good deeds have\n",
      "Chunk 5: everything but at the end of the day from all this experience what I feel is that when you do good when you are truthful to yourself in your truthful to others at the end of the day it is always beneficial for you so this is what Dussehra all signifies in our life itself again guys I really want to stressed on this specific point Burai ko hatao remove all the negativity in our life try to be truthful try to be try to bring that positive means by that everybody around\n",
      "Chunk 6: you will always be happy so yes I do not want to take much time but they are a lot of free things that I am actually giving in my YouTube channel everything with respect to this just to start this positivity you can also share this message with everyone go ahead and tell your friend something good about them I know you may be you may be having someone name is it's ok I love your enemies at the end of the day the human being you know the thought process that goes in their mind because of some something you know it may be something right\n",
      "Chunk 7: of the day we are all brothers out their living in this beautiful Earth and we should definitely help out each other so considering this I'm just starting this positivity message to everyone all the free content with respect to data science will be available in the description along with the return material along with the videos everything as such again guys don't study today at least enjoy this day with your family and friends this is the festival this month then Diwali and many more things I hope you like this particular\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"Many-pepoles.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in Sudha.murthy.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#its video to audio \n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def video_to_audio(video_file, audio_file):\n",
    "    # Load the video file\n",
    "    video_clip = VideoFileClip(video_file)\n",
    "\n",
    "    # Extract the audio from the video\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    # Save the extracted audio to a file\n",
    "    audio_clip.write_audiofile(audio_file)\n",
    "\n",
    "    # Close the video and audio clips\n",
    "    video_clip.close()\n",
    "    audio_clip.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"videos\\Portado Story by Sudha Murthy ðŸ’¯.mp4\"\n",
    "    audio_file = \"Sudha.murthy.wav\"  # You can specify the desired audio format here\n",
    "\n",
    "    # Convert video to audio\n",
    "    video_to_audio(video_file, audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: I was 18 years old when I got married I belong to a very conservative family a village family where good daughters never say no to their parents my father wanted me to get married and all I said that makes you happy young says and of course it was never a happy marriage just about after two years\n",
      "Chunk 2: somehow my husband in British he managed to jump out save themselves I am happy for him but I said inside the car and I started a lot of interest\n",
      "Chunk 3: the Drag me out of the car and while they were dragging me out I got the complete transaction of my spinal cord those two and half month in the hospital I was at the verge of despair one day doctor came to me and said well I heard that you wanted to be an artist but you ended up Being a housewife I have a bad news for you\n",
      "Chunk 4: you won't be able to paint again next day Dr came to me and said you spine injury is so bad you won't be able to work again next day Dr came to me and said because of your spine injury and the fixation that you have in your back you won't be able to give birth to a child again\n",
      "Chunk 5: help me going voice one day I ask my brothers I know I had but I'm tired of looking at these white balls in the hospital and wearing this white scrubs bring me some colours bring me some small Canvas I want to paint so the very first painting I made was on my dead bed where I painted for the very first time what is amazing\n",
      "Chunk 6: play my story people used to come and say what lovely painting so much colour nobody could see the grief in it only I could and that day I decided that I am going to live life for myself I am not going to be that perfect person for someone I am just going to take this moment I am going\n",
      "Chunk 7: you know what was my biggest fear divorce but the day I decided that this is nothing but my fear I liberated myself by sitting him free and I made myself emotionally so strong that the day I got the news that he is getting married I sent him a text that I am so happy for you and I wish you all the best\n",
      "Chunk 8: so there is no point of crying just go and adopt one and that's what I do my name in different organisations difference\n",
      "Chunk 9: I am coming to take him home and that day that was two years two days old and today is\n",
      "Chunk 10: I started to paint I have done a lot of modelling campaigns I decided that I am going to join the National TV of Pakistan the national goodwill ambassador for you and women in Pakistan and now I speak women children I was featured in BBC 100 women for 2015\n",
      "Chunk 11: so when you accept yourself The Way You Are The World recognises you it all starts from within we have this amazing fantasy about life this is how things should work this is my plan it should go as per my plan if that doesn't happen\n",
      "Chunk 12: I never supposed to be easy so when you're expecting East from life and life gives you lemons then you make the lemonade and then do not blame life for that it is OK to be scared it is OK to cry everything is ok but giving up should not be an option they always say that failure is not an option failure should be an option because when you fail you get\n",
      "Chunk 13: celebrate your life leave it don't die before your death real happiness lies in gratitude so be grateful be alive and live every moment\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"Muniba.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: potato is the real life story potato was my classmate reasonably bright may be very bright I suppose never he came to school never came to college engineering college he used to stay here from Goa he used to come here the hostel and he never to come to college or college of 7 to 11 he will come about 10:00 a.m. and you will get into classes after that after end of the year\n",
      "Chunk 2: one month you will get up early and you will study and you know there is a method in college you take question papers April and October whatever comes in October will never come in April ok I am sure the school also you have come to final ok eliminate this question answers and then make approximate question paper and study that and used to get the first class he read all 4 years like this person you don't come to\n",
      "Chunk 3: classification regularly you can get distinction you can get a rank he said so that you do not know in life it is not the education in the connection it really works and we have to be a lot of connections you know in when you are young it helps you I came from a middle class family my father was a doctor gynecologist I never understood disconnection all those things then he said my father was in military where in Kolkata club which were never seen in our real\n",
      "Chunk 4: so I thought maybe the big man and it is the beyond our Bus ka Baat Nahin I felt it should something else past in 1972 I finish my engineering so as potato and I didn't meet him for long time after 3035 years I think 2008 or 9 I had been to Dubai to deliver a lecture and when the lecture was over I saw somebody waiting for waiting for you will\n",
      "Chunk 5: then he said I am potato I could not recognise because in 30 40 years people put on weight your hair will be grey you know all those physical changes will happen I would not very happy I told him oh let's learn and talk is that no I just wanted to tell you one words one word to you so I was what happened to remember in college I used to bunk classes of course I remember banking class\n",
      "Chunk 6: he said I should bunk class and I joined a job and because in those days the moment you have been used to get job in my time and my boss is came to know that my subordinates came to know that I don't have knowledge getting a first class marks are different than knowledge please remember parents your children should be in the pursuit of knowledge than the marks though it's a competitive examination it is a competitive world it is a competitive world\n",
      "Chunk 7: 1000 times for 10000 times more than what time I had but with all those things the knowledge which always help you then your marks in real life so he said I got marks but I did not have knowledge once the subordinates know that you don't have knowledge they don't respect you what is the boss no you don't have knowledge then he wants and he wants reason to kick you out I was kicked out from one job and so much used to getting up 9 in the morning I'll go let to your office\n",
      "Chunk 8: check out there also over a period of time I could get job anywhere and now I have a job as a manager job or smaller job in Dubai and my I have to I said what about your family should have two daughters left them in Chennai to study I said what he said yes I want they should study well they should not become like me I just waited for you to tell to the one thing when I used to make fun of you he is to tell me you are like a you are like\n",
      "Chunk 9: running race horse because of the horse will see neither left or right it runs same way you always wear academic and wish to make fun of you today when we call you nerd and I thought I was very smart today I feel you are successful and I wasted my life I just wanted to tell you if you don't if you don't discipline if you don't discipline when you are fool or in College you pay heavy\n",
      "Chunk 10: later part of your life discipline like coming up in a proper way writing properly ok talking properly communicating properly respecting people properly this really helps a long way in your life so I don't want you should be like potato regretting in your old age you should remember that to be successful in life there is no shortcut the only method is hard work and straight way and that you have to do\n",
      "Chunk 11: remember one more part that your school you come as a rock product to school like a raw material to school school Church you out 14 years and takes you out a finished good you come out the young toddler here and you go out at the fine teenager the School has done so much not for the fees what you give your fees are minimal when you compare to the kind of advantage you get in real life your teacher your parents everyone put so much\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Create a temporary WAV file\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_wav_file:\n",
    "            temp_wav_filename = temp_wav_file.name\n",
    "            chunk.export(temp_wav_filename, format=\"wav\")\n",
    "\n",
    "            # Recognize speech from the temporary WAV file\n",
    "            with sr.AudioFile(temp_wav_filename) as source:\n",
    "                audio_data = recognizer.record(source)\n",
    "            \n",
    "            try:\n",
    "                text = recognizer.recognize_google(audio_data)\n",
    "                text_chunks.append(text)\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Google Speech Recognition could not understand audio\")\n",
    "                text_chunks.append(\"\")  # Append an empty string if recognition fails\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "            # Close the temporary WAV file\n",
    "            temp_wav_file.close()\n",
    "\n",
    "            # Remove the temporary WAV file\n",
    "            os.remove(temp_wav_filename)\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"Sudha.murthy.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ffmpeg-python\n",
    "pip install SpeechRecognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install SpeechRecognition moviepy\n",
    "#install libs\n",
    "!pip install SpeechRecognition moviepy\n",
    "import moviepy.editor as mpe\n",
    "#convert to audio\n",
    "video = mpe.VideoFileClip(\"videos\\An Important Advice-AI Is Advancing Just Start Learning AI Before Its Late.mp4\")\n",
    "video.audio.write_audiofile(r\"An Important Advice-AI .wav\")\n",
    "import wave, math, contextlib\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import AudioFileClip\n",
    "transcribed_audio_file_name = \"DATASCIENCE.mp4.wav\"\n",
    "zoom_video_file_name = \"An Important Advice-AI .wav\"\n",
    "audioclip = AudioFileClip(zoom_video_file_name)\n",
    "audioclip.write_audiofile(transcribed_audio_file_name)\n",
    "with contextlib.closing(wave.open(transcribed_audio_file_name,'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)\n",
    "total_duration = math.ceil(duration / 60)\n",
    "r = sr.Recognizer()\n",
    "for i in range(0, total_duration):\n",
    "    with sr.AudioFile(transcribed_audio_file_name) as source:\n",
    "        audio = r.record(source, offset=i*60, duration=60)\n",
    "    f = open(\"DATASCIENCE.txt\", \"a\")\n",
    "    f.write(r.recognize_google(audio))\n",
    "    f.write(\" \")\n",
    "f.close()\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    # Load the audio file\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio = recognizer.record(source, duration=300)  # Transcribe the first 300 seconds (5 minutes) of audio\n",
    "    \n",
    "    # Transcribe the audio\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"An Important Advice-AI .wav\"  # Path to your input audio file\n",
    "    transcription = transcribe_audio(audio_file)\n",
    "    print(\"Transcription:\", transcription)\n",
    "\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def split_audio(input_audio, output_folder, segment_duration=60000):\n",
    "    audio = AudioSegment.from_file(input_audio)\n",
    "    segment_index = 0\n",
    "\n",
    "    while len(audio) > segment_duration:\n",
    "        segment = audio[:segment_duration]\n",
    "        segment.export(os.path.join(output_folder, f\"segment_{segment_index}.wav\"), format=\"wav\")\n",
    "        audio = audio[segment_duration:]\n",
    "        segment_index += 1\n",
    "\n",
    "    if len(audio) > 0:\n",
    "        audio.export(os.path.join(output_folder, f\"segment_{segment_index}.wav\"), format=\"wav\")\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio_data)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def main(input_audio, output_text):\n",
    "    output_folder = \"audio_segments\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Split audio into segments\n",
    "    split_audio(input_audio, output_folder)\n",
    "\n",
    "    # Transcribe each segment\n",
    "    transcriptions = []\n",
    "    for segment_file in os.listdir(output_folder):\n",
    "        segment_path = os.path.join(output_folder, segment_file)\n",
    "        transcription = transcribe_audio(segment_path)\n",
    "        transcriptions.append(transcription)\n",
    "\n",
    "    # Combine transcriptions\n",
    "    with open(output_text, \"w\") as f:\n",
    "        for transcription in transcriptions:\n",
    "            f.write(transcription + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_audio = \"An Important Advice-AI .wav\"  # Path to your input audio file\n",
    "    output_text = \"output_transcription.txt\"  # Path to save the transcription\n",
    "    main(input_audio, output_text)\n",
    "\n",
    "#its video to audio \n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def video_to_audio(video_file, audio_file):\n",
    "    # Load the video file\n",
    "    video_clip = VideoFileClip(video_file)\n",
    "\n",
    "    # Extract the audio from the video\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    # Save the extracted audio to a file\n",
    "    audio_clip.write_audiofile(audio_file)\n",
    "\n",
    "    # Close the video and audio clips\n",
    "    video_clip.close()\n",
    "    audio_clip.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"videos\\Devin AI Capabilities,What Can First AI Software Engineer Do_ Future Of Software Engineering.mp4\"\n",
    "    audio_file = \"davin.wav\"  # You can specify the desired audio format here\n",
    "\n",
    "    # Convert video to audio\n",
    "    video_to_audio(video_file, audio_file)\n",
    "\n",
    "#its working good \n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio_data)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def split_audio(input_audio):\n",
    "    audio = AudioSegment.from_file(input_audio)\n",
    "    segment_duration_ms = 60 * 1000  # 1 minute in milliseconds\n",
    "    segments = []\n",
    "\n",
    "    for start_time in range(0, len(audio), segment_duration_ms):\n",
    "        segment = audio[start_time:start_time + segment_duration_ms]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def main(input_audio):\n",
    "    segments = split_audio(input_audio)\n",
    "    transcriptions = []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment.export(f\"segment_{i}.wav\", format=\"wav\")\n",
    "        transcription = transcribe_audio(f\"segment_{i}.wav\")\n",
    "        transcriptions.append(transcription)\n",
    "        os.remove(f\"segment_{i}.wav\")\n",
    "\n",
    "    combined_transcription = \" \".join(transcriptions)\n",
    "    return combined_transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_audio = \"DATASCIENCE.mp4.wav\"  # Path to your input audio file\n",
    "    transcription = main(input_audio)\n",
    "    print(\"Transcription:\", transcription)\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"DATASCIENCE.mp4.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"davin.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n",
    "\n",
    "pip install whisper\n",
    "pip install git+https://github.com/openai/whisper.git\n",
    "pip install ffmpeg\n",
    "pip install torch\n",
    "pip install transformers\n",
    "\n",
    "pip install ffmpeg-python\n",
    "pip choco install ffmpeg \n",
    "# Uninstall ffmpeg\n",
    "# pip3 uninstall ffmpeg\n",
    "# pip3 uninstall ffmpeg-python\n",
    "# pip uninstall ffmpeg\n",
    "# pip uninstall ffmpeg-python\n",
    "# brew uninstall ffmpeg\n",
    "# Install ffmpeg binary using this 405 guide.\n",
    "\n",
    "# Install ffmpeg for python\n",
    "\n",
    "# pip3 install ffmpeg\n",
    "# pip3 install ffmpeg-python\n",
    "# pip install ffmpeg\n",
    "# pip install ffmpeg-python\n",
    "# # Thatâ€™s all it should work \n",
    "pip install ffmpeg --upgrade\n",
    "pip install pip3 --upgrade\n",
    "pip install ffmpeg-python --upgrade\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the device\n",
    "pytorch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current PyTorch device is set to\", pytorch_device)\n",
    "\n",
    "# Define the path to the audio file\n",
    "audio_path = \"DATASCIENCE.mp4.wav\"\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = pipeline(model=\"openai/whisper-small\", device=pytorch_device)\n",
    "\n",
    "# Process the audio file\n",
    "predictions = pipe(audio_path, chunk_length_s=20, stride_length_s=5)\n",
    "\n",
    "# Extract the recognized text from the predictions\n",
    "recognized_text = predictions[\"text\"]\n",
    "print(recognized_text)\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the device\n",
    "pytorch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current PyTorch device is set to\", pytorch_device)\n",
    "\n",
    "# Define the path to the audio file\n",
    "audio_path = \"videos\\Many People Have Forgetten This!.mp4\"\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = pipeline(model=\"openai/whisper-small\", device=pytorch_device)\n",
    "\n",
    "# Process the audio file\n",
    "predictions = pipe(audio_path, chunk_length_s=20, stride_length_s=5)\n",
    "\n",
    "# Extract the recognized text from the predictions\n",
    "recognized_text = predictions[\"text\"]\n",
    "print(recognized_text)\n",
    "\n",
    "pip install git+https://github.com/openai/whisper.git\n",
    "import whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"Many-pepoles.wav\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions(fp16 = False)\n",
    "#options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "# print the recognized text\n",
    "print(result.text)\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"Many-pepoles.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n",
    "\n",
    "#its video to audio \n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def video_to_audio(video_file, audio_file):\n",
    "    # Load the video file\n",
    "    video_clip = VideoFileClip(video_file)\n",
    "\n",
    "    # Extract the audio from the video\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    # Save the extracted audio to a file\n",
    "    audio_clip.write_audiofile(audio_file)\n",
    "\n",
    "    # Close the video and audio clips\n",
    "    video_clip.close()\n",
    "    audio_clip.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"videos\\Portado Story by Sudha Murthy ðŸ’¯.mp4\"\n",
    "    audio_file = \"Sudha.murthy.wav\"  # You can specify the desired audio format here\n",
    "\n",
    "    # Convert video to audio\n",
    "    video_to_audio(video_file, audio_file)\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"Muniba.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Create a temporary WAV file\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_wav_file:\n",
    "            temp_wav_filename = temp_wav_file.name\n",
    "            chunk.export(temp_wav_filename, format=\"wav\")\n",
    "\n",
    "            # Recognize speech from the temporary WAV file\n",
    "            with sr.AudioFile(temp_wav_filename) as source:\n",
    "                audio_data = recognizer.record(source)\n",
    "            \n",
    "            try:\n",
    "                text = recognizer.recognize_google(audio_data)\n",
    "                text_chunks.append(text)\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Google Speech Recognition could not understand audio\")\n",
    "                text_chunks.append(\"\")  # Append an empty string if recognition fails\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "            # Close the temporary WAV file\n",
    "            temp_wav_file.close()\n",
    "\n",
    "            # Remove the temporary WAV file\n",
    "            os.remove(temp_wav_filename)\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"Sudha.murthy.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
