{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: SpeechRecognition in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (4.10.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (69.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install moviepy SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Wave in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (0.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install Wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from SpeechRecognition) (4.10.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (57.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install moviepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: brew in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (0.1.3)\n",
      "Requirement already satisfied: ffmpeg in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install brew ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in chunk_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 1: tell me about yourself and I completed my B tech CSE and in VIT Chennai from Hyderabad and my skills and all language python\n",
      "MoviePy - Writing audio in chunk_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 2: what is the formula which is used for data manipulation and creation and also for data cleaning and it is a part of python what are the main methods available in Pandas to generate the data there are two main method they are series and data frame what is the series method is the\n",
      "MoviePy - Writing audio in chunk_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 3: Single Single column but with indexes and what is the data frame data frame we can create multiple columns and multiple rows at a time ok so what is the describe method in one class method is used for knowing the standard deviation and mean and the minimum value and maximum value of the data so what is the information\n",
      "MoviePy - Writing audio in chunk_3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 4: columns data type and columns how many numbers of columns present in the data frame so how to remove the double duplicates in the data set using underscore duplicate we can remove the and also we can use by Group by method to check the duplicates and remove so how to rename the column\n",
      "MoviePy - Writing audio in chunk_4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 5: the linear method we can rename the column mentioning the data frame so how to identify the null values inside the data frame null values there are three methods to identify to manipulate first first is fill in the and replace method and also I am asking\n",
      "MoviePy - Writing audio in chunk_5.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 6: regional we can use the regional and also and also is any so how to remove the duplicates sorry delete the elements do you know there are three method so how to calculate\n",
      "MoviePy - Writing audio in chunk_6.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 7: calculate the mean we can use the mean method so what is the difference between all and anything sorry how to how to read the data set in Windows using read csv we can read the csv file\n",
      "MoviePy - Writing audio in chunk_7.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 8: how to convert csv to accept yes we can convert using to using 2 to excel and giving the Excel so can I possible to connect with Pandas through database as we can connect any SQL database using Pandas and we can also manipulate the data using once you read the data set OK so the data set it is\n",
      "MoviePy - Writing audio in chunk_8.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 9: healthy or not how to check the healthy data centre using describe we can check all the so I have a one character problem so that can I have a 34 values are repeated so how to identify using value counts you can see the number of number of times the values repeated we can say\n",
      "MoviePy - Writing audio in chunk_9.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 10: what about the characteristics of the most repeated value and will replace that into the character column so in Pandas how to generate the customer\n",
      "MoviePy - Writing audio in chunk_10.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 11: using the column name and equals to the values with first the data frame then the column name then equal to the values then the column will be inserted So In basic Python you know that list what is the list list it is used with close bracket which consists of value second what is the set method using set or are also\n",
      "MoviePy - Writing audio in chunk_11.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 12: child brackets suppose you have a list of some duplicates how to delete the duplicates what is the dictionary dictionary it consist of key and values in which we can reduce the values using the keys\n",
      "MoviePy - Writing audio in chunk_12.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 13: what is I need something can you explain ok so so how to show you are missing partner what is the how to register so I have some values how to delete\n",
      "MoviePy - Writing audio in chunk_13.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 14: so same way in Pandas also ok some questions are missing so how to drop the elements how to replace how to have this practice is\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load the video\n",
    "video_path = 'videos\\DATASCIENCE  MOCK INTERVIEW _ Technical Round _ DATASCIENCE Interview _  @magneqsoftware6896 _.mp4'\n",
    "video_clip = mp.VideoFileClip(video_path)\n",
    "\n",
    "# Extract audio from the video\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Split audio into chunks if necessary\n",
    "# You may want to adjust chunk_duration as needed\n",
    "chunk_duration = 30  # seconds\n",
    "chunks = [audio_clip.subclip(t_start, min(t_start + chunk_duration, audio_clip.duration))\n",
    "          for t_start in range(0, int(audio_clip.duration), chunk_duration)]\n",
    "\n",
    "# Perform speech recognition on each chunk\n",
    "recognizer = sr.Recognizer()\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_path = f'chunk_{i}.wav'\n",
    "    chunk.write_audiofile(chunk_path)\n",
    "\n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(f\"Chunk {i+1}:\", text)\n",
    "\n",
    "# Cleanup\n",
    "audio_clip.close()\n",
    "video_clip.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in chunk_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 1: tell me about yourself and I completed my B tech CSE and in VIT Chennai from Hyderabad and my skills and all language which is used for data manipulation and creation and also for data cleaning and it is a part of what are the main method they are series and date of\n",
      "MoviePy - Writing audio in chunk_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 2: Single Single column but with indexes and what is the data frame data frame we can create multiple columns and multiple rows at a time ok so what is the described method in one class method is used for knowing the standard deviation and mean and the minimum value and maximum value of the data so what is the information is used to know the columns data type and columns how many numbers of columns present in the data so how to remove the duplicates in the dataset using underscore duplicate we can remove the and also we can use by Group by method to check the duplicates and how to\n",
      "MoviePy - Writing audio in chunk_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 3: the linear method we can rename the column mentioning the data frame so how to identify the null values inside the data frame null values there are three methods to identify to manipulate first first fill in the and replace method and also we can use the legal and also how to remove the duplicates delete and replace method so how to calculate\n",
      "MoviePy - Writing audio in chunk_3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 4: calculate the mean we can use the mean method so what is the difference between all and anything sorry how to how to read the data set in Windows using read csv we can read the csv file to convert we can convert using to using to excel and giving the excellent possible to connect to database as we can connect SQL database using Pandas and we can also manipulate the data using once you read the data set OK so the data set it is\n",
      "MoviePy - Writing audio in chunk_4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 5: healthy or not how to check the healthy data centre using describe we can check all the so I have a one character problem so that can I have a 34 values are repeated so how to identify using value counts you can see the number of number of times the values repeated we can change the most repeated value and will replace that into the character called generator\n",
      "MoviePy - Writing audio in chunk_5.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 6: using the column name and equals to the values with first the data frame then the column name then equal to the values then the column will be inserted So In basic Python you know that list what is the list list it is used with close bracket which consists of value second what is the second method using set or are also brackets suppose you have a list of some duplicates how to delete the duplicates what is the dictionary it consists of key and values in which we can reduce the values using the keys\n",
      "MoviePy - Writing audio in chunk_6.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 7: what is can you explain ok so so how to show you are missing partner is the how to register so I have some values how to delete so how to drop the elements how to request\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load the video\n",
    "video_path = 'videos\\DATASCIENCE  MOCK INTERVIEW _ Technical Round _ DATASCIENCE Interview _  @magneqsoftware6896 _.mp4'\n",
    "video_clip = mp.VideoFileClip(video_path)\n",
    "\n",
    "# Extract audio from the video\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Split audio into chunks if necessary\n",
    "# You may want to adjust chunk_duration as needed\n",
    "chunk_duration =60 # seconds\n",
    "chunks = [audio_clip.subclip(t_start, min(t_start + chunk_duration, audio_clip.duration))\n",
    "          for t_start in range(0, int(audio_clip.duration), chunk_duration)]\n",
    "\n",
    "# Perform speech recognition on each chunk\n",
    "recognizer = sr.Recognizer()\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_path = f'chunk_{i}.wav'\n",
    "    chunk.write_audiofile(chunk_path)\n",
    "\n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(f\"Chunk {i+1}:\", text)\n",
    "\n",
    "# Cleanup\n",
    "audio_clip.close()\n",
    "video_clip.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in chunk_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 1: Mary hi hello handsome and I am applying for one of your kitchen jobs here is the copy of my recipe thank you but I want to learn I work hard at home I love to learn new things very organised and I found the direction of the company actually give me a special certificate for coming to work on time every day for you and\n",
      "MoviePy - Writing audio in chunk_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 2: improve my writing skills that's great why did you leave your laptop it was graveyard and I need to work days I said I am from 8:00 a.m. and 5 p.m. ok would you have any questions for yes what kind of training is needed not a lot more questions\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load the video\n",
    "video_path = 'videos\\Job Interview_ I Want to Learn (ESL).mp4'\n",
    "video_clip = mp.VideoFileClip(video_path)\n",
    "\n",
    "# Extract aud\n",
    "# io from the video\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Split audio into chunks if necessary\n",
    "# You may want to adjust chunk_duration as needed\n",
    "chunk_duration =60 # seconds\n",
    "chunks = [audio_clip.subclip(t_start, min(t_start + chunk_duration, audio_clip.duration))\n",
    "          for t_start in range(0, int(audio_clip.duration), chunk_duration)]\n",
    "\n",
    "# Perform speech recognition on each chunk\n",
    "recognizer = sr.Recognizer()\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_path = f'chunk_{i}.wav'\n",
    "    chunk.write_audiofile(chunk_path)\n",
    "\n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(f\"Chunk {i+1}:\", text)\n",
    "\n",
    "# Cleanup\n",
    "audio_clip.close()\n",
    "video_clip.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in chunk_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 1: given an array of integers with n + 1 numbers and its ranging from 1 to N there is only one repeated number in this entire array how can you find it without modifying the array and only using constant extra space hi everyone and welcome to another mark interview with exponent I am here today with Ahmed would you like to introduce yourself one numbers in IT ranging from 1 to 10 number in this entire how can you find it without modifying the array and only using constant extra space\n",
      "MoviePy - Writing audio in chunk_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 2: OK so the Android for this question is an array of integers from 1 to N and I am required to return this element that repeated in this array right yes exactly ok so what I'm thinking about is the brute force solution which is operating through all the array two times the first one is activated through every element and the second time is comparing this element with all the other elements in the ring and if we find the duplicate we can return the message and we will be\n",
      "MoviePy - Writing audio in chunk_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 3: the first one is Looking through all the elements in Delhi and inside this folder we will have another for loop by creating through all the elements again in the rain what's starting from this end because we don't need to be repeating comparisons more than one time starting\n",
      "MoviePy - Writing audio in chunk_3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 4: and if we finish the photo without finding any element I believe you should return some invalid value like negative one example but this shouldn't be happening according to our places you always have it for now and then yeah that's so this is the first session and the believe the time complexity here is in square because we have two for loops and space complexity because we want to change the time complexity here and improve it first of all what is the time complexity right now and how would you improve\n",
      "MoviePy - Writing audio in chunk_4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 5: what's the temperature of the array and it's not the best I think the second solution to improve the time complexity is sorting the array and comparing every element with the element that after it but this will only increase the time will improve the time complexity to end login I believe that the easiest solution is used then it means that we already found another one that's the same problem\n",
      "MoviePy - Writing audio in chunk_5.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 6: we know that you mentioned in the in the problem that we can use extra space here is the tricky part we already know that the elements starting from 1 to N so we can use the array itself as a set of OK so how we will be doing this is a great things to all the elements and whenever we find the element we go to its index or the index that is equal to its value and we want this element is visited by negative\n",
      "MoviePy - Writing audio in chunk_6.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 7: is the index needs to be the element -1 but the element might be visited before so it might have negative values that we need to get the absolute value for this image and since the elements from 1 to end but the array is zero based index so we need to subtract one to be from 0 to and - 1 4 1 5 9 1\n",
      "MoviePy - Writing audio in chunk_7.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 8: other than that we need to work this element as visited so we say reflex ok every element we calculate the corresponding index which is like using Hash Function and for this and this will check if we visited this element before or not if we visited so duplicate if we didn't know how we can use that ok\n",
      "MoviePy - Writing audio in chunk_8.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 9: start with the simple one to use this function inside the main since the meaning of statistics through it had to be also sorry that has values from 12 and is 4 cm and\n",
      "MoviePy - Writing audio in chunk_9.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 10: show the applications for example if you have this duplicate but for multiple times exam believe it's like this so you only have one duplicate values duplicated for more than two times you have any suggestions for you know I think the thing that people mostly get stuck on is starting with the optimal solution and try to improve it not starting\n",
      "MoviePy - Writing audio in chunk_10.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Chunk 11: yeah exactly thank you very much really appreciate having you here today thanks so much for watching don't forget to hit the like and subscribe videos\n"
     ]
    }
   ],
   "source": [
    "#its 10.40 minutes video take time to convert into text 8.6 mintues \n",
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load the video\n",
    "video_path = 'videos\\Software Engineering Mock Interview_ Find the Duplicate Number (with Google SWE).mp4'\n",
    "video_clip = mp.VideoFileClip(video_path)\n",
    "\n",
    "# Extract audio from the video\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Split audio into chunks if necessary\n",
    "# You may want to adjust chunk_duration as needed\n",
    "chunk_duration =60 # seconds\n",
    "chunks = [audio_clip.subclip(t_start, min(t_start + chunk_duration, audio_clip.duration))\n",
    "          for t_start in range(0, int(audio_clip.duration), chunk_duration)]\n",
    "\n",
    "# Perform speech recognition on each chunk\n",
    "recognizer = sr.Recognizer()\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_path = f'chunk_{i}.wav'\n",
    "    chunk.write_audiofile(chunk_path)\n",
    "\n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(f\"Chunk {i+1}:\", text)\n",
    "\n",
    "# Cleanup\n",
    "audio_clip.close()\n",
    "video_clip.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in chunk_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in chunk_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcribed text: Mary hi hello handsome and I am applying for one of your kitchen jobs here is the copy of my recipe thank you but I want to learn I work hard at home I love to learn new things very organised and I found the direction of the company actually give me a special certificate for coming to work on time every day for you and\n",
      "improve my writing skills that's great why did you leave your laptop it was graveyard and I need to work days I said I am from 8:00 a.m. and 5 p.m. ok would you have any questions for yes what kind of training is needed not a lot more questions\n",
      "Word count: 116\n"
     ]
    }
   ],
   "source": [
    "#its 1.45 mintues video take time convert into text 2.79 minutes\n",
    "import os\n",
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "def transcribe_video(video_path, chunk_duration=60):\n",
    "    # Load the video\n",
    "    video_clip = mp.VideoFileClip(video_path)\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    # Split audio into chunks\n",
    "    chunks = [audio_clip.subclip(t_start, min(t_start + chunk_duration, audio_clip.duration))\n",
    "              for t_start in range(0, int(audio_clip.duration), chunk_duration)]\n",
    "\n",
    "    # Perform speech recognition on each chunk\n",
    "    recognizer = sr.Recognizer()\n",
    "    transcribed_texts = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_path = f'chunk_{i}.wav'\n",
    "        chunk.write_audiofile(chunk_path)\n",
    "\n",
    "        with sr.AudioFile(chunk_path) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            transcribed_texts.append(text)\n",
    "\n",
    "        # Remove the chunk file\n",
    "        os.remove(chunk_path)\n",
    "\n",
    "    # Combine text from different chunks\n",
    "    combined_text = '\\n'.join(transcribed_texts)\n",
    "    word_count = len(combined_text.split())\n",
    "\n",
    "    # Cleanup\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n",
    "\n",
    "    return combined_text, word_count\n",
    "\n",
    "# Example usage\n",
    "video_path = 'videos\\Job Interview_ I Want to Learn (ESL).mp4'\n",
    "transcribed_text, word_count = transcribe_video(video_path)\n",
    "print(\"Transcribed text:\", transcribed_text)\n",
    "print(\"Word count:\", word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepspeech\n",
      "  Downloading deepspeech-0.9.3-cp39-cp39-win_amd64.whl (8.0 MB)\n",
      "Requirement already satisfied: numpy>=1.19.4 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from deepspeech) (1.26.4)\n",
      "Installing collected packages: deepspeech\n",
      "Successfully installed deepspeech-0.9.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install deepspeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in newdata.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "An error occurred: name 'deepspeech' is not defined\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import moviepy.editor as mp\n",
    "#import deepspeech \n",
    "\n",
    "import deepspeech as ds\n",
    "\n",
    "# Step 1: Load the video and extract audio\n",
    "video_path = 'videos\\Job Interview_ I Want to Learn (ESL).mp4'\n",
    "video_clip = mp.VideoFileClip(video_path)\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Step 2: Save audio to a temporary file\n",
    "temp_audio_path = 'newdata.wav'\n",
    "audio_clip.write_audiofile(temp_audio_path)\n",
    "\n",
    "# Step 3: Initialize DeepSpeech model\n",
    "#model_path=ds.Model(r\"..\\deepspeech\\deepspeech-0.7.3-models.pbmm\")\n",
    "model_path ='path/to/deepspeech-0.9.3-models.pbmm'\n",
    "scorer_path = 'path/to/deepspeech-0.9.3-models.scorer'\n",
    "\n",
    "try:\n",
    "    model = deepspeech.Model(model_path)\n",
    "    model.enableExternalScorer(scorer_path)\n",
    "\n",
    "    # Step 4: Transcribe audio to text using DeepSpeech\n",
    "    with open(temp_audio_path, 'rb') as audio_file:\n",
    "        audio_data = audio_file.read()\n",
    "        text = model.stt(audio_data)\n",
    "\n",
    "    print(\"Transcribed text:\", text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "finally:\n",
    "    # Step 5: Cleanup\n",
    "    os.remove(temp_audio_path)\n",
    "\n",
    "    # Close video and audio clips\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in newspeech.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "#this method take less time convert into text . its fast \n",
    "import wave, math, contextlib\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import AudioFileClip\n",
    "transcribed_audio_file_name = \"newspeech.wav\"\n",
    "zoom_video_file_name = \"videos\\Job Interview_ I Want to Learn (ESL).mp4\"\n",
    "audioclip = AudioFileClip(zoom_video_file_name)\n",
    "audioclip.write_audiofile(transcribed_audio_file_name)\n",
    "with contextlib.closing(wave.open(transcribed_audio_file_name,'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)\n",
    "total_duration = math.ceil(duration / 60)\n",
    "r = sr.Recognizer()\n",
    "for i in range(0, total_duration):\n",
    "    with sr.AudioFile(transcribed_audio_file_name) as source:\n",
    "        audio = r.record(source, offset=i*60, duration=60)\n",
    "    f = open(\"transcription.txt\", \"a\")\n",
    "    f.write(r.recognize_google(audio))\n",
    "    f.write(\" \")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in Software_Engineering.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "import wave, math, contextlib\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import AudioFileClip\n",
    "transcribed_audio_file_name = \"Software_Engineering.wav\"\n",
    "zoom_video_file_name = \"videos\\Softwar.mp4\"\n",
    "audioclip = AudioFileClip(zoom_video_file_name)\n",
    "audioclip.write_audiofile(transcribed_audio_file_name)\n",
    "with contextlib.closing(wave.open(transcribed_audio_file_name,'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)\n",
    "total_duration = math.ceil(duration / 60)\n",
    "r = sr.Recognizer()\n",
    "for i in range(0, total_duration):\n",
    "    with sr.AudioFile(transcribed_audio_file_name) as source:\n",
    "        audio = r.record(source, offset=i*60, duration=60)\n",
    "    f = open(\"Software_Engineering.txt\", \"a\")\n",
    "    f.write(r.recognize_google(audio))\n",
    "    f.write(\" \")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in DATASCIENCE.mp4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "import wave, math, contextlib\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import AudioFileClip\n",
    "transcribed_audio_file_name = \"DATASCIENCE.mp4.wav\"\n",
    "zoom_video_file_name = \"videos\\DATASCIENCE.mp4\"\n",
    "audioclip = AudioFileClip(zoom_video_file_name)\n",
    "audioclip.write_audiofile(transcribed_audio_file_name)\n",
    "with contextlib.closing(wave.open(transcribed_audio_file_name,'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)\n",
    "total_duration = math.ceil(duration / 60)\n",
    "r = sr.Recognizer()\n",
    "for i in range(0, total_duration):\n",
    "    with sr.AudioFile(transcribed_audio_file_name) as source:\n",
    "        audio = r.record(source, offset=i*60, duration=60)\n",
    "    f = open(\"DATASCIENCE.txt\", \"a\")\n",
    "    f.write(r.recognize_google(audio))\n",
    "    f.write(\" \")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcript:\n",
      " Mary Hi hello and I am applying for one of your kitchen jobs here is the copy of my recipe thank you but I want to learn I work hard at home I love to learn new things very organised and I found the company actually give me a special certificate for coming to work on time every time you so much for your time thank you\n",
      "Word Count: 67\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "def extract_audio(video_file, audio_file):\n",
    "    video_clip = mp.VideoFileClip(video_file)\n",
    "    audio_clip = video_clip.audio\n",
    "    audio_clip.write_audiofile(audio_file)\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand the audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"videos\\Job Interview_ I Want to Learn (ESL).mp4\"  # Provide the path to your video file\n",
    "    audio_file = \"temp_audio.wav\"  # Temporary audio file path\n",
    "\n",
    "    extract_audio(video_file, audio_file)\n",
    "    transcript = transcribe_audio(audio_file)\n",
    "    \n",
    "    if transcript:\n",
    "        word_count = count_words(transcript)\n",
    "        print(\"Transcript:\\n\", transcript)\n",
    "        print(\"Word Count:\", word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in newlearn.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Total Word Count: 128\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import math\n",
    "import contextlib\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import AudioFileClip\n",
    "\n",
    "transcribed_audio_file_name = \"newlearn.wav\"\n",
    "zoom_video_file_name = r\"videos\\Job Interview_ I Want to Learn (ESL).mp4\"\n",
    "\n",
    "# Extract audio from video\n",
    "audioclip = AudioFileClip(zoom_video_file_name)\n",
    "audioclip.write_audiofile(transcribed_audio_file_name)\n",
    "\n",
    "# Calculate duration\n",
    "with contextlib.closing(wave.open(transcribed_audio_file_name, 'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)\n",
    "\n",
    "total_duration = math.ceil(duration / 60)\n",
    "\n",
    "# Transcribe audio in chunks of 60 seconds and count words\n",
    "word_count = 0\n",
    "r = sr.Recognizer()\n",
    "\n",
    "for i in range(0, total_duration):\n",
    "    with sr.AudioFile(transcribed_audio_file_name) as source:\n",
    "        audio = r.record(source, offset=i*60, duration=60)\n",
    "    \n",
    "    # Transcribe audio\n",
    "    transcribed_text = r.recognize_google(audio)\n",
    "\n",
    "    # Count words\n",
    "    words = transcribed_text.split()\n",
    "    word_count += len(words)\n",
    "\n",
    "    # Write transcribed text to file\n",
    "    with open(\"newDATASCIENCE.txt\", \"a\") as f:\n",
    "        f.write(transcribed_text)\n",
    "        f.write(\" \")\n",
    "\n",
    "# Print word count\n",
    "print(\"Total Word Count:\", word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in teio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcript:\n",
      " Mary hi hello handsome and I am applying for one of your kitchen jobs here is the copy of my recipe thank you but I want to learn I work hard at home I love to learn new things very organised and I follow directions\n",
      "Reference Text:\n",
      " Your reference text goes here\n",
      "Accuracy: 0.2\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "import Levenshtein\n",
    "\n",
    "def extract_audio(video_file, audio_file):\n",
    "    video_clip = mp.VideoFileClip(video_file)\n",
    "    audio_clip = video_clip.audio\n",
    "    audio_clip.write_audiofile(audio_file)\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand the audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "\n",
    "# def calculate_accuracy(reference_text, transcribed_text):\n",
    "#     distance = Levenshtein.distance(reference_text, transcribed_text)\n",
    "#     accuracy = 1 - (distance / len(reference_text))\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "def calculate_accuracy(reference_text, transcribed_text):\n",
    "    reference_words = reference_text.split()\n",
    "    transcribed_words = transcribed_text.split()\n",
    "\n",
    "    common_words = set(reference_words) & set(transcribed_words)\n",
    "    accuracy = len(common_words) / len(reference_words)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"videos\\Job Interview_ I Want to Learn (ESL).mp4\"  # Provide the path to your video file\n",
    "    audio_file = \"teio.wav\"  # Temporary audio file path\n",
    "    reference_text = \"Your reference text goes here\"  # Actual text of what is being said in the video\n",
    "\n",
    "    extract_audio(video_file, audio_file)\n",
    "    transcript = transcribe_audio(audio_file)\n",
    "    \n",
    "    if transcript:\n",
    "        print(\"Transcript:\\n\", transcript)\n",
    "        print(\"Reference Text:\\n\", reference_text)\n",
    "        accuracy = calculate_accuracy(reference_text, transcript)\n",
    "        print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting whisper\n",
      "  Using cached whisper-1.1.10-py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from whisper) (1.16.0)\n",
      "Installing collected packages: whisper\n",
      "Successfully installed whisper-1.1.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyDub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: PyDub\n",
      "Successfully installed PyDub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install PyDub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyDub in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (0.25.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pip install PyDub --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingsound\n",
      "  Using cached huggingsound-0.1.6-py3-none-any.whl (28 kB)\n",
      "Collecting jiwer<3.0.0,>=2.5.1\n",
      "  Using cached jiwer-2.6.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: datasets<3.0.0,>=2.6.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingsound) (2.18.0)\n",
      "Collecting torch!=1.12.0,<1.13.0,>=1.7\n",
      "  Using cached torch-1.12.1-cp39-cp39-win_amd64.whl (161.8 MB)\n",
      "Collecting librosa<0.10.0,>=0.9.2\n",
      "  Using cached librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.23.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingsound) (4.39.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (1.26.4)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (2024.2.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (4.66.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (2.31.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (3.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (0.21.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (3.13.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (6.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (0.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (2.2.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (24.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets<3.0.0,>=2.6.1->huggingsound) (15.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.6.1->huggingsound) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.6.1->huggingsound) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.6.1->huggingsound) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.6.1->huggingsound) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.6.1->huggingsound) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.6.1->huggingsound) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets<3.0.0,>=2.6.1->huggingsound) (4.10.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jiwer<3.0.0,>=2.5.1->huggingsound) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz==2.13.7 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jiwer<3.0.0,>=2.5.1->huggingsound) (2.13.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from click<9.0.0,>=8.1.3->jiwer<3.0.0,>=2.5.1->huggingsound) (0.4.6)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (1.3.2)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (1.8.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (0.4.3)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (1.4.1.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (3.0.1)\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.45.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (0.59.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (1.12.0)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (0.12.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from numba>=0.45.1->librosa<0.10.0,>=0.9.2->huggingsound) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pooch>=1.0->librosa<0.10.0,>=0.9.2->huggingsound) (4.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.6.1->huggingsound) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.6.1->huggingsound) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.6.1->huggingsound) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.6.1->huggingsound) (2024.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from scikit-learn>=0.19.1->librosa<0.10.0,>=0.9.2->huggingsound) (3.4.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from soundfile>=0.10.2->librosa<0.10.0,>=0.9.2->huggingsound) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.10.0,>=0.9.2->huggingsound) (2.21)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.23.1->huggingsound) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.23.1->huggingsound) (0.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.23.1->huggingsound) (2023.12.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.6.1->huggingsound) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.6.1->huggingsound) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.6.1->huggingsound) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets<3.0.0,>=2.6.1->huggingsound) (1.16.0)\n",
      "Installing collected packages: torch, librosa, jiwer, huggingsound\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.1\n",
      "    Uninstalling torch-2.2.1:\n",
      "      Successfully uninstalled torch-2.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Shamshad ahmed\\\\object detection\\\\myenv\\\\Lib\\\\site-packages\\\\~orch\\\\lib\\\\asmjit.dll'\n",
      "Check the permissions.\n",
      "\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.gitNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\shamshad ahmed\\appdata\\local\\temp\\pip-req-build-54dhm1rm\n",
      "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (1.26.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (0.6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (4.66.2)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (10.2.0)\n",
      "Requirement already satisfied: numba in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (0.59.1)\n",
      "Requirement already satisfied: torch in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper==20231117) (1.12.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from numba->openai-whisper==20231117) (0.42.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper==20231117) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm->openai-whisper==20231117) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages)\n",
      "  Running command git clone -q https://github.com/openai/whisper.git 'C:\\Users\\Shamshad ahmed\\AppData\\Local\\Temp\\pip-req-build-54dhm1rm'\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/openai/whisper.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"choco\" - maybe you meant \"check\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip choco install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"scoop\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip scoop install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools-rust\n",
      "  Downloading setuptools_rust-1.9.0-py3-none-any.whl (26 kB)\n",
      "Collecting tomli>=1.2.1\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting semantic-version<3,>=2.8.2\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting setuptools>=62.4\n",
      "  Using cached setuptools-69.2.0-py3-none-any.whl (821 kB)\n",
      "Installing collected packages: tomli, setuptools, semantic-version, setuptools-rust\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 57.4.0\n",
      "    Uninstalling setuptools-57.4.0:\n",
      "      Successfully uninstalled setuptools-57.4.0\n",
      "Successfully installed semantic-version-2.10.0 setuptools-69.2.0 setuptools-rust-1.9.0 tomli-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install setuptools-rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.2.1 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyannote.core\n",
      "  Downloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
      "Collecting sortedcontainers>=2.0.4\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.core) (1.26.4)\n",
      "Collecting scipy>=1.1\n",
      "  Downloading scipy-1.12.0-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.core) (4.10.0)\n",
      "Installing collected packages: sortedcontainers, scipy, pyannote.core\n",
      "Successfully installed pyannote.core-5.0.0 scipy-1.12.0 sortedcontainers-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pyannote.core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyannote.audioNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading pyannote.audio-3.1.1-py2.py3-none-any.whl (208 kB)\n",
      "Collecting huggingface-hub>=0.13.0\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Collecting lightning>=2.0.1\n",
      "  Downloading lightning-2.2.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting einops>=0.6.0\n",
      "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "Collecting torch-audiomentations>=0.11.0\n",
      "  Downloading torch_audiomentations-0.11.1-py3-none-any.whl (50 kB)\n",
      "Collecting pyannote.pipeline>=3.0.1\n",
      "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
      "Collecting rich>=12.0.0\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Collecting omegaconf<3.0,>=2.1\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Collecting pyannote.metrics>=3.2\n",
      "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
      "Collecting tensorboardX>=2.6\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: pyannote.core>=5.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.audio) (5.0.0)\n",
      "Collecting torchaudio>=2.0.0\n",
      "  Downloading torchaudio-2.2.1-cp39-cp39-win_amd64.whl (2.4 MB)\n",
      "Collecting soundfile>=0.12.1\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Collecting asteroid-filterbanks>=0.4\n",
      "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
      "Collecting pytorch-metric-learning>=2.1.0\n",
      "  Downloading pytorch_metric_learning-2.4.1-py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.audio) (2.2.1)\n",
      "Collecting pyannote.database>=5.0.1\n",
      "  Downloading pyannote.database-5.0.1-py3-none-any.whl (48 kB)\n",
      "Collecting speechbrain>=0.5.14\n",
      "  Downloading speechbrain-1.0.0-py3-none-any.whl (760 kB)\n",
      "Collecting semver>=3.0.0\n",
      "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Collecting torchmetrics>=0.11.0\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (4.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.31.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (3.13.1)\n",
      "Collecting lightning-utilities<2.0,>=0.8.0\n",
      "  Downloading lightning_utilities-0.11.0-py3-none-any.whl (25 kB)\n",
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Using cached aiohttp-3.9.3-cp39-cp39-win_amd64.whl (366 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.5-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (23.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.4-cp39-cp39-win_amd64.whl (76 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.1-cp39-cp39-win_amd64.whl (50 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from lightning-utilities<2.0,>=0.8.0->lightning>=2.0.1->pyannote.audio) (69.2.0)\n",
      "Collecting antlr4-python3-runtime==4.9.*\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=1.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.12.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.4 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
      "Requirement already satisfied: pandas>=0.19 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.1)\n",
      "Collecting typer[all]>=0.2.1\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2024.1)\n",
      "Requirement already satisfied: sympy>=1.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.12)\n",
      "Collecting matplotlib>=2.0.0\n",
      "  Downloading matplotlib-3.8.3-cp39-cp39-win_amd64.whl (7.6 MB)\n",
      "Collecting scikit-learn>=0.17.1\n",
      "  Downloading scikit_learn-1.4.1.post1-cp39-cp39-win_amd64.whl (10.6 MB)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting docopt>=0.6.2\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (10.2.0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.5-cp39-cp39-win_amd64.whl (56 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.2.0-cp39-cp39-win_amd64.whl (181 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.50.0-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.18.1)\n",
      "Collecting optuna>=3.1\n",
      "  Downloading optuna-3.6.0-py3-none-any.whl (379 kB)\n",
      "Collecting sqlalchemy>=1.3.0\n",
      "  Downloading SQLAlchemy-2.0.28-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "Collecting Mako\n",
      "  Using cached Mako-1.3.2-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from rich>=12.0.0->pyannote.audio) (2.17.2)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Collecting cffi>=1.0\n",
      "  Downloading cffi-1.16.0-cp39-cp39-win_amd64.whl (181 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting hyperpyyaml\n",
      "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.0.3-cp39-cp39-win_amd64.whl (290 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio) (1.3.0)\n",
      "Collecting protobuf>=3.20\n",
      "  Downloading protobuf-5.26.0-cp39-cp39-win_amd64.whl (420 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch>=2.0.0->pyannote.audio) (3.1.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch>=2.0.0->pyannote.audio) (3.2.1)\n",
      "Collecting torch-pitch-shift>=1.2.2\n",
      "  Downloading torch_pitch_shift-1.2.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting julius<0.3,>=0.2.3\n",
      "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
      "Collecting librosa>=0.6.0\n",
      "  Downloading librosa-0.10.1-py3-none-any.whl (253 kB)\n",
      "Collecting pooch>=1.0\n",
      "  Downloading pooch-1.8.1-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.59.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (4.4.2)\n",
      "Collecting msgpack>=1.0\n",
      "  Downloading msgpack-1.0.8-cp39-cp39-win_amd64.whl (75 kB)\n",
      "Collecting lazy-loader>=0.1\n",
      "  Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Collecting audioread>=2.1.9\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Collecting soxr>=0.3.2\n",
      "  Downloading soxr-0.3.7-cp39-cp39-win_amd64.whl (184 kB)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from numba>=0.51.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pooch>=1.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2024.2.2)\n",
      "Collecting primePy>=1.3\n",
      "  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.13.0->pyannote.audio) (0.4.6)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting shellingham<2.0.0,>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting ruamel.yaml>=0.17.28\n",
      "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
      "Collecting ruamel.yaml.clib>=0.2.7\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp39-cp39-win_amd64.whl (118 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jinja2->torch>=2.0.0->pyannote.audio) (2.1.5)\n",
      "Using legacy 'setup.py install' for julius, since package 'wheel' is not installed.\n",
      "Installing collected packages: multidict, mdurl, frozenlist, yarl, pycparser, markdown-it-py, greenlet, click, async-timeout, aiosignal, typer, threadpoolctl, sqlalchemy, shellingham, ruamel.yaml.clib, rich, Mako, lightning-utilities, joblib, cffi, aiohttp, torchmetrics, torchaudio, soxr, soundfile, scikit-learn, ruamel.yaml, pyyaml, pyparsing, primePy, pooch, msgpack, lazy-loader, kiwisolver, importlib-resources, fonttools, cycler, contourpy, colorlog, audioread, alembic, torch-pitch-shift, tabulate, sentencepiece, pytorch-lightning, pyannote.database, protobuf, optuna, matplotlib, librosa, julius, hyperpyyaml, huggingface-hub, docopt, antlr4-python3-runtime, torch-audiomentations, tensorboardX, speechbrain, semver, pytorch-metric-learning, pyannote.pipeline, pyannote.metrics, omegaconf, lightning, einops, asteroid-filterbanks, pyannote.audio\n",
      "    Running setup.py install for julius: started\n",
      "    Running setup.py install for julius: finished with status 'done'\n",
      "Successfully installed Mako-1.3.2 aiohttp-3.9.3 aiosignal-1.3.1 alembic-1.13.1 antlr4-python3-runtime-4.9.3 asteroid-filterbanks-0.4.0 async-timeout-4.0.3 audioread-3.0.1 cffi-1.16.0 click-8.1.7 colorlog-6.8.2 contourpy-1.2.0 cycler-0.12.1 docopt-0.6.2 einops-0.7.0 fonttools-4.50.0 frozenlist-1.4.1 greenlet-3.0.3 huggingface-hub-0.21.4 hyperpyyaml-1.2.2 importlib-resources-6.4.0 joblib-1.3.2 julius-0.2.7 kiwisolver-1.4.5 lazy-loader-0.3 librosa-0.10.1 lightning-2.2.1 lightning-utilities-0.11.0 markdown-it-py-3.0.0 matplotlib-3.8.3 mdurl-0.1.2 msgpack-1.0.8 multidict-6.0.5 omegaconf-2.3.0 optuna-3.6.0 pooch-1.8.1 primePy-1.3 protobuf-5.26.0 pyannote.audio-3.1.1 pyannote.database-5.0.1 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pycparser-2.21 pyparsing-3.1.2 pytorch-lightning-2.2.1 pytorch-metric-learning-2.4.1 pyyaml-6.0.1 rich-13.7.1 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 scikit-learn-1.4.1.post1 semver-3.0.2 sentencepiece-0.2.0 shellingham-1.5.4 soundfile-0.12.1 soxr-0.3.7 speechbrain-1.0.0 sqlalchemy-2.0.28 tabulate-0.9.0 tensorboardX-2.6.2.2 threadpoolctl-3.4.0 torch-audiomentations-0.11.1 torch-pitch-shift-1.2.4 torchaudio-2.2.1 torchmetrics-1.3.2 typer-0.9.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "pip install pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (20231117)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numba in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper) (0.59.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper) (4.66.2)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper) (10.2.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper) (0.6.0)\n",
      "Requirement already satisfied: torch in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper) (2.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from openai-whisper) (1.26.4)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from numba->openai-whisper) (0.42.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper) (4.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper) (3.2.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from torch->openai-whisper) (1.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (4.39.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shamshad ahmed\\object detection\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Shamshad ahmed\\object detection\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "LANG_ID = \"en\"\n",
    "MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "SAMPLES = 10\n",
    "\n",
    "test_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to read the audio files as arrays\n",
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n",
    "    batch[\"speech\"] = speech_array\n",
    "    batch[\"sentence\"] = batch[\"sentence\"].upper()\n",
    "    return batch\n",
    "\n",
    "test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
    "inputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "predicted_sentences = processor.batch_decode(predicted_ids)\n",
    "\n",
    "for i, predicted_sentence in enumerate(predicted_sentences):\n",
    "    print(\"-\" * 100)\n",
    "    print(\"Reference:\", test_dataset[i][\"sentence\"])\n",
    "    print(\"Prediction:\", predicted_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: hello all my name is krish Naik and welcome to my YouTube channel so guys yes this particular video is again related to some advice and guidance that I really want to provide you all and yes in this video I am going to talk about AI I want to go to provide you some kind of advice at least to start learning AI and there is a reason why I am specifically saying you this I am not saying that ok and goal should be probably making a transition working in some AI companies analytics industry not as such but start incorporating AI in your day to day life evolving a lot of new things are probably coming up initially we had machine learning deep learning not generally models and probably in the upcoming two years definitely a lot of companies are come going to come up with different different Startup ideas just using this kind of LLB models where they are specifically solving some kind of problem ok but my advice will be that you know start learning AI to incorporate the capabilities that you can actually put in your day to day activities you know as you know data data Speaks a lot you know and if you probably know AI machine learning deep learning trust me you will be able to explore many more information from that specific data and not saying that ok learn just to get a job but instead learn to make yourself more productive and you can definitely do that I know many people are working in different domains different technologies in different programming language they have been a different work but at least have an idea about AI start incorporating that in your life you may be thinking Crush you are a youtuber you are doing it for your purpose you know you may gain subscribers you may probably bring up your telling people to buy courses I am not saying nothing as such I am saying that wherever you get some sources learning at not my channel at least from somewhere else is so many open source documentation that are available you know once you start incorporating it trust me opportunities are them anymore in the world not only see if you don't want to work anyway at least use it in your personal life right you have your financial data right you have your let's say I'll give you one example one of my friend is call me this morning right he was saying the crush I am running a business you know I have this specific use cases and this is only possible because of machine learning can you help me out in this you know that I also want to learn this now let it be a business create you are a person who is running a business you are a person who is working on some of the other thing and there if you get a bit of chances of automatic things trust me I will come into picture ok and this is super important this is the advice that I really want to give to everyone ok my main aim is to democracy this AI education to everyone that is the reason why I have come up with this YouTube channel when I specifically upload videos related to everything that is in Ai tomorrow anything that probably comes I will be explaining you I will be teaching you I will be showing you multiple examples you know yeah it is up to you whether you want to make it as a full-time opportunity whether you want to learn it in such a way that you may probably get a job but start incorporating it is up to you guys I am not forcing you but starting it try to use it in your day to day practices you will be able to see the changes have seen some of my friends getting productive you know I have my cousin brother who is working in US and from past two years you know he is working his architect working with something else some other Technology some other domain but still he has started using AI in his day to day activities and this is the most important advice that I really want to give it to you right again my to democracy education to everyone that is the reason I am uploading this many number of videos with respect to learning anything it is up to you find out any sources but start learning this is the advice that I really want to give it to you because you will be seeing how much changes it is going to come up in the upcoming 2 years right now lines in is going on you can actually create your own LLM models you know I was just solving a use case right now what are documents I have let's say I want to I want to probably create an LLM model with respect to my data and I was seeing that I had a 2GB of files that is present PDF files which are a lot of content I was able to train my own chat box so this kind of examples will definitely come up I'll show you how you can probably train it right but I want to do it for my day today let's say that I have my Excel sheet of all the expenditures all the expenses that I am doing right in some format right I can also train that specific thing Malayalam models can I give an advice where I can probably save some money right this is just one some of the examples that I really want to come up with in front of you right lot of applications will be there and definitely do make sure that you take up this advice take it seriously not for making not for working full time at least but at least try to incorporate this knowledge in your life so yes I hope you like this particular video if you like this I'll see all in the next video I have a great day thank you wonder take care\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio_data)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def split_audio(input_audio):\n",
    "    audio = AudioSegment.from_file(input_audio)\n",
    "    segment_duration_ms = 60 * 1000  # 1 minute in milliseconds\n",
    "    segments = []\n",
    "\n",
    "    for start_time in range(0, len(audio), segment_duration_ms):\n",
    "        segment = audio[start_time:start_time + segment_duration_ms]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def main(input_audio):\n",
    "    segments = split_audio(input_audio)\n",
    "    transcriptions = []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment.export(f\"segment_{i}.wav\", format=\"wav\")\n",
    "        transcription = transcribe_audio(f\"segment_{i}.wav\")\n",
    "        transcriptions.append(transcription)\n",
    "        os.remove(f\"segment_{i}.wav\")\n",
    "\n",
    "    combined_transcription = \" \".join(transcriptions)\n",
    "    return combined_transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_audio = \"DATASCIENCE.mp4.wav\"  # Path to your input audio file\n",
    "    transcription = main(input_audio)\n",
    "    print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in Many-pepoles.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#video to audio \n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def video_to_audio(video_file, audio_file):\n",
    "    # Load the video file\n",
    "    video_clip = VideoFileClip(video_file)\n",
    "\n",
    "    # Extract the audio from the video\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    # Save the extracted audio to a file\n",
    "    audio_clip.write_audiofile(audio_file)\n",
    "\n",
    "    # Close the video and audio clips\n",
    "    video_clip.close()\n",
    "    audio_clip.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"videos\\Many People Have Forgetten This!.mp4\"\n",
    "    audio_file = \"Many-pepoles.wav\"  # You can specify the desired audio format here\n",
    "\n",
    "    # Convert video to audio\n",
    "    video_to_audio(video_file, audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: hello Google my name is krish Naik and welcome to my YouTube channel so guys first of all a very happy Dussehra to all of you I will hardly take around 2 to 3 minutes because I really want to convey a very important message as this is your festival I definitely you want you all to enjoy with your family if I talk about Dussehra guys I just want to say this saying Burai Ko Aag lagao achhai ko Apna aur Khushiyan Bano that basically means remove all negativity from your life do good things in your life always be truthful to yourself truthful with others I know in this world of negativity competition jealousy many more things are there in our life you know in your office place in your workplace in your day to day activities with your friends with your enemies you know what does specifies to you is that at the end of the day the truth wins and I feel people have forgotten this again I just want to stress on one point please bring positivity in your life please bring positivity in others life also if you can you know always try to do the right thing I know many people do not do the right thing they may be multiple things multiple factors that may be affected them to do the right things it is always good and better because at the end of the day the good deed that you specifically do wins everybody's heart in this world I know right now if I speaking about this word many people may not agree with me have spent more than 13 plus years in IIT company right and everything basically it company have worked in multiple places with respect to work with respect to multiple things within of friends group everything but at the end of the day from all this experience what I feel is that when you do good when you are truthful to yourself in your truthful to others at the end of the day it is always beneficial for you so this is what Dussehra all signifies in our life itself again guys I really want to stressed on this specific point Burai ko hatao remove all the negativity in our life try to be truthful try to be try to bring that positive means by that everybody around you will always be happy so yes I did not want to take much time but the a lot of free things that I am actually giving in my YouTube channel everything with respect to start this positivity you can also share this message with everyone go ahead and tell your friend something good about them I know you may be you may be having some Enemies it's ok I love your enemies at the end of the day the human being you know the thought process that goes in their mind because of some something you know it may be something right of the day we are all brothers out their living in this beautiful Earth and we should definitely help out each other so considering this I'm just starting this positivity message to everyone all the free content with respect to data science will be available in the description along with the return material along with the videos everything as such again guys don't study today at least enjoy this day with your family and friends this is the festival this month then Diwali and many more things I hope you like this particular take care bye-bye\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio_data)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def split_audio(input_audio):\n",
    "    audio = AudioSegment.from_file(input_audio)\n",
    "    segment_duration_ms = 60 * 1000  # 1 minute in milliseconds\n",
    "    segments = []\n",
    "\n",
    "    for start_time in range(0, len(audio), segment_duration_ms):\n",
    "        segment = audio[start_time:start_time + segment_duration_ms]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def main(input_audio):\n",
    "    segments = split_audio(input_audio)\n",
    "    transcriptions = []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment.export(f\"segment_{i}.wav\", format=\"wav\")\n",
    "        transcription = transcribe_audio(f\"segment_{i}.wav\")\n",
    "        transcriptions.append(transcription)\n",
    "        os.remove(f\"segment_{i}.wav\")\n",
    "\n",
    "    combined_transcription = \" \".join(transcriptions)\n",
    "    return combined_transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_audio = \"Many-pepoles.wav\"  # Path to your input audio file\n",
    "    transcription = main(input_audio)\n",
    "    print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: hello Google my name is krish Naik and welcome to my YouTube channel so guys first of all a very happy Dussehra to all of you I will hardly take around 2 to 3 minutes because I really want to convey a very important message as this is your festival I definitely you want you all to enjoy with your family if I talk about Dussehra guys I just want to say this saying Burai Ko Aag lagao achhai ko Apna aur Khushiyan Bano that basically means\n",
      "Chunk 2: remove all negativity from your life do good things in your life always be truthful to yourself truthful with others I know in this world of negativity competition jealousy many more things are there in our life you know in your office place in your workplace in your day to day activities with your friends with your enemies you know what Dussehra specifies to you is that at the end of the day the truth wins and I feel\n",
      "Chunk 3: people have forgotten this again I just want to stress on one point please bring positivity in your life please bring positivity in others life also if you can you know always try to do the right thing I know many people do not do the right thing they may be multiple things multiple factors that may be affected them to do the right things it is always good and better because at the end of the day\n",
      "Chunk 4: the good deed that you specifically do wins everybody's heart in this world I know right now if I speaking about this word many people may not agree with me have spent more than 13 plus years in IIT company right I have seen each and everything basically in IT company I have worked in multiple places with respect to work with respect to multiple things within of friends group have seen good deeds have\n",
      "Chunk 5: everything but at the end of the day from all this experience what I feel is that when you do good when you are truthful to yourself in your truthful to others at the end of the day it is always beneficial for you so this is what Dussehra all signifies in our life itself again guys I really want to stressed on this specific point Burai ko hatao remove all the negativity in our life try to be truthful try to be try to bring that positive means by that everybody around\n",
      "Chunk 6: you will always be happy so yes I do not want to take much time but they are a lot of free things that I am actually giving in my YouTube channel everything with respect to this just to start this positivity you can also share this message with everyone go ahead and tell your friend something good about them I know you may be you may be having someone name is it's ok I love your enemies at the end of the day the human being you know the thought process that goes in their mind because of some something you know it may be something right\n",
      "Chunk 7: of the day we are all brothers out their living in this beautiful Earth and we should definitely help out each other so considering this I'm just starting this positivity message to everyone all the free content with respect to data science will be available in the description along with the return material along with the videos everything as such again guys don't study today at least enjoy this day with your family and friends this is the festival this month then Diwali and many more things I hope you like this particular\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"Many-pepoles.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: potato is the real life story potato was my classmate reasonably bright may be very bright I suppose never he came to school never came to college engineering college he used to stay here from Goa he used to come here the hostel and he never to come to college or college of 7 to 11 he will come about 10:00 a.m. and you will get into classes after that after end of the year\n",
      "Chunk 2: one month you will get up early and you will study and you know there is a method in college you take question papers April and October whatever comes in October will never come in April ok I am sure the school also you have come to final ok eliminate this question answers and then make approximate question paper and study that and used to get the first class he read all 4 years like this bright person you don't come to\n",
      "Chunk 3: classification regularly you can get distinction you can get a rank he said so that you do not know in life it is not the education in the connection it really works and we have to be a lot of connections you know in when you are young it helps you I came from a middle class family my father was a doctor gynecologist I never understood disconnection all those things then he said my father was in military where in Kolkata club which were never seen in our real\n",
      "Chunk 4: so I thought maybe the big man and it is the beyond our Bus ka Baat Nahin I felt it should something else past in 1972 I finish my engineering so as potato and I didn't meet him for long time after 3035 years I think 2008 or 9 I had been to Dubai to deliver a lecture and when the lecture was over I saw somebody waiting for waiting for you will\n",
      "Chunk 5: then he said I am potato I could not recognise because in 30 40 years people put on weight your hair will be grey you know all those physical changes will happen I would not very happy I told him oh let's learn and talk is it no I just wanted to tell you one words one word to you so I was what happened to remember in college I used to bunk classes of course\n",
      "Chunk 6: he said I should bunk class and I joined a job and because in those days the moment you have be used to get job in my time and my boss is came to know that my subordinates came to know that I don't have knowledge getting a first class marks are different than knowledge please remember parents your children should be in the pursuit of knowledge than the marks though it's a competitive examination it is a competitive world it is a competitive world\n",
      "Chunk 7: 1000 times for 10000 times more than what time I had but with all those things the knowledge which always help you then your marks in real life so he said I got marks but I did not have knowledge once the subordinates know that you don't have knowledge they don't respect you what is the boss no you don't have knowledge then he wants and he wants reason to kick you out I was kicked out from one job and so much used to getting up 9 in the morning I'll go let to your office\n",
      "Chunk 8: check out there also Oracle of time I could get job anywhere and now I have a job as a manager job or smaller job in Dubai and my I have to I said what about your family should have two daughters left them in Chennai to study I said what he said yes I want they should study well they should not become like me I just waited for you to tell to the one thing when I used to make fun of you he is to tell me you are like a you are like\n",
      "Chunk 9: running race horse because of the horse will see neither left or right it runs same way you always wear academic and wish to make fun of you today when we call you nerd and I thought I was very smart today I feel you are successful and I wasted my life I just wanted to tell you if you don't if you don't discipline if you don't discipline when you are fool or in College you pay heavy\n",
      "Chunk 10: later part of your life discipline like coming up in a proper way writing properly ok talking properly communicating properly respecting people properly this really helps a long way in your life so I don't want you should be like potato regretting in your old age you should remember that to be successful in life there is no shortcut the only method is hard work and straight way and that you have to do\n",
      "Chunk 11: remember one more part that your school you come as a rock product to school like a raw material to school school Church you out 14 years and takes you out a finished good you come out the young toddler here and you go out at the fine teenager the School has done so much not for the fees what you give your fees are minimal when you compare to the kind of advantage you get in real life your parents everyone put so much\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Create a temporary WAV file\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_wav_file:\n",
    "            temp_wav_filename = temp_wav_file.name\n",
    "            chunk.export(temp_wav_filename, format=\"wav\")\n",
    "\n",
    "            # Recognize speech from the temporary WAV file\n",
    "            with sr.AudioFile(temp_wav_filename) as source:\n",
    "                audio_data = recognizer.record(source)\n",
    "            \n",
    "            try:\n",
    "                text = recognizer.recognize_google(audio_data)\n",
    "                text_chunks.append(text)\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Google Speech Recognition could not understand audio\")\n",
    "                text_chunks.append(\"\")  # Append an empty string if recognition fails\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "            # Close the temporary WAV file\n",
    "            temp_wav_file.close()\n",
    "\n",
    "            # Remove the temporary WAV file\n",
    "            os.remove(temp_wav_filename)\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"Sudha.murthy.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: potato is the real life story potato was my classmate reasonably bright may be very bright I suppose never he came to school never came to college engineering college he used to stay here from Goa he used to come here the hostel and he never to come to college or college of 7 to 11 he will come about 10:00 a.m. and you will get into classes after that after end of the year\n",
      "Chunk 2: one month you will get up early and you will study and you know there is a method in college you take question papers April and October whatever comes in October will never come in April ok I am sure the school also you have come to final ok eliminate this question answers and then make approximate question paper and study that and used to get the first class he read all 4 years like this person you don't come to\n",
      "Chunk 3: classification regularly you can get distinction you can get a rank he said so that you do not know in life it is not the education in the connection it really works and we have to be a lot of connections you know in when you are young it helps you I came from a middle class family my father was a doctor gynecologist I never understood disconnection all those things then he said my father was in military wear in Kolkata club which were never seen in our real\n",
      "Chunk 4: so I thought maybe the big man and it is the beyond our Bus ka Baat Nahin I felt it should something else past in 1972 I finish my engineering so as potato and I didn't meet him for long time after 3035 years I think 2008 or 9 I had been to Dubai to deliver a lecture and when the lecture was over I saw somebody waiting for waiting for you will\n",
      "Chunk 5: then he said I am potato I could not recognise because in 30 40 years people put on weight your hair will be grey you know all those physical changes will happen I would not very happy I told him oh let's learn and talk is that no I just wanted to tell you one words one word to you so I was what happened to remember in college I used to bunk classes of course I remember banking class\n",
      "Chunk 6: he said I should bunk class and I joined a job and because in those days the moment you have be used to get job in my time and my boss is came to know that my subordinates came to know that I don't have knowledge getting a first class marks are different than knowledge please remember parents your children should be in the pursuit of knowledge than the marks though it's a competitive examination it is a competitive world it is a competitive world\n",
      "Chunk 7: 1000 times for 10000 times more than what time I had but with all those things the knowledge which always help you then your marks in real life so he said I got marks but I did not have knowledge once the subordinates know that you don't have knowledge they don't respect you what is the boss no you don't have knowledge then he wants and he wants reason to kick you out I was kicked out from one job and so much used to getting up 9 in the morning I'll go let to your office\n",
      "Chunk 8: check out there also Oracle of time I could get job anywhere and now I have a job as a manager job or smaller job in Dubai and my I have to I said what about your family should have two daughters left them in Chennai to study I said what he said yes I want they should study well they should not become like me I just waited for you to tell to the one thing when I used to make fun of you he is to tell me you are like a you are like\n",
      "Chunk 9: running race horse because of the horse will see neither left or right it runs same way you always wear academic and wish to make fun of you today when we call you nerd and I thought I was very smart today I feel you are successful and I wasted my life I just wanted to tell you if you don't if you don't discipline if you don't discipline when you are fool or in College you pay heavy\n",
      "Chunk 10: later part of your life discipline like coming up in a proper way writing properly ok talking properly communicating properly respecting people properly this really helps a long way in your life so I don't want you should be like potato regretting in your old age you should remember that to be successful in life there is no shortcut the only method is hard work and straight way and that you have to do\n",
      "Chunk 11: remember one more part that your school you come as a rock product to school like a raw material to school school Church you out 14 years and takes you out a finished good you come out the young toddler here and you go out at the fine teenager the School has done so much not for the fees what you give your fees are minimal when you compare to the kind of advantage you get in real life your parents everyone put so much\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"Sudha.murthy.wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: potato is the real life story potato was my classmate reasonably bright may be very bright I suppose never he came to school never came to college engineering college he used to stay here from Goa he used to come here the hostel and he never to come to college or college of 7 to 11 you will come about 10.009 and you will get into classes after that after one month you will get up early and study and college you take question paper April and October will never come in April approximate question paper of bright person you don't come classification regularly you can get distinction you can get a rank he said so that you do not know in life it is not the education in the connection it really works and we have to be a lot of connections you know in when you are young it helps you I came from a middle class family my father was a doctor gynecologist I never understood disconnection all those things then he said my father was in military wear in Kolkata club which were never seen in our real life so I thought maybe the big man and it is the beyonder in 1972 I finish my engineering and I didn't meet him for long time after 3035 years I think 2008 I had been to Dubai to deliver a lecture of waiting for you then he said I am potato I could not recognise because in 30 40 years people put on weight your hair will be grey you know all those physical changes will happen I would not very happy I told him oh let's learn and talk is that no I just wanted to tell you one words one word to you so I was what happened to remember in college I used to bunk classes of course I remember that I don't have knowledge please remember knowledge than the marks competitive examination 1000 times for 10000 times more than what time I had but with all those things the knowledge which always help you then your marks in real life so he said I got marks but I did not have knowledge once the subordinates know that you don't have knowledge they don't respect you what is the boss no you don't have knowledge then he wants and he wants reason to kick you out I was kicked out from one job and so much used to getting up 9 in the morning I'll go let to the office of time I could get job anyway and now I have a job as a manager job in Dubai and what about your family should have two daughters left them in Chennai to study I said what he said yes I want to study well they should not become like me I just waited for you to tell the one thing is to make fun of you please tell me you are like a you are like running race horse because of the horse will see neither left or right it runs same way you always wear academic and wish to make fun of you today when we call you nerd and I thought I was very smart today I feel you are successful and I wasted my life I just wanted to tell you if you don't if you don't discipline if you don't discipline when your school or in College you pay heavy price in later part of your life discipline coming up in a proper way writing properly ok talking properly communicating properly respecting people properly this really helps a long way in your life so I don't want you should be like potato regretting in your old age you should remember that to be successful in life there is no shortcut the only method is hard work and straight way and that you have to do remember one more part that your school you come as a rock product to school like a raw material to school school Church you out 14 years and takes you out a finished good you come out the young toddler here and you go out at the fine teenager the School has done so much not for the fees what you give your fees are minimal when you compare to the kind of advantage you get in real life your parents everyone put so much hard work to make your young teenager please remember the school is your alma mater and you make money more than what you need you should always help your alma mater because that is the place which has given you a life in real life\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio_data)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def split_audio(input_audio):\n",
    "    audio = AudioSegment.from_file(input_audio)\n",
    "    segment_duration_ms = 60 * 1000  # 1 minute in milliseconds\n",
    "    segments = []\n",
    "\n",
    "    for start_time in range(0, len(audio), segment_duration_ms):\n",
    "        segment = audio[start_time:start_time + segment_duration_ms]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def main(input_audio):\n",
    "    segments = split_audio(input_audio)\n",
    "    transcriptions = []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment.export(f\"segment_{i}.wav\", format=\"wav\")\n",
    "        transcription = transcribe_audio(f\"segment_{i}.wav\")\n",
    "        transcriptions.append(transcription)\n",
    "        os.remove(f\"segment_{i}.wav\")\n",
    "\n",
    "    combined_transcription = \" \".join(transcriptions)\n",
    "    return combined_transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_audio = \"Sudha.murthy.wav\"  # Path to your input audio file\n",
    "    transcription = main(input_audio)\n",
    "    print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: hello all my name is krish Naik and welcome to my YouTube channel so guys yes this particular video is again related to some advice and guidance that I really want to provide you all and yes in this video I am going to talk about AI I want to go to provide you some kind of advice at least to start learning AI and there is a reason why I am specifically saying you this I am not saying that ok and goal should be probably making a transition working in some\n",
      "Chunk 2: companies analytics industry not as such but start incorporating AI in your day to day lives ai's evolving a lot lot of new things are probably coming up initially we had machine learning deep learning models and probably in the upcoming two years definitely lot of companies are come going to come up with different different Startup ideas just using this kind of LLM models where they are specifically solving some kind of problem ok\n",
      "Chunk 3: but my advice will be that you know start learning AI to incorporate the capabilities that you can actually put in your day-to-day activities you know as you know data data Speaks a lot you know and if you probably know AI machine learning deep learning trust me you will be able to explore many more information from that specific data and not saying that ok learn just to get a job but instead learn to make\n",
      "Chunk 4: more productive and you can definitely do that I know many people are working in different domains different Technologies and different programming language they having a different work but at least have an idea about AI start incorporating that in your life you may be thinking Crush you are a youtuber you are doing it for your purpose you know you may gain subscribers you may probably bring up your telling people to buy courses I am not saying nothing as such I am saying that wherever you get some sources\n",
      "Chunk 5: learning at not my channel at least from somewhere else is so many open source documentation that are available you know once you start incorporating it trust me opportunities are there many more in the world not only see if you don't want to work anyway at least use it in your personal life right you have your financial data right you have your let's say I'll give you one example one of my friend is called me this morning right he was saying the question I am running a business you know I have this specific use cases and this is\n",
      "Chunk 6: possible because of machine learning can you help me out in this you know and he is saying his purposely saying that I also want to learn the snap you know let it be a business create you are a person who is running a business you are a person who who is working on some of the other thing and there if you get a bit of chances of automatic things trust me AI will come into picture ok and this is super important this is the advice that I really want to give to everyone ok again\n",
      "Chunk 7: my name is to democracy education to everyone that is the reason why I have come up with this YouTube channel when I specifically upload videos related to everything that is in Ai tomorrow anything that probably comes I will be explaining you I will be teaching you I will be showing you multiple examples you know yeah it is up to you whether you want to make it as a full-time opportunity whether you want to learn it in such a way that you may probably get a job but start incorporating it is up to you guys I am not forcing you but start\n",
      "Chunk 8: getting it try to use it in your day to day practices you will be able to see the changes at seen some of my friends getting productive you know I have my cousin brother who is working in US and from past two years you know he is working in his architect working with something else some other Technology some other domain but still he has started using AI in his day to day activities and this is the most important advice that I really want to give it to you right again my\n",
      "Chunk 9: to democracy education to everyone that is the reason I am uploading this many number of videos with respect to learning anything it is up to you find out any sources but start learning this is the advice that I really want to give it to you because you will be seeing how much changes it is going to come up in the upcoming 2 years right now lines in is going on you can actually create your own LLM models you know I was just solving a use case right now whatever documents I have let's say\n",
      "Chunk 10: I want to I want to probably create an LLM model with respect to my data and I was just seeing that I had a 2GB of files that is present PDF files which are a lot of content I was able to train my own chat box so this kind of examples will definitely come up I will show you how you can probably train it right but I want to do it for my day to day purpose see let's say that I have my Excel sheets of all the expenditures all the expenses that I am doing right in some format right I can also train that specific thing\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text_chunks(audio_file, chunk_duration=30):\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    total_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
    "    chunk_count = int(total_duration / chunk_duration)\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    text_chunks = []\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_duration * 1000  # Convert seconds to milliseconds\n",
    "        chunk_end = min((i + 1) * chunk_duration * 1000, len(audio))\n",
    "        chunk = audio[chunk_start:chunk_end]\n",
    "\n",
    "        # Export chunk as a temporary WAV file\n",
    "        chunk.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "        # Recognize speech from the temporary WAV file\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            text_chunks.append(text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"An Important Advice-AI .wav\"\n",
    "    chunks = audio_to_text_chunks(audio_file)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: hello all my name is krish Naik and welcome to my YouTube channel so guys yes this particular video is again related to some advice and guidance that I really want to provide you all and yes in this video I am going to talk about AI I want to go to provide you some kind of advice at least to start learning AI and there is a reason why I am specifically saying you this I am not saying that ok and goal should be probably making a transition working in some AI companies analytics industry not as such but start incorporating AI in your day to day life evolving a lot of new things are probably coming up initially we had machine learning deep learning models and probably in the upcoming two years definitely a lot of companies are coming to come up with different different Startup ideas just using this kind of LLB models where they are specifically solving some kind of problem ok but my advice will be that you know start learning AI to incorporate the capabilities that you can actually put in your day-to-day activities you know as you know data data Speaks a lot you know and if you probably know AI machine learning deep learning trust me you will be able to explore many more information from that specific data and not saying that ok learn just to get a job but instead learn to make yourself more productive and you can definitely do that I know many people are working in different domains different technologies in different programming language they have been a different work but at least have an idea about AI start incorporating that in your life you may be thinking Crush you are a youtuber you are doing it for your purpose you know you may gain subscribers you may probably bring up your telling people to buy courses I am not saying nothing as such I am saying that wherever you get some sources learning at not my channel at least from somewhere else is so many open source documentation that are available you know once you start incorporating it trust me opportunities are there many more in the world not only see if you don't want to work anyway at least use it in your personal life right you have your financial data right you have your let's say I'll give you one example one of my friend is call me this morning right he was saying the Krishna I am running a business you know I have this specific use cases and this is only possible because of machine learning can you help me out in this you know that I also want to learn this now let it be a business create you are a person who is running a business you are a person who is working on some of the other thing and there if you get a bit of chances of automatic things trust me I will come into picture ok and this is super important this is the advice that I really want to give to everyone ok my name is to democracy this AI education to everyone that is the reason why I have come up with this YouTube channel very specifically upload videos related to everything that is in Ai tomorrow anything that probably comes I will be explaining you I will be teaching you I will be showing you multiple examples you know yeah it is up to you whether you want to make it as a full-time opportunity weather you want to learn it in such a way that you may probably get a job but start incorporating it is up to you guys I am not forcing you but starting it try to use it in your day to day practices you will be able to see the changes have seen some of my friends getting productive you know I have my cousin brother who is working in US and from past two years you know his working his architect working with something else some other Technology some other domain but still he has started using AI in his day to day activities and this is the most important advice that I really want to give it to you again my to democracy education to everyone that is the reason I am uploading this many number of videos with respect to learning anything it is up to you find out any sources but start learning this is the advice that I really want to give it to you because you will be seeing how much changes it is going to come up in the upcoming 2 years right now lines in is going on you can actually create your own LLM models you know I was just solving a use case right now whatever documents I have let's say I want to I want to problem create an LLM model with respect to my data and I was seeing that I had a 2GB of files that is present PDF files which are a lot of content I was able to train my own chat box so this kind of examples will definitely come up and I'll show you how you can probably train it right but I want to do it for my day today let's say that I have my Excel sheet of all the expenditures all the expenses that I am doing right in some format right I can also train that specific thing Malayalam models can I give an advice where I can probably save some money right this is just one some of the examples that I really want to come up with in front of you right lot of applications will be there and definitely do make sure that you take up this advice take it seriously not for making not for working full time at least but at least try to incorporate this knowledge in your life so yes I hope you like this particular video if you like this I have a great day and thank you wonder bye bye take care\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio_data)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def split_audio(input_audio):\n",
    "    audio = AudioSegment.from_file(input_audio)\n",
    "    segment_duration_ms = 60 * 1000  # 1 minute in milliseconds\n",
    "    segments = []\n",
    "\n",
    "    for start_time in range(0, len(audio), segment_duration_ms):\n",
    "        segment = audio[start_time:start_time + segment_duration_ms]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def main(input_audio):\n",
    "    segments = split_audio(input_audio)\n",
    "    transcriptions = []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment.export(f\"segment_{i}.wav\", format=\"wav\")\n",
    "        transcription = transcribe_audio(f\"segment_{i}.wav\")\n",
    "        transcriptions.append(transcription)\n",
    "        os.remove(f\"segment_{i}.wav\")\n",
    "\n",
    "    combined_transcription = \" \".join(transcriptions)\n",
    "    return combined_transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_audio = \"An Important Advice-AI .wav\"  # Path to your input audio file\n",
    "    transcription = main(input_audio)\n",
    "    print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch device is set to cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello all, my name is Krashanayak and welcome to my YouTube channel. So guys, yes, this particular video is again related to some advice, some guidance that I really want to provide you all. And yes, in this video, I'm going to talk about AI. I want to go to provide you some kind of advice at least to start learning AI. And there is a reason why I am specifically saying you this. I'm not saying that, okay that your end goal should be probably making a transition, working in some AI companies, analytics industry, not as such, but start incorporating AI in your day to day lives. AI is evolving a lot, a lot of new things are probably coming up. Initially, we had machine learning, deep learning, now, generally AI, L LLM models and probably in the upcoming two years definitely a lot of companies are going to come up with different different startup ideas just using this kind of LLM models where they are specifically solving some kind of problem. But my advice will be that start learning AI to incorporate the capabilities that you can actually put in your day to day activities, you know, as you know, data, data speaks a lot, you know, and if you probably know AI, machine learning, deep learning, trust me you will be able to explore many more information from that specific data. I'm not saying that okay learn just to get a job but instead learn to make yourself more productive and you can definitely do that. I know many people are working in different domains, different technologies and different programming languages, they are having a different work but at least have an idea about AI, start incorporating that in your life. You may be thinking, Chris, you are a YouTuber, you're doing it for your purpose. You know, you may gain subscribers, you may probably bring up, you're telling people to buy courses. I'm not saying nothing as such. I'm saying that wherever you get some sources, start learning it, not my channel, at least from somewhere else. There's so many open source documentation that are available. You know, once you start incorporating it, trust me, opportunities are there many more in the world, not only see if you don't want to work anywhere, at least use it in your personal life. Right? You have your financial data, right? You have your, let's say, I'll give you one example. One of my friends just called me this morning, right? He was saying that I'm running a business, you know, I have this specific use cases and this is only possible because of machine learning. Can you help me out in this? You know, and he's saying, he's purposely saying that I also want to learn this now. Let it be a business creator, you are a person who is running a business, you are a person who is working on some or the other thing. And there if you get a bit of chances of automating things, trust me, AI will come into picture. Okay. And this is super important democratize this AI education to everyone. That is the reason I have come up with this YouTube channel where I specifically upload videos related to everything that is in AI. Tomorrow anything that probably comes, I will be explaining you. I will be teaching you. I will be showing you multiple examples. It is up to you whether you want to make it as a full time opportunity, whether you want to learn it in such a way that you may probably get a job but start incorporating it. It is up to you guys. I'm not forcing you but start incorporating it. Try to use it in your day to day practices. You will be able to see the changes. I've seen some of my friends getting productive. You know, I have my cousin brother who is working in US and from past two years, you know, he's working in some, he's a architect working in something else, some other technology, some other domain, but still he has started using AI in his day to day activities and you can see that productivity. And this is the most important advice that I really want to give it to you. Again, my main aim is to democratize AI education to everyone. That is the reason why I'm uploading this many number of videos with respect to learning anything. It is up to you. Find out any sources but start learning. This is the advice I want to give to you. Because you will be seeing how much changes it is going to come up in the upcoming two years. Right now, Lanchin is going on. You can actually create your own LLM models. I was just solving a use case right now. Whatever documents I have, let's say I want to probably create an LLM model with respect to my data. I was just seeing that I had a 2GB of files that is present, PDF files which had a lot of content, I was able to train my own chatbot. So this kind of examples will definitely come up. I'll show you how you can probably train it. Right. But I want to do it for my day to day purpose. and my LLM models can give advice where I can probably save some money. This is just one of the examples that I really want to come up with in front of you. A lot of applications will be there and definitely do make sure that you take up this advice, take it seriously, not for making, not for working full time at least, but at least try to incorporate this knowledge in your life. So yes, I hope you like this particular video. If you like this, I'll see you all in the next video. Have a great day. Thank you, Wanda. Bye-bye, take care.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the device\n",
    "pytorch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current PyTorch device is set to\", pytorch_device)\n",
    "\n",
    "# Define the path to the audio file\n",
    "audio_path = \"An Important Advice-AI .wav\"\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = pipeline(model=\"openai/whisper-small\", device=pytorch_device)\n",
    "\n",
    "# Process the audio file\n",
    "predictions = pipe(audio_path, chunk_length_s=20, stride_length_s=5)\n",
    "\n",
    "# Extract the recognized text from the predictions\n",
    "recognized_text = predictions[\"text\"]\n",
    "print(recognized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch device is set to cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Portado was my classmate. He was reasonably bright. Never came to college. He used to stay from Goa. He was in the hostel and to come to college. Our college was 7 to 11, he will come around 10 o'clock or 9 o'clock, earliest mean 9 o'clock. And he will sit here, sit there, attend two classes. After that, that afternoon he will come for the lab etc. end of the year he will one month he will get up early and he will study and you know there is a method in college. You take question papers, April and October. Whatever comes in October will never come in April. I am sure in the school also you have. Midterm will never come to final. So eliminate these question answers and then make an approximate question paper and study that and he used to get a first class. He did all four years like this. Once I asked Portado, Portado why you are such a bright person, you don't come to class. If you come regularly, he can get distinction. You can get a rank. He said, Suda, you do not know. In life, it is not the education. It is the connection that really works. And we have to build a lot of connections. You know, when you are young, it helps you. I came from a middle-class family my father was a doctor a gynecologist I never understood this connection all those things then I said my father was in military were in Calcutta, there was a club which we have never seen in our real life. So we thought maybe the big man and it is no beyond our, our bus is not nahi I felt it said something years past in 1972 I finished my engineering so was Portado and I didn't meet him for long time after 30 30-35 years, I think 2008 or 2009, I had been to Dubai to deliver a lecture and when the lecture was over, I saw somebody waiting for me and then he said, ma'am, I was waiting for you. I was surprised. The place like Dubai. Who will wait for me? Then he said, I am Portado. I could not recognize because in 30, 40 years people put on weight, your hair will be grey. You know all those physical changes will happen. I said, Portado, I would not recognize. I was very happy. I told him, oh let us sit down and talk. He said, no, I just wanted to tell you one word. One word to you, so I was waiting. I said, what happened? He said, do you remember in college I used to bunk class I said of course I remember you ported out means bunking class he said I used to bunk class and I joined a job and the because in those days moment you have BE used to get job in my time. And my bosses came to know that, my subordinates came to know that I don't have knowledge. Getting a first class marks different than knowledge. Please remember, parents, your children should be in the pursuit of knowledge than the marks. Though it is a competitive examination. I agree. It is a competitive world. It is a competitive examination. I agree it is a competitive world. It is a competitive world. Competition is thousand times or ten thousand times more than what time I had. But with all those things, the knowledge which always helps you than your marks in real life. So he said, I got marks but I did not have knowledge. Once the subordinates know that you don't have knowledge, they don't respect you. Once the boss knows you don't have knowledge, then he wants a reason to kick you out. I was kicked out from one job, then I took a second job and I was so was so much used to getting up 9 in the morning I will go late to the office I was kicked out there also over a period of time I could not get job anyway and now I have a job as a manager job or a smaller job in Dubai. And I said, what about your family? I said, I have two daughters. I left them in Chennai to study. I said what? He said yes, I want they should study well, they should not become like me. I just waited for you to tell Suda one thing, when we were young I used to to make fun of you. He used to tell me, you are like a running race horse because the horse will see neither left nor right, it runs.. Same way you always were academic and used to make fun of you. Today when we called you nerd and I thought I was very smart know, today I feel you are successful and I wasted my life. I just wanted to tell you, if you don't discipline, if you don't discipline when you are in school or college, you pay a heavy price in later part of your life. Discipline, like coming up in a proper way, writing properly, talking properly, communicating properly,                                                                                                    Respecting people properly, this really helps a long way in your life. So I don't want you to be like a potato regretting your old age. You should remember that to be successful in life there is no shortcut. The only method is hard work and straight way and that you have to do. And remember one more part that your school you come as a raw product to school like a raw material to school school turns you out 14 years and takes you out as a finished good you come out as a young toddler here you and you go out a fine teenager. The school has done so much not for the fees what you give. Your fees are minimal when you compare to the kind of advantage you get in real life. Your teachers, your parents, everyone put so much hard work to make you a young teenager. Please remember the school is your alma mater. And when you grow big, when you make money more than what you need, you should always help your alma mater because that is the place which has given you a life in real life.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the device\n",
    "pytorch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current PyTorch device is set to\", pytorch_device)\n",
    "\n",
    "# Define the path to the audio file\n",
    "audio_path = \"Sudha.murthy.wav\"\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = pipeline(model=\"openai/whisper-small\", device=pytorch_device)\n",
    "\n",
    "# Process the audio file\n",
    "predictions = pipe(audio_path, chunk_length_s=20, stride_length_s=5)\n",
    "\n",
    "# Extract the recognized text from the predictions\n",
    "recognized_text = predictions[\"text\"]\n",
    "print(recognized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "Hello all my name is Krushnayak and welcome to my YouTube channel. So guys, yes, this particular video is again related to some advice, some guidance that I really want to provide you all. And yes, in this video, I'm going to talk about AI. I'm going to provide you some kind of advice at least to start learning AI. And there is a reason why I am specifically saying you this.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"An Important Advice-AI .wav\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions(fp16 = False)\n",
    "#options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#its taking more than hour\n",
    "import subprocess\n",
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text(audio_file):\n",
    "    # Extract audio from the input file using FFmpeg\n",
    "    output_wav_file = \"audio_extracted.wav\"\n",
    "    subprocess.run([\"Ffmpeg\", \"-i\", audio_file, \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"44100\", \"-ac\", \"2\", output_wav_file])\n",
    "\n",
    "    # Transcribe the extracted audio into text\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(output_wav_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    \n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech Recognition could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Speech Recognition service; {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"DATASCIENCE.mp4.wav\"  # Replace with the path to your audio file\n",
    "    transcribed_text = audio_to_text(audio_file)\n",
    "    print(\"Transcribed text:\", transcribed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shamshad ahmed\\object detection\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch device is set to cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, my name is Krishnayak and welcome to my YouTube channel. So guys, I hope for the past couple of days you have seen a lot of videos related to Devon, the first AI software engineer. Probably every YouTuber has uploaded this specific video and obviously they have brought a lot of points with respect comparing to a software engineer jobs. But let's understand in this video what all things Devan can actually do and as you all know many people have applied for the beta version of Devan and they have got the access and with respect to that they have created a lot of videos. So we'll just try to get a clear idea like what all efficient task it can actually do. And after understanding all these things, we will be discussing about the pros and cons. And as a software engineer, what you really need to focus to be in the industry with a good demand what all things you can actually do we'll also be discussing about that. So please make sure that it was this video till the end because there will be a lot of things to discuss in this. Okay, so here was the video of the Devin first AI software engineer. We just saw the demo. But just to understand, you can see what is exactly Devon. It is the world's first autonomous AI software engineer. Devon is tireless, skilled, he met equally ready to build alongside you or independently complete task for you to review. So in short, since it is an AI, so it really need to work tirelessly, right? With human beings, it is not possible, right? Now, a very important is over here. With Devan engineers can focus more on the interesting problem statement and engineer teams can strive for more ambitious goals because all the previous problem statements are already solved by the engineers and with respect to that they already have the data code, whatever things are specifically required. So obviously you really need to have a news problem statement to think upon which Devan cannot solve. Whatever things are actually solved, Devan will be able to solve it because they are already trained on that. So this is the most major point. Still, any new thing, any new problem statement that may come, Devin will not be able to solve it because it needs to get trained. It's just like like an AI if it if there is a solution then it will be trained then it will be able to do it okay. Unless and until AGI does not come I don't think so it will be able to do it. Now let's understand about Devin's capability. Okay. Here you'll be able to see we have given Devin the ability to actively collaborate with the user. Devin reports on its progress in real time, accepts feedback and works together with you through design choices as needed. Obviously, as a software engineer, whatever task is required for a software engineer, it is able to do they're working in a collaborative way. They have to make sure that they have all the documentation, sprints, stories needs to be assigned very much in an equal way to all the developers itself. You actually do with respect to the implementation that they have done. So here you can see Devin can learn how to use unfamiliar technologies also. Right. So after reading a blog post, Devin runs control not on model to produce images which conceals messages for Sara. So one of the users they were able to do from this particular blog. Devin can build end and deploy apps end to end. So this was actually possible by Devin. They were able to build and deploy it because already, you know, how to build it and all and how to deploy it, that is the tie end of task. And obviously from the demo that we have seen a couple days back, it has those kind of functionalities. Along with this, whenever we see the next task over here, you can see that Devin can also autonomously find and fix bugs in the code basis. That basically means Devan helps Andrew maintain and debug its open source competitive programming book. So obviously when you have this entire code base, you know, and it is being able to solve it. Okay. And we'll also be seeing there is a something called as a SWE benchmark. Okay. We'll discuss about it. Like in the GitHub open, open issues, like how much it was able to solve, you know. So here, the next task here, you can see Devon can train and fine tune its own AI models. Devon can address bugs and feature requests in open source repositories. Just give the GitHub issues and do the setup and context gathering that is only needed and Devon will be able to do it. Devin can contribute to mature production repositories, which is also very, really good. We even tried giving Devin real jobs on Upwork and it could do those two, right? So if you don't know about Upwork, it is all about freelancing. Now just imagine you don't even have to stay there to do any freelancing work. It can probably consider it. it can probably solve it. And with respect to all this work that you can actually see, all the demo video is actually given and how it was able to do it. Now just by seeing this obviously we can actually see that Devin can really do a lot many things right and if I probably consider a normal software engineer right now what is actually expected in interviews in the future in upcoming one to two years, there will be a lot of expectation. You really need to be a jack of all trades, you know, you should have knowledge with respect to everything. But yes, if there is a problem statement that requires actual research out of box thinking out of box thinking to solve it, then human beings are the best people to solve it. Later on, whatever task are there, whatever repetitive task are there, whatever things that are already implemented, Devin can actually do it for you. Now, this was one of the things, which I was talking about SWE bench. So in the GitHub repository, whatever are the open issues, right? You can probably see World GitHub issues, how much it was able to solve it. So here you can see we evaluated Devin on HWE Bench, a challenging benchmark that asked agent to resolve real world GitHub issues found in open source projects like Django and Skykitla. Devin correctly resolves 13.86%. All the issues end to end, which is quite good. And the previous state of art was 1.96. Now, with respect to all the other tools, like Cloudy 2, SW, Lama 13, BSHW, Lama 7, BGPT, 4GPT 3.5, here you can actually see, you know, the difference is quite huge and Devin is really, really having a good number over here. That is 13.86. Now, you know, this, this really looks, yes, Devin can do a lot many tasks. But as I said, when it comes to problem statement that comes out of the box to solve it, definitely Devan cannot do because they are not trained in those kind of data. But as a software engineer, one thing that you really need to do is that companies will also look at, you know, whether it's their simpler tasks, whether they are common implemented tasks to quickly do it. And when there is some very hard task complex tasks, obviously human will be required. So please make sure that you have skill sets, technologies with respect to the complete 360 degree. So I hope you like this particular video. This was it for my side. I'll see you on the next video. Have a great day. Thank you, Wandaul. Take care. Bye-bye.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the device\n",
    "pytorch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current PyTorch device is set to\", pytorch_device)\n",
    "\n",
    "# Define the path to the audio file\n",
    "audio_path = \"videos\\Devin AI Capabilities,What Can First AI Software Engineer Do_ Future Of Software Engineering.mp4\"\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = pipeline(model=\"openai/whisper-small\", device=pytorch_device)\n",
    "\n",
    "# Process the audio file\n",
    "predictions = pipe(audio_path, chunk_length_s=20, stride_length_s=5)\n",
    "\n",
    "# Extract the recognized text from the predictions\n",
    "recognized_text = predictions[\"text\"]\n",
    "print(recognized_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
